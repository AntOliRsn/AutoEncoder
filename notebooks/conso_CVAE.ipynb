{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/antorosi/PycharmProjects/KERAS-TS-VENV/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "path_main_folder = '/home/antorosi/Documents/AutoEncoder'\n",
    "sys.path.append(path_main_folder)\n",
    "\n",
    "from CVAE.cvae import compile_cvae, run_cvae\n",
    "from CVAE.cvae_model import CVAE\n",
    "from conso.load_shape_data import *  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shape data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "path_data = os.path.join(path_main_folder, 'data')\n",
    "dict_data_conso = load_data_conso(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unifomization\n",
    "data_conso_df, dict_colnames_conso = get_uniformed_data_conso(dict_data_conso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ganularity ?\n",
    "# data_conso_df = change_granularity(data_conso_df, granularity=\"1H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x_conso dataframe for autoencoder purpose\n",
    "x_conso = get_x_conso_autoencoder(data_conso_df, dict_colnames_conso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "dict_xconso = {'train': x_conso}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize x_conso\n",
    "type_scaler = 'standard'\n",
    "dict_xconso = normalize_xconso(dict_xconso, dict_colnames_conso, type_scaler = 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x and cond for cvae\n",
    "x, cond, cvae_ds = get_x_cond_autoencoder(dict_xconso['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset \n",
    "dataset = {}\n",
    "dataset['train'] = {'x': [x, cond], 'y': x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out = os.path.join(path_main_folder, 'out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = 'cvae_conso_e24-12_d24-48_z2_norma-s_v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cond_dim = cond.shape[1]\n",
    "e_dims=[48,24]\n",
    "d_dims=[24,48]\n",
    "z_dim=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6604        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6312        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 12,916\n",
      "Trainable params: 12,916\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            50          enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            50          enc_dense_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,604\n",
      "Trainable params: 6,604\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 24)           408         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 48)           1200        dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,312\n",
      "Trainable params: 6,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CVAE(cond_dim=cond_dim, \n",
    "             e_dims=e_dims, \n",
    "             d_dims=d_dims, \n",
    "             z_dim=z_dim, \n",
    "             name=name_model, \n",
    "             output=path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n",
      "Epoch 1/200\n",
      "1830/1830 [==============================] - 0s 136us/step - loss: 39.9962 - kl_loss: 5.7785 - recon_loss: 34.2177\n",
      "Epoch 2/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 9.7092 - kl_loss: 3.8937 - recon_loss: 5.8155\n",
      "Epoch 3/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 7.2375 - kl_loss: 3.3274 - recon_loss: 3.9102\n",
      "Epoch 4/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 6.6087 - kl_loss: 3.1537 - recon_loss: 3.4550\n",
      "Epoch 5/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 5.9812 - kl_loss: 2.8933 - recon_loss: 3.0879\n",
      "Epoch 6/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 5.6448 - kl_loss: 2.7228 - recon_loss: 2.9219\n",
      "Epoch 7/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 5.4566 - kl_loss: 2.6046 - recon_loss: 2.8521\n",
      "Epoch 8/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 5.1181 - kl_loss: 2.4875 - recon_loss: 2.6306\n",
      "Epoch 9/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 4.8926 - kl_loss: 2.4296 - recon_loss: 2.4631\n",
      "Epoch 10/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 4.8097 - kl_loss: 2.3715 - recon_loss: 2.4382\n",
      "Epoch 11/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 4.5788 - kl_loss: 2.2695 - recon_loss: 2.3093\n",
      "Epoch 12/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 4.5164 - kl_loss: 2.2311 - recon_loss: 2.2853\n",
      "Epoch 13/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 4.4658 - kl_loss: 2.2434 - recon_loss: 2.2223\n",
      "Epoch 14/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 4.3710 - kl_loss: 2.1938 - recon_loss: 2.1772\n",
      "Epoch 15/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 4.2911 - kl_loss: 2.1366 - recon_loss: 2.1545\n",
      "Epoch 16/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 4.2193 - kl_loss: 2.1199 - recon_loss: 2.0994\n",
      "Epoch 17/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 4.1871 - kl_loss: 2.1120 - recon_loss: 2.0751\n",
      "Epoch 18/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 4.2324 - kl_loss: 2.0932 - recon_loss: 2.1392\n",
      "Epoch 19/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 4.1051 - kl_loss: 2.0659 - recon_loss: 2.0392\n",
      "Epoch 20/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 4.1743 - kl_loss: 2.0890 - recon_loss: 2.0852\n",
      "Epoch 21/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 4.0933 - kl_loss: 2.0560 - recon_loss: 2.0373\n",
      "Epoch 22/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 4.0118 - kl_loss: 2.0128 - recon_loss: 1.9990\n",
      "Epoch 23/200\n",
      "1830/1830 [==============================] - 0s 59us/step - loss: 4.0275 - kl_loss: 2.0250 - recon_loss: 2.0024\n",
      "Epoch 24/200\n",
      "1830/1830 [==============================] - 0s 59us/step - loss: 3.9840 - kl_loss: 2.0096 - recon_loss: 1.9744\n",
      "Epoch 25/200\n",
      "1830/1830 [==============================] - 0s 61us/step - loss: 3.9334 - kl_loss: 1.9840 - recon_loss: 1.9495\n",
      "Epoch 26/200\n",
      "1830/1830 [==============================] - 0s 63us/step - loss: 3.8790 - kl_loss: 1.9340 - recon_loss: 1.9450\n",
      "Epoch 27/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.9448 - kl_loss: 1.9303 - recon_loss: 2.0145\n",
      "Epoch 28/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.8199 - kl_loss: 1.9240 - recon_loss: 1.8960\n",
      "Epoch 29/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.8517 - kl_loss: 1.8844 - recon_loss: 1.9673\n",
      "Epoch 30/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.7614 - kl_loss: 1.8544 - recon_loss: 1.9070\n",
      "Epoch 31/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.7430 - kl_loss: 1.8688 - recon_loss: 1.8742\n",
      "Epoch 32/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.7269 - kl_loss: 1.8115 - recon_loss: 1.9154\n",
      "Epoch 33/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.6003 - kl_loss: 1.8038 - recon_loss: 1.7965\n",
      "Epoch 34/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.7460 - kl_loss: 1.7988 - recon_loss: 1.9472\n",
      "Epoch 35/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.6112 - kl_loss: 1.7591 - recon_loss: 1.8521\n",
      "Epoch 36/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.7112 - kl_loss: 1.8138 - recon_loss: 1.8973\n",
      "Epoch 37/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.5972 - kl_loss: 1.7533 - recon_loss: 1.8438\n",
      "Epoch 38/200\n",
      "1830/1830 [==============================] - 0s 61us/step - loss: 3.6186 - kl_loss: 1.7634 - recon_loss: 1.8552\n",
      "Epoch 39/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.5163 - kl_loss: 1.6975 - recon_loss: 1.8187\n",
      "Epoch 40/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.5834 - kl_loss: 1.7084 - recon_loss: 1.8750\n",
      "Epoch 41/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.5646 - kl_loss: 1.7043 - recon_loss: 1.8603\n",
      "Epoch 42/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.5586 - kl_loss: 1.6865 - recon_loss: 1.8721\n",
      "Epoch 43/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.4568 - kl_loss: 1.6747 - recon_loss: 1.7821\n",
      "Epoch 44/200\n",
      "1830/1830 [==============================] - 0s 62us/step - loss: 3.5188 - kl_loss: 1.6535 - recon_loss: 1.8653\n",
      "Epoch 45/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.4794 - kl_loss: 1.6751 - recon_loss: 1.8043\n",
      "Epoch 46/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.4398 - kl_loss: 1.6406 - recon_loss: 1.7992\n",
      "Epoch 47/200\n",
      "1830/1830 [==============================] - 0s 61us/step - loss: 3.4114 - kl_loss: 1.6301 - recon_loss: 1.7813\n",
      "Epoch 48/200\n",
      "1830/1830 [==============================] - 0s 62us/step - loss: 3.4652 - kl_loss: 1.6399 - recon_loss: 1.8253\n",
      "Epoch 49/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.3874 - kl_loss: 1.6176 - recon_loss: 1.7698\n",
      "Epoch 50/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.3775 - kl_loss: 1.6012 - recon_loss: 1.7763\n",
      "Epoch 51/200\n",
      "1830/1830 [==============================] - 0s 61us/step - loss: 3.3755 - kl_loss: 1.5836 - recon_loss: 1.7920\n",
      "Epoch 52/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.3542 - kl_loss: 1.5767 - recon_loss: 1.7775\n",
      "Epoch 53/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.3085 - kl_loss: 1.5454 - recon_loss: 1.7631\n",
      "Epoch 54/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.3836 - kl_loss: 1.5674 - recon_loss: 1.8162\n",
      "Epoch 55/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.3060 - kl_loss: 1.5312 - recon_loss: 1.7749\n",
      "Epoch 56/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.3130 - kl_loss: 1.5329 - recon_loss: 1.7801\n",
      "Epoch 57/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.2771 - kl_loss: 1.5248 - recon_loss: 1.7522\n",
      "Epoch 58/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.3011 - kl_loss: 1.5284 - recon_loss: 1.7727\n",
      "Epoch 59/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 3.2324 - kl_loss: 1.5070 - recon_loss: 1.7254\n",
      "Epoch 60/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.2858 - kl_loss: 1.5144 - recon_loss: 1.7714\n",
      "Epoch 61/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.2378 - kl_loss: 1.5102 - recon_loss: 1.7276\n",
      "Epoch 62/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.2174 - kl_loss: 1.4764 - recon_loss: 1.7410\n",
      "Epoch 63/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.2112 - kl_loss: 1.4966 - recon_loss: 1.7146\n",
      "Epoch 64/200\n",
      "1830/1830 [==============================] - 0s 63us/step - loss: 3.2285 - kl_loss: 1.5125 - recon_loss: 1.7160\n",
      "Epoch 65/200\n",
      "1830/1830 [==============================] - 0s 63us/step - loss: 3.1954 - kl_loss: 1.4588 - recon_loss: 1.7366\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.1808 - kl_loss: 1.4793 - recon_loss: 1.7015\n",
      "Epoch 67/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 3.2411 - kl_loss: 1.4926 - recon_loss: 1.7485\n",
      "Epoch 68/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1927 - kl_loss: 1.4521 - recon_loss: 1.7406\n",
      "Epoch 69/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1693 - kl_loss: 1.4773 - recon_loss: 1.6919\n",
      "Epoch 70/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.1571 - kl_loss: 1.4149 - recon_loss: 1.7422\n",
      "Epoch 71/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1917 - kl_loss: 1.4678 - recon_loss: 1.7238\n",
      "Epoch 72/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 3.1931 - kl_loss: 1.4665 - recon_loss: 1.7266\n",
      "Epoch 73/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1583 - kl_loss: 1.4529 - recon_loss: 1.7054\n",
      "Epoch 74/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.1756 - kl_loss: 1.4911 - recon_loss: 1.6845\n",
      "Epoch 75/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1461 - kl_loss: 1.4566 - recon_loss: 1.6895\n",
      "Epoch 76/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.1527 - kl_loss: 1.4640 - recon_loss: 1.6887\n",
      "Epoch 77/200\n",
      "1830/1830 [==============================] - 0s 63us/step - loss: 3.0943 - kl_loss: 1.4112 - recon_loss: 1.6831\n",
      "Epoch 78/200\n",
      "1830/1830 [==============================] - 0s 76us/step - loss: 3.1367 - kl_loss: 1.4094 - recon_loss: 1.7273\n",
      "Epoch 79/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.0587 - kl_loss: 1.3798 - recon_loss: 1.6789\n",
      "Epoch 80/200\n",
      "1830/1830 [==============================] - 0s 61us/step - loss: 3.1157 - kl_loss: 1.4560 - recon_loss: 1.6597\n",
      "Epoch 81/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1468 - kl_loss: 1.4371 - recon_loss: 1.7096\n",
      "Epoch 82/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.1081 - kl_loss: 1.4452 - recon_loss: 1.6628\n",
      "Epoch 83/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1338 - kl_loss: 1.4413 - recon_loss: 1.6926\n",
      "Epoch 84/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.1103 - kl_loss: 1.4203 - recon_loss: 1.6900\n",
      "Epoch 85/200\n",
      "1830/1830 [==============================] - 0s 58us/step - loss: 3.0553 - kl_loss: 1.3981 - recon_loss: 1.6571\n",
      "Epoch 86/200\n",
      "1830/1830 [==============================] - 0s 54us/step - loss: 3.0949 - kl_loss: 1.4206 - recon_loss: 1.6742\n",
      "Epoch 87/200\n",
      "1830/1830 [==============================] - 0s 53us/step - loss: 3.1069 - kl_loss: 1.4399 - recon_loss: 1.6670\n",
      "Epoch 88/200\n",
      "1830/1830 [==============================] - 0s 54us/step - loss: 3.1287 - kl_loss: 1.4336 - recon_loss: 1.6952\n",
      "Epoch 89/200\n",
      "1830/1830 [==============================] - 0s 54us/step - loss: 3.1398 - kl_loss: 1.4373 - recon_loss: 1.7025\n",
      "Epoch 90/200\n",
      "1830/1830 [==============================] - 0s 56us/step - loss: 3.0279 - kl_loss: 1.4068 - recon_loss: 1.6211\n",
      "Epoch 91/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.1149 - kl_loss: 1.4172 - recon_loss: 1.6977\n",
      "Epoch 92/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.1146 - kl_loss: 1.4381 - recon_loss: 1.6765\n",
      "Epoch 93/200\n",
      "1830/1830 [==============================] - 0s 63us/step - loss: 3.0809 - kl_loss: 1.4112 - recon_loss: 1.6697\n",
      "Epoch 94/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.0328 - kl_loss: 1.3608 - recon_loss: 1.6721\n",
      "Epoch 95/200\n",
      "1830/1830 [==============================] - 0s 62us/step - loss: 3.0389 - kl_loss: 1.3876 - recon_loss: 1.6513\n",
      "Epoch 96/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0781 - kl_loss: 1.4335 - recon_loss: 1.6446\n",
      "Epoch 97/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0557 - kl_loss: 1.4078 - recon_loss: 1.6479\n",
      "Epoch 98/200\n",
      "1830/1830 [==============================] - 0s 62us/step - loss: 3.0578 - kl_loss: 1.4229 - recon_loss: 1.6349\n",
      "Epoch 99/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0543 - kl_loss: 1.3793 - recon_loss: 1.6750\n",
      "Epoch 100/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0144 - kl_loss: 1.3901 - recon_loss: 1.6243\n",
      "Epoch 101/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0604 - kl_loss: 1.4083 - recon_loss: 1.6521\n",
      "Epoch 102/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.0473 - kl_loss: 1.4017 - recon_loss: 1.6456\n",
      "Epoch 103/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0443 - kl_loss: 1.4086 - recon_loss: 1.6357\n",
      "Epoch 104/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.0459 - kl_loss: 1.3917 - recon_loss: 1.6543\n",
      "Epoch 105/200\n",
      "1830/1830 [==============================] - 0s 72us/step - loss: 2.9977 - kl_loss: 1.3671 - recon_loss: 1.6306\n",
      "Epoch 106/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.0582 - kl_loss: 1.4054 - recon_loss: 1.6528\n",
      "Epoch 107/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 3.0377 - kl_loss: 1.4288 - recon_loss: 1.6088\n",
      "Epoch 108/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0265 - kl_loss: 1.4147 - recon_loss: 1.6118\n",
      "Epoch 109/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0679 - kl_loss: 1.4271 - recon_loss: 1.6408\n",
      "Epoch 110/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0282 - kl_loss: 1.3759 - recon_loss: 1.6523\n",
      "Epoch 111/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0308 - kl_loss: 1.4048 - recon_loss: 1.6260\n",
      "Epoch 112/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.0007 - kl_loss: 1.3887 - recon_loss: 1.6120\n",
      "Epoch 113/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0004 - kl_loss: 1.3644 - recon_loss: 1.6359\n",
      "Epoch 114/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.0109 - kl_loss: 1.3784 - recon_loss: 1.6324\n",
      "Epoch 115/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9952 - kl_loss: 1.3885 - recon_loss: 1.6067\n",
      "Epoch 116/200\n",
      "1830/1830 [==============================] - 0s 72us/step - loss: 3.0102 - kl_loss: 1.3873 - recon_loss: 1.6229\n",
      "Epoch 117/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.0452 - kl_loss: 1.3914 - recon_loss: 1.6538\n",
      "Epoch 118/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 3.0259 - kl_loss: 1.3980 - recon_loss: 1.6279\n",
      "Epoch 119/200\n",
      "1830/1830 [==============================] - 0s 72us/step - loss: 2.9805 - kl_loss: 1.3959 - recon_loss: 1.5847\n",
      "Epoch 120/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 2.9670 - kl_loss: 1.3541 - recon_loss: 1.6129\n",
      "Epoch 121/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 2.9555 - kl_loss: 1.3458 - recon_loss: 1.6097\n",
      "Epoch 122/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9961 - kl_loss: 1.3604 - recon_loss: 1.6356\n",
      "Epoch 123/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 3.0294 - kl_loss: 1.4302 - recon_loss: 1.5992\n",
      "Epoch 124/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0242 - kl_loss: 1.3980 - recon_loss: 1.6262\n",
      "Epoch 125/200\n",
      "1830/1830 [==============================] - 0s 62us/step - loss: 3.0066 - kl_loss: 1.3801 - recon_loss: 1.6265\n",
      "Epoch 126/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 3.0079 - kl_loss: 1.3749 - recon_loss: 1.6329\n",
      "Epoch 127/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 3.0005 - kl_loss: 1.3929 - recon_loss: 1.6076\n",
      "Epoch 128/200\n",
      "1830/1830 [==============================] - 0s 60us/step - loss: 2.9657 - kl_loss: 1.3823 - recon_loss: 1.5834\n",
      "Epoch 129/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9807 - kl_loss: 1.3885 - recon_loss: 1.5923\n",
      "Epoch 130/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9815 - kl_loss: 1.3726 - recon_loss: 1.6088\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9392 - kl_loss: 1.3623 - recon_loss: 1.5769\n",
      "Epoch 132/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 2.9833 - kl_loss: 1.3506 - recon_loss: 1.6327\n",
      "Epoch 133/200\n",
      "1830/1830 [==============================] - 0s 73us/step - loss: 2.9632 - kl_loss: 1.3860 - recon_loss: 1.5772\n",
      "Epoch 134/200\n",
      "1830/1830 [==============================] - 0s 75us/step - loss: 2.9468 - kl_loss: 1.3517 - recon_loss: 1.5951\n",
      "Epoch 135/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 2.9855 - kl_loss: 1.3851 - recon_loss: 1.6003\n",
      "Epoch 136/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 2.9587 - kl_loss: 1.3624 - recon_loss: 1.5963\n",
      "Epoch 137/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9772 - kl_loss: 1.3977 - recon_loss: 1.5795\n",
      "Epoch 138/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 2.9640 - kl_loss: 1.3894 - recon_loss: 1.5746\n",
      "Epoch 139/200\n",
      "1830/1830 [==============================] - 0s 76us/step - loss: 2.9729 - kl_loss: 1.3910 - recon_loss: 1.5819\n",
      "Epoch 140/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 3.0026 - kl_loss: 1.3949 - recon_loss: 1.6077\n",
      "Epoch 141/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 2.9806 - kl_loss: 1.3833 - recon_loss: 1.5974\n",
      "Epoch 142/200\n",
      "1830/1830 [==============================] - 0s 65us/step - loss: 2.9564 - kl_loss: 1.3800 - recon_loss: 1.5764\n",
      "Epoch 143/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 3.0026 - kl_loss: 1.3949 - recon_loss: 1.6077\n",
      "Epoch 144/200\n",
      "1830/1830 [==============================] - 0s 75us/step - loss: 2.9455 - kl_loss: 1.3774 - recon_loss: 1.5682\n",
      "Epoch 145/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 2.9902 - kl_loss: 1.4126 - recon_loss: 1.5776\n",
      "Epoch 146/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9272 - kl_loss: 1.3609 - recon_loss: 1.5663\n",
      "Epoch 147/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9773 - kl_loss: 1.3860 - recon_loss: 1.5913\n",
      "Epoch 148/200\n",
      "1830/1830 [==============================] - 0s 78us/step - loss: 2.9944 - kl_loss: 1.3953 - recon_loss: 1.5991\n",
      "Epoch 149/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9439 - kl_loss: 1.3854 - recon_loss: 1.5585\n",
      "Epoch 150/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 2.9515 - kl_loss: 1.3825 - recon_loss: 1.5690\n",
      "Epoch 151/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9513 - kl_loss: 1.3894 - recon_loss: 1.5619\n",
      "Epoch 152/200\n",
      "1830/1830 [==============================] - 0s 73us/step - loss: 2.9827 - kl_loss: 1.3911 - recon_loss: 1.5916\n",
      "Epoch 153/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9381 - kl_loss: 1.3677 - recon_loss: 1.5704\n",
      "Epoch 154/200\n",
      "1830/1830 [==============================] - 0s 70us/step - loss: 2.9728 - kl_loss: 1.3971 - recon_loss: 1.5757\n",
      "Epoch 155/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9451 - kl_loss: 1.3781 - recon_loss: 1.5670\n",
      "Epoch 156/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 2.9729 - kl_loss: 1.4182 - recon_loss: 1.5547\n",
      "Epoch 157/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9525 - kl_loss: 1.3914 - recon_loss: 1.5611\n",
      "Epoch 158/200\n",
      "1830/1830 [==============================] - 0s 64us/step - loss: 2.8942 - kl_loss: 1.3347 - recon_loss: 1.5596\n",
      "Epoch 159/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9513 - kl_loss: 1.3959 - recon_loss: 1.5553\n",
      "Epoch 160/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9362 - kl_loss: 1.3669 - recon_loss: 1.5693\n",
      "Epoch 161/200\n",
      "1830/1830 [==============================] - 0s 71us/step - loss: 2.9706 - kl_loss: 1.3795 - recon_loss: 1.5910\n",
      "Epoch 162/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9268 - kl_loss: 1.3762 - recon_loss: 1.5506\n",
      "Epoch 163/200\n",
      "1830/1830 [==============================] - 0s 69us/step - loss: 2.9517 - kl_loss: 1.3925 - recon_loss: 1.5592\n",
      "Epoch 164/200\n",
      "1830/1830 [==============================] - 0s 66us/step - loss: 2.9566 - kl_loss: 1.3997 - recon_loss: 1.5569\n",
      "Epoch 165/200\n",
      "1830/1830 [==============================] - 0s 67us/step - loss: 2.9395 - kl_loss: 1.3602 - recon_loss: 1.5793\n",
      "Epoch 166/200\n",
      "1830/1830 [==============================] - 0s 68us/step - loss: 2.9216 - kl_loss: 1.3480 - recon_loss: 1.5736\n",
      "Epoch 167/200\n",
      " 880/1830 [=============>................] - ETA: 0s - loss: 2.9724 - kl_loss: 1.3877 - recon_loss: 1.5847"
     ]
    }
   ],
   "source": [
    "model.main_train(dataset, training_epochs=200, batch_size=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
