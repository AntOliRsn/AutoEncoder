{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "path_main_folder = '/home/antorosi/Documents/AutoEncoder'\n",
    "\n",
    "sys.path.append(path_main_folder)\n",
    "\n",
    "from CVAE.cvae import compile_cvae, run_cvae\n",
    "from CVAE.cvae_model import CVAE\n",
    "from conso.load_shape_data import *  \n",
    "from conso.conso_helpers import plot_latent_space_projection, pyplot_latent_space_projection_temp, pyplot_latent_space_projection_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "path_data = os.path.join(path_main_folder, 'data')\n",
    "dict_data_conso = load_data_conso(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Holiday day information\n",
    "holiday_days_csv = os.path.join(path_data, \"joursFeries.csv\")\n",
    "holiday_days_df = pd.read_csv(holiday_days_csv, sep=\";\")\n",
    "holiday_days_df.ds = pd.to_datetime(holiday_days_df.ds)\n",
    "holiday_days_df['is_hd'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unifomization\n",
    "data_conso_df, dict_colnames_conso = get_uniformed_data_conso(dict_data_conso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ganularity ?\n",
    "name_granu = '15m'\n",
    "data_conso_df = change_granularity(data_conso_df, granularity=\"15min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get x_conso dataframe for autoencoder purpose\n",
    "x_conso = get_x_conso_autoencoder(data_conso_df, dict_colnames_conso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to store results\n",
    "path_out = os.path.join(path_main_folder, 'out', 'cv_model_3')\n",
    "if not os.path.exists(path_out):\n",
    "    os.mkdir(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test periods for each K step of the cross-validation\n",
    "cv_periods = {}\n",
    "cv_periods['period_1'] = (datetime.datetime(year=2013, month=1, day=1), datetime.datetime(year=2013, month=12, day=31))\n",
    "cv_periods['period_2'] = (datetime.datetime(year=2014, month=1, day=1), datetime.datetime(year=2014, month=12, day=31))\n",
    "cv_periods['period_3'] = (datetime.datetime(year=2015, month=1, day=1), datetime.datetime(year=2015, month=12, day=31))\n",
    "cv_periods['period_4'] = (datetime.datetime(year=2016, month=1, day=1), datetime.datetime(year=2016, month=12, day=31))\n",
    "cv_periods['period_5'] = (datetime.datetime(year=2017, month=1, day=1), datetime.datetime(year=2017, month=12, day=31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_scaler = 'standard'\n",
    "name_type_cond = 'm-wd'\n",
    "name_train = '0' # 1: first period ; 0 all periods\n",
    "name_type_x = 'c'\n",
    "type_x = ['conso']\n",
    "type_cond = ['month', 'weekday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting each datasets\n",
    "dict_datasets = {}\n",
    "for key, date_period in cv_periods.items():\n",
    "    dict_xconso = get_train_test_x_conso(x_conso, date_period[0], date_period[1])\n",
    "    dict_xconso = normalize_xconso(dict_xconso, dict_colnames_conso, type_scaler = type_scaler)\n",
    "    dataset = get_dataset_autoencoder(dict_xconso=dict_xconso, type_x=type_x, type_cond=type_cond)\n",
    "    \n",
    "    dict_datasets[key] = {'dataset': dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "cond_dim = dict_datasets['period_1']['dataset']['train']['x'][1].shape[1]\n",
    "e_dims=[48,24,12]\n",
    "d_dims=[12,24,48]\n",
    "z_dim= 2\n",
    "beta = 0.3\n",
    "\n",
    "training_epochs=200\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results wrap up \n",
    "results_df = pd.DataFrame(columns=['name', \n",
    "                                   'loss', 'kl_loss', 'recon_loss', \n",
    "                                   'val_loss', 'val_kl_loss', 'val_recon_loss'])\n",
    "path_results = path_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 1 =========================\n",
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6856        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6420        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 13,276\n",
      "Trainable params: 13,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_2 (Dense)             (None, 12)           300         enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            26          enc_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            26          enc_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,856\n",
      "Trainable params: 6,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 12)           204         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 24)           312         dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_2 (Dense)             (None, 48)           1200        dec_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,420\n",
      "Trainable params: 6,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n",
      "Train on 1465 samples, validate on 365 samples\n",
      "Epoch 1/200\n",
      "1465/1465 [==============================] - 0s 146us/step - loss: 39.8661 - kl_loss: 12.1104 - recon_loss: 36.2330 - val_loss: 13.7854 - val_kl_loss: 15.8425 - val_recon_loss: 9.0326\n",
      "Epoch 2/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 8.3572 - kl_loss: 7.8761 - recon_loss: 5.9944 - val_loss: 7.7639 - val_kl_loss: 7.6044 - val_recon_loss: 5.4825\n",
      "Epoch 3/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 5.9934 - kl_loss: 5.8834 - recon_loss: 4.2284 - val_loss: 5.7501 - val_kl_loss: 6.8003 - val_recon_loss: 3.7100\n",
      "Epoch 4/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 4.8526 - kl_loss: 5.2953 - recon_loss: 3.2640 - val_loss: 7.0357 - val_kl_loss: 5.1678 - val_recon_loss: 5.4853\n",
      "Epoch 5/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 4.3354 - kl_loss: 4.7778 - recon_loss: 2.9021 - val_loss: 4.4408 - val_kl_loss: 5.3068 - val_recon_loss: 2.8487\n",
      "Epoch 6/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 4.0512 - kl_loss: 4.4664 - recon_loss: 2.7113 - val_loss: 5.0684 - val_kl_loss: 5.6650 - val_recon_loss: 3.3689\n",
      "Epoch 7/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 3.8472 - kl_loss: 4.3413 - recon_loss: 2.5448 - val_loss: 4.8336 - val_kl_loss: 5.2875 - val_recon_loss: 3.2473\n",
      "Epoch 8/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 3.6951 - kl_loss: 4.1453 - recon_loss: 2.4515 - val_loss: 4.2868 - val_kl_loss: 5.1538 - val_recon_loss: 2.7407\n",
      "Epoch 9/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.5432 - kl_loss: 4.0111 - recon_loss: 2.3399 - val_loss: 4.5801 - val_kl_loss: 4.7540 - val_recon_loss: 3.1539\n",
      "Epoch 10/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 3.4214 - kl_loss: 3.8260 - recon_loss: 2.2736 - val_loss: 4.2407 - val_kl_loss: 4.4375 - val_recon_loss: 2.9094\n",
      "Epoch 11/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 3.3513 - kl_loss: 3.6614 - recon_loss: 2.2529 - val_loss: 4.4643 - val_kl_loss: 4.0145 - val_recon_loss: 3.2600\n",
      "Epoch 12/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 3.2587 - kl_loss: 3.5697 - recon_loss: 2.1878 - val_loss: 3.3272 - val_kl_loss: 4.1169 - val_recon_loss: 2.0921\n",
      "Epoch 13/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 3.1281 - kl_loss: 3.4308 - recon_loss: 2.0989 - val_loss: 3.5637 - val_kl_loss: 3.9025 - val_recon_loss: 2.3929\n",
      "Epoch 14/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.1002 - kl_loss: 3.3712 - recon_loss: 2.0888 - val_loss: 4.1781 - val_kl_loss: 4.1761 - val_recon_loss: 2.9253\n",
      "Epoch 15/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.9801 - kl_loss: 3.3701 - recon_loss: 1.9691 - val_loss: 3.1507 - val_kl_loss: 3.8862 - val_recon_loss: 1.9848\n",
      "Epoch 16/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.9765 - kl_loss: 3.3235 - recon_loss: 1.9795 - val_loss: 3.1332 - val_kl_loss: 3.8117 - val_recon_loss: 1.9897\n",
      "Epoch 17/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.8659 - kl_loss: 3.2182 - recon_loss: 1.9004 - val_loss: 3.9751 - val_kl_loss: 3.9172 - val_recon_loss: 2.7999\n",
      "Epoch 18/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.8038 - kl_loss: 3.1839 - recon_loss: 1.8487 - val_loss: 3.5921 - val_kl_loss: 3.5835 - val_recon_loss: 2.5171\n",
      "Epoch 19/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.8194 - kl_loss: 3.1857 - recon_loss: 1.8637 - val_loss: 3.2339 - val_kl_loss: 3.6668 - val_recon_loss: 2.1339\n",
      "Epoch 20/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.7121 - kl_loss: 3.0706 - recon_loss: 1.7910 - val_loss: 3.6421 - val_kl_loss: 3.3599 - val_recon_loss: 2.6342\n",
      "Epoch 21/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 2.7170 - kl_loss: 3.1192 - recon_loss: 1.7813 - val_loss: 5.0903 - val_kl_loss: 3.4267 - val_recon_loss: 4.0623\n",
      "Epoch 22/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.7176 - kl_loss: 3.1348 - recon_loss: 1.7772 - val_loss: 3.2046 - val_kl_loss: 3.3826 - val_recon_loss: 2.1898\n",
      "Epoch 23/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.6679 - kl_loss: 3.0689 - recon_loss: 1.7472 - val_loss: 4.0730 - val_kl_loss: 3.6677 - val_recon_loss: 2.9727\n",
      "Epoch 24/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.6192 - kl_loss: 3.0402 - recon_loss: 1.7072 - val_loss: 3.7574 - val_kl_loss: 3.5072 - val_recon_loss: 2.7052\n",
      "Epoch 25/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.6465 - kl_loss: 3.0296 - recon_loss: 1.7376 - val_loss: 2.9063 - val_kl_loss: 3.3397 - val_recon_loss: 1.9044\n",
      "Epoch 26/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.6374 - kl_loss: 3.0333 - recon_loss: 1.7274 - val_loss: 2.9000 - val_kl_loss: 3.4443 - val_recon_loss: 1.8667\n",
      "Epoch 27/200\n",
      "1465/1465 [==============================] - 0s 68us/step - loss: 2.5759 - kl_loss: 3.0307 - recon_loss: 1.6667 - val_loss: 3.0954 - val_kl_loss: 3.6079 - val_recon_loss: 2.0131\n",
      "Epoch 28/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.5618 - kl_loss: 3.0261 - recon_loss: 1.6540 - val_loss: 2.7440 - val_kl_loss: 3.4143 - val_recon_loss: 1.7197\n",
      "Epoch 29/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 2.5435 - kl_loss: 2.9940 - recon_loss: 1.6453 - val_loss: 2.7655 - val_kl_loss: 3.2982 - val_recon_loss: 1.7760\n",
      "Epoch 30/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.5310 - kl_loss: 3.0084 - recon_loss: 1.6285 - val_loss: 2.8507 - val_kl_loss: 3.3263 - val_recon_loss: 1.8528\n",
      "Epoch 31/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.4652 - kl_loss: 2.9561 - recon_loss: 1.5784 - val_loss: 3.5451 - val_kl_loss: 3.4630 - val_recon_loss: 2.5062\n",
      "Epoch 32/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.4740 - kl_loss: 2.9442 - recon_loss: 1.5907 - val_loss: 4.3689 - val_kl_loss: 3.5923 - val_recon_loss: 3.2912\n",
      "Epoch 33/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.4638 - kl_loss: 2.9664 - recon_loss: 1.5738 - val_loss: 3.1635 - val_kl_loss: 3.1607 - val_recon_loss: 2.2153\n",
      "Epoch 34/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.4485 - kl_loss: 2.9109 - recon_loss: 1.5753 - val_loss: 2.7674 - val_kl_loss: 3.3118 - val_recon_loss: 1.7738\n",
      "Epoch 35/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.4758 - kl_loss: 2.9628 - recon_loss: 1.5869 - val_loss: 4.5265 - val_kl_loss: 3.4610 - val_recon_loss: 3.4882\n",
      "Epoch 36/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.3977 - kl_loss: 2.9328 - recon_loss: 1.5179 - val_loss: 2.8643 - val_kl_loss: 3.4267 - val_recon_loss: 1.8363\n",
      "Epoch 37/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.4090 - kl_loss: 2.9394 - recon_loss: 1.5272 - val_loss: 3.8060 - val_kl_loss: 3.3486 - val_recon_loss: 2.8014\n",
      "Epoch 38/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.4136 - kl_loss: 2.9049 - recon_loss: 1.5421 - val_loss: 2.8127 - val_kl_loss: 3.2417 - val_recon_loss: 1.8402\n",
      "Epoch 39/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.4223 - kl_loss: 2.9151 - recon_loss: 1.5477 - val_loss: 3.0820 - val_kl_loss: 3.3558 - val_recon_loss: 2.0753\n",
      "Epoch 40/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.3689 - kl_loss: 2.9099 - recon_loss: 1.4960 - val_loss: 2.6677 - val_kl_loss: 3.3234 - val_recon_loss: 1.6707\n",
      "Epoch 41/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.4106 - kl_loss: 2.9022 - recon_loss: 1.5399 - val_loss: 2.4501 - val_kl_loss: 3.3242 - val_recon_loss: 1.4529\n",
      "Epoch 42/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.3534 - kl_loss: 2.9105 - recon_loss: 1.4803 - val_loss: 3.3084 - val_kl_loss: 3.1120 - val_recon_loss: 2.3748\n",
      "Epoch 43/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.3397 - kl_loss: 2.8820 - recon_loss: 1.4751 - val_loss: 2.7691 - val_kl_loss: 3.1805 - val_recon_loss: 1.8149\n",
      "Epoch 44/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3539 - kl_loss: 2.8568 - recon_loss: 1.4968 - val_loss: 2.6465 - val_kl_loss: 3.3358 - val_recon_loss: 1.6457\n",
      "Epoch 45/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.3362 - kl_loss: 2.8483 - recon_loss: 1.4817 - val_loss: 4.9189 - val_kl_loss: 2.9815 - val_recon_loss: 4.0245\n",
      "Epoch 46/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.3292 - kl_loss: 2.8572 - recon_loss: 1.4720 - val_loss: 2.4501 - val_kl_loss: 3.1950 - val_recon_loss: 1.4916\n",
      "Epoch 47/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2991 - kl_loss: 2.8214 - recon_loss: 1.4527 - val_loss: 2.4481 - val_kl_loss: 3.3358 - val_recon_loss: 1.4474\n",
      "Epoch 48/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.2885 - kl_loss: 2.8474 - recon_loss: 1.4343 - val_loss: 2.5115 - val_kl_loss: 2.9575 - val_recon_loss: 1.6243\n",
      "Epoch 49/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.3070 - kl_loss: 2.8469 - recon_loss: 1.4529 - val_loss: 2.8489 - val_kl_loss: 2.9997 - val_recon_loss: 1.9490\n",
      "Epoch 50/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2701 - kl_loss: 2.7843 - recon_loss: 1.4348 - val_loss: 3.1684 - val_kl_loss: 2.9459 - val_recon_loss: 2.2846\n",
      "Epoch 51/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2798 - kl_loss: 2.8025 - recon_loss: 1.4390 - val_loss: 3.2354 - val_kl_loss: 3.1321 - val_recon_loss: 2.2958\n",
      "Epoch 52/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.2913 - kl_loss: 2.8304 - recon_loss: 1.4422 - val_loss: 3.4728 - val_kl_loss: 3.2262 - val_recon_loss: 2.5050\n",
      "Epoch 53/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.2692 - kl_loss: 2.7859 - recon_loss: 1.4335 - val_loss: 2.5599 - val_kl_loss: 3.0363 - val_recon_loss: 1.6490\n",
      "Epoch 54/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.2488 - kl_loss: 2.7942 - recon_loss: 1.4105 - val_loss: 2.7568 - val_kl_loss: 2.9908 - val_recon_loss: 1.8596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.2647 - kl_loss: 2.8057 - recon_loss: 1.4230 - val_loss: 2.9511 - val_kl_loss: 3.0526 - val_recon_loss: 2.0353\n",
      "Epoch 56/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 2.2546 - kl_loss: 2.7756 - recon_loss: 1.4219 - val_loss: 3.4056 - val_kl_loss: 3.1425 - val_recon_loss: 2.4629\n",
      "Epoch 57/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.2279 - kl_loss: 2.8109 - recon_loss: 1.3846 - val_loss: 2.9299 - val_kl_loss: 3.0386 - val_recon_loss: 2.0183\n",
      "Epoch 58/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.2446 - kl_loss: 2.7839 - recon_loss: 1.4094 - val_loss: 3.6456 - val_kl_loss: 3.3782 - val_recon_loss: 2.6321\n",
      "Epoch 59/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.2287 - kl_loss: 2.8187 - recon_loss: 1.3831 - val_loss: 2.7684 - val_kl_loss: 2.9315 - val_recon_loss: 1.8890\n",
      "Epoch 60/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.2219 - kl_loss: 2.7843 - recon_loss: 1.3866 - val_loss: 3.5396 - val_kl_loss: 3.1946 - val_recon_loss: 2.5812\n",
      "Epoch 61/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.2453 - kl_loss: 2.7623 - recon_loss: 1.4166 - val_loss: 3.3849 - val_kl_loss: 3.2636 - val_recon_loss: 2.4058\n",
      "Epoch 62/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2589 - kl_loss: 2.8170 - recon_loss: 1.4138 - val_loss: 3.0701 - val_kl_loss: 3.1761 - val_recon_loss: 2.1172\n",
      "Epoch 63/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2350 - kl_loss: 2.8191 - recon_loss: 1.3893 - val_loss: 2.3950 - val_kl_loss: 3.0302 - val_recon_loss: 1.4859\n",
      "Epoch 64/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1608 - kl_loss: 2.7070 - recon_loss: 1.3487 - val_loss: 2.6711 - val_kl_loss: 2.9249 - val_recon_loss: 1.7936\n",
      "Epoch 65/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.1992 - kl_loss: 2.7331 - recon_loss: 1.3793 - val_loss: 2.5993 - val_kl_loss: 2.9227 - val_recon_loss: 1.7225\n",
      "Epoch 66/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.1963 - kl_loss: 2.7659 - recon_loss: 1.3665 - val_loss: 4.6574 - val_kl_loss: 2.9410 - val_recon_loss: 3.7751\n",
      "Epoch 67/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2079 - kl_loss: 2.8058 - recon_loss: 1.3661 - val_loss: 3.9923 - val_kl_loss: 3.0834 - val_recon_loss: 3.0673\n",
      "Epoch 68/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.1674 - kl_loss: 2.7455 - recon_loss: 1.3437 - val_loss: 2.8219 - val_kl_loss: 3.1041 - val_recon_loss: 1.8907\n",
      "Epoch 69/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2021 - kl_loss: 2.7938 - recon_loss: 1.3640 - val_loss: 2.5243 - val_kl_loss: 2.9544 - val_recon_loss: 1.6380\n",
      "Epoch 70/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.1700 - kl_loss: 2.7167 - recon_loss: 1.3550 - val_loss: 2.5747 - val_kl_loss: 3.0193 - val_recon_loss: 1.6689\n",
      "Epoch 71/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.2023 - kl_loss: 2.7671 - recon_loss: 1.3721 - val_loss: 4.4920 - val_kl_loss: 3.3924 - val_recon_loss: 3.4743\n",
      "Epoch 72/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1646 - kl_loss: 2.7750 - recon_loss: 1.3321 - val_loss: 2.4967 - val_kl_loss: 2.8723 - val_recon_loss: 1.6350\n",
      "Epoch 73/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.1634 - kl_loss: 2.7573 - recon_loss: 1.3362 - val_loss: 3.1779 - val_kl_loss: 3.2244 - val_recon_loss: 2.2106\n",
      "Epoch 74/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1335 - kl_loss: 2.7546 - recon_loss: 1.3071 - val_loss: 3.2135 - val_kl_loss: 3.1624 - val_recon_loss: 2.2648\n",
      "Epoch 75/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1851 - kl_loss: 2.7563 - recon_loss: 1.3582 - val_loss: 2.6158 - val_kl_loss: 3.0883 - val_recon_loss: 1.6893\n",
      "Epoch 76/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1155 - kl_loss: 2.7049 - recon_loss: 1.3040 - val_loss: 2.7657 - val_kl_loss: 3.0964 - val_recon_loss: 1.8368\n",
      "Epoch 77/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1329 - kl_loss: 2.7222 - recon_loss: 1.3162 - val_loss: 2.4741 - val_kl_loss: 2.8862 - val_recon_loss: 1.6082\n",
      "Epoch 78/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.1537 - kl_loss: 2.6962 - recon_loss: 1.3449 - val_loss: 2.8991 - val_kl_loss: 3.0301 - val_recon_loss: 1.9901\n",
      "Epoch 79/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1228 - kl_loss: 2.7287 - recon_loss: 1.3042 - val_loss: 2.4142 - val_kl_loss: 2.8677 - val_recon_loss: 1.5539\n",
      "Epoch 80/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.1317 - kl_loss: 2.7043 - recon_loss: 1.3204 - val_loss: 4.7676 - val_kl_loss: 2.5457 - val_recon_loss: 4.0039\n",
      "Epoch 81/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.1387 - kl_loss: 2.6851 - recon_loss: 1.3331 - val_loss: 2.3708 - val_kl_loss: 2.8729 - val_recon_loss: 1.5089\n",
      "Epoch 82/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1152 - kl_loss: 2.6946 - recon_loss: 1.3068 - val_loss: 2.8260 - val_kl_loss: 2.9764 - val_recon_loss: 1.9331\n",
      "Epoch 83/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0973 - kl_loss: 2.6698 - recon_loss: 1.2964 - val_loss: 3.2593 - val_kl_loss: 3.1723 - val_recon_loss: 2.3076\n",
      "Epoch 84/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1373 - kl_loss: 2.6938 - recon_loss: 1.3291 - val_loss: 2.3808 - val_kl_loss: 2.9646 - val_recon_loss: 1.4914\n",
      "Epoch 85/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0850 - kl_loss: 2.6928 - recon_loss: 1.2771 - val_loss: 2.4249 - val_kl_loss: 2.8360 - val_recon_loss: 1.5741\n",
      "Epoch 86/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.1321 - kl_loss: 2.6666 - recon_loss: 1.3321 - val_loss: 2.8823 - val_kl_loss: 2.8891 - val_recon_loss: 2.0156\n",
      "Epoch 87/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0842 - kl_loss: 2.6915 - recon_loss: 1.2768 - val_loss: 3.0929 - val_kl_loss: 2.6776 - val_recon_loss: 2.2896\n",
      "Epoch 88/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.1074 - kl_loss: 2.6689 - recon_loss: 1.3067 - val_loss: 2.9400 - val_kl_loss: 2.9220 - val_recon_loss: 2.0635\n",
      "Epoch 89/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.0861 - kl_loss: 2.7108 - recon_loss: 1.2729 - val_loss: 2.4222 - val_kl_loss: 2.9502 - val_recon_loss: 1.5371\n",
      "Epoch 90/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0713 - kl_loss: 2.6459 - recon_loss: 1.2775 - val_loss: 2.4161 - val_kl_loss: 2.9278 - val_recon_loss: 1.5378\n",
      "Epoch 91/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0810 - kl_loss: 2.6539 - recon_loss: 1.2848 - val_loss: 2.8396 - val_kl_loss: 3.0705 - val_recon_loss: 1.9184\n",
      "Epoch 92/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0891 - kl_loss: 2.7324 - recon_loss: 1.2694 - val_loss: 2.4762 - val_kl_loss: 2.9132 - val_recon_loss: 1.6023\n",
      "Epoch 93/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.0730 - kl_loss: 2.7068 - recon_loss: 1.2609 - val_loss: 2.8008 - val_kl_loss: 3.1508 - val_recon_loss: 1.8556\n",
      "Epoch 94/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0680 - kl_loss: 2.6831 - recon_loss: 1.2631 - val_loss: 2.4574 - val_kl_loss: 2.8042 - val_recon_loss: 1.6161\n",
      "Epoch 95/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.0848 - kl_loss: 2.6813 - recon_loss: 1.2804 - val_loss: 2.7254 - val_kl_loss: 2.8628 - val_recon_loss: 1.8665\n",
      "Epoch 96/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0870 - kl_loss: 2.6717 - recon_loss: 1.2855 - val_loss: 2.2560 - val_kl_loss: 2.8337 - val_recon_loss: 1.4059\n",
      "Epoch 97/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0425 - kl_loss: 2.6682 - recon_loss: 1.2420 - val_loss: 3.9979 - val_kl_loss: 2.9318 - val_recon_loss: 3.1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0516 - kl_loss: 2.6833 - recon_loss: 1.2466 - val_loss: 2.6609 - val_kl_loss: 3.1414 - val_recon_loss: 1.7185\n",
      "Epoch 99/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0875 - kl_loss: 2.6859 - recon_loss: 1.2818 - val_loss: 2.4911 - val_kl_loss: 2.9974 - val_recon_loss: 1.5919\n",
      "Epoch 100/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.0328 - kl_loss: 2.6730 - recon_loss: 1.2309 - val_loss: 2.4103 - val_kl_loss: 3.0082 - val_recon_loss: 1.5078\n",
      "Epoch 101/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.0492 - kl_loss: 2.6293 - recon_loss: 1.2604 - val_loss: 3.3163 - val_kl_loss: 2.9551 - val_recon_loss: 2.4297\n",
      "Epoch 102/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0569 - kl_loss: 2.6737 - recon_loss: 1.2548 - val_loss: 2.5320 - val_kl_loss: 2.9844 - val_recon_loss: 1.6366\n",
      "Epoch 103/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0555 - kl_loss: 2.6587 - recon_loss: 1.2579 - val_loss: 2.6221 - val_kl_loss: 2.8629 - val_recon_loss: 1.7632\n",
      "Epoch 104/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.0488 - kl_loss: 2.6656 - recon_loss: 1.2491 - val_loss: 2.6767 - val_kl_loss: 2.9196 - val_recon_loss: 1.8008\n",
      "Epoch 105/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0796 - kl_loss: 2.7263 - recon_loss: 1.2617 - val_loss: 2.3602 - val_kl_loss: 2.9158 - val_recon_loss: 1.4854\n",
      "Epoch 106/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0411 - kl_loss: 2.6477 - recon_loss: 1.2468 - val_loss: 2.7838 - val_kl_loss: 2.8064 - val_recon_loss: 1.9419\n",
      "Epoch 107/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0256 - kl_loss: 2.6280 - recon_loss: 1.2372 - val_loss: 2.9600 - val_kl_loss: 3.1759 - val_recon_loss: 2.0072\n",
      "Epoch 108/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0552 - kl_loss: 2.6571 - recon_loss: 1.2580 - val_loss: 2.5549 - val_kl_loss: 2.9125 - val_recon_loss: 1.6811\n",
      "Epoch 109/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.0372 - kl_loss: 2.6887 - recon_loss: 1.2306 - val_loss: 2.3824 - val_kl_loss: 2.9521 - val_recon_loss: 1.4968\n",
      "Epoch 110/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.0436 - kl_loss: 2.6258 - recon_loss: 1.2558 - val_loss: 2.3099 - val_kl_loss: 3.0446 - val_recon_loss: 1.3966\n",
      "Epoch 111/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0264 - kl_loss: 2.6663 - recon_loss: 1.2265 - val_loss: 2.3272 - val_kl_loss: 2.9548 - val_recon_loss: 1.4408\n",
      "Epoch 112/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0432 - kl_loss: 2.6439 - recon_loss: 1.2501 - val_loss: 2.7783 - val_kl_loss: 2.9964 - val_recon_loss: 1.8794\n",
      "Epoch 113/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0041 - kl_loss: 2.6516 - recon_loss: 1.2086 - val_loss: 2.6196 - val_kl_loss: 3.0677 - val_recon_loss: 1.6993\n",
      "Epoch 114/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0228 - kl_loss: 2.6467 - recon_loss: 1.2288 - val_loss: 2.6688 - val_kl_loss: 3.0329 - val_recon_loss: 1.7589\n",
      "Epoch 115/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0368 - kl_loss: 2.6471 - recon_loss: 1.2427 - val_loss: 2.3269 - val_kl_loss: 2.9596 - val_recon_loss: 1.4390\n",
      "Epoch 116/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.0138 - kl_loss: 2.6662 - recon_loss: 1.2139 - val_loss: 2.6992 - val_kl_loss: 2.8441 - val_recon_loss: 1.8460\n",
      "Epoch 117/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0443 - kl_loss: 2.6431 - recon_loss: 1.2514 - val_loss: 2.5967 - val_kl_loss: 2.9093 - val_recon_loss: 1.7239\n",
      "Epoch 118/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9914 - kl_loss: 2.6162 - recon_loss: 1.2066 - val_loss: 2.9594 - val_kl_loss: 2.7786 - val_recon_loss: 2.1259\n",
      "Epoch 119/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0083 - kl_loss: 2.5872 - recon_loss: 1.2321 - val_loss: 2.6113 - val_kl_loss: 2.7436 - val_recon_loss: 1.7882\n",
      "Epoch 120/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0278 - kl_loss: 2.6358 - recon_loss: 1.2371 - val_loss: 3.0517 - val_kl_loss: 2.9412 - val_recon_loss: 2.1694\n",
      "Epoch 121/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.0078 - kl_loss: 2.6659 - recon_loss: 1.2080 - val_loss: 2.2788 - val_kl_loss: 2.7423 - val_recon_loss: 1.4562\n",
      "Epoch 122/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9907 - kl_loss: 2.5919 - recon_loss: 1.2131 - val_loss: 2.3038 - val_kl_loss: 2.8332 - val_recon_loss: 1.4539\n",
      "Epoch 123/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.9944 - kl_loss: 2.5968 - recon_loss: 1.2154 - val_loss: 2.7829 - val_kl_loss: 2.7170 - val_recon_loss: 1.9678\n",
      "Epoch 124/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9653 - kl_loss: 2.5891 - recon_loss: 1.1886 - val_loss: 2.3390 - val_kl_loss: 2.6749 - val_recon_loss: 1.5365\n",
      "Epoch 125/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0127 - kl_loss: 2.5608 - recon_loss: 1.2445 - val_loss: 2.3498 - val_kl_loss: 3.0010 - val_recon_loss: 1.4495\n",
      "Epoch 126/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9702 - kl_loss: 2.5838 - recon_loss: 1.1950 - val_loss: 2.3855 - val_kl_loss: 2.8134 - val_recon_loss: 1.5415\n",
      "Epoch 127/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.9909 - kl_loss: 2.6081 - recon_loss: 1.2085 - val_loss: 2.7490 - val_kl_loss: 2.8088 - val_recon_loss: 1.9064\n",
      "Epoch 128/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9818 - kl_loss: 2.5572 - recon_loss: 1.2147 - val_loss: 2.2775 - val_kl_loss: 2.8322 - val_recon_loss: 1.4278\n",
      "Epoch 129/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9774 - kl_loss: 2.5925 - recon_loss: 1.1997 - val_loss: 2.4849 - val_kl_loss: 2.8748 - val_recon_loss: 1.6224\n",
      "Epoch 130/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9909 - kl_loss: 2.6010 - recon_loss: 1.2106 - val_loss: 4.0393 - val_kl_loss: 3.0411 - val_recon_loss: 3.1270\n",
      "Epoch 131/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9981 - kl_loss: 2.5901 - recon_loss: 1.2210 - val_loss: 2.2748 - val_kl_loss: 2.7489 - val_recon_loss: 1.4502\n",
      "Epoch 132/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9754 - kl_loss: 2.5798 - recon_loss: 1.2014 - val_loss: 2.9243 - val_kl_loss: 2.8322 - val_recon_loss: 2.0746\n",
      "Epoch 133/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0034 - kl_loss: 2.6270 - recon_loss: 1.2153 - val_loss: 2.6847 - val_kl_loss: 2.8888 - val_recon_loss: 1.8181\n",
      "Epoch 134/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9726 - kl_loss: 2.6052 - recon_loss: 1.1910 - val_loss: 2.7218 - val_kl_loss: 2.6655 - val_recon_loss: 1.9222\n",
      "Epoch 135/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.0022 - kl_loss: 2.6070 - recon_loss: 1.2201 - val_loss: 2.3198 - val_kl_loss: 2.8278 - val_recon_loss: 1.4714\n",
      "Epoch 136/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.9784 - kl_loss: 2.5676 - recon_loss: 1.2082 - val_loss: 2.2546 - val_kl_loss: 2.8335 - val_recon_loss: 1.4045\n",
      "Epoch 137/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.9494 - kl_loss: 2.5696 - recon_loss: 1.1785 - val_loss: 2.5913 - val_kl_loss: 2.6712 - val_recon_loss: 1.7899\n",
      "Epoch 138/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9571 - kl_loss: 2.5898 - recon_loss: 1.1802 - val_loss: 2.8779 - val_kl_loss: 2.8620 - val_recon_loss: 2.0193\n",
      "Epoch 139/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9751 - kl_loss: 2.6084 - recon_loss: 1.1925 - val_loss: 3.0480 - val_kl_loss: 2.9015 - val_recon_loss: 2.1775\n",
      "Epoch 140/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9717 - kl_loss: 2.5700 - recon_loss: 1.2007 - val_loss: 3.1405 - val_kl_loss: 2.7855 - val_recon_loss: 2.3049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200\n",
      "1465/1465 [==============================] - 0s 69us/step - loss: 1.9612 - kl_loss: 2.5568 - recon_loss: 1.1941 - val_loss: 3.2007 - val_kl_loss: 2.9727 - val_recon_loss: 2.3089\n",
      "Epoch 142/200\n",
      "1465/1465 [==============================] - 0s 70us/step - loss: 1.9639 - kl_loss: 2.6027 - recon_loss: 1.1831 - val_loss: 3.4198 - val_kl_loss: 2.6004 - val_recon_loss: 2.6397\n",
      "Epoch 143/200\n",
      "1465/1465 [==============================] - 0s 70us/step - loss: 1.9482 - kl_loss: 2.5651 - recon_loss: 1.1787 - val_loss: 2.5733 - val_kl_loss: 2.7702 - val_recon_loss: 1.7423\n",
      "Epoch 144/200\n",
      "1465/1465 [==============================] - 0s 69us/step - loss: 1.9590 - kl_loss: 2.5776 - recon_loss: 1.1857 - val_loss: 3.5491 - val_kl_loss: 2.8816 - val_recon_loss: 2.6846\n",
      "Epoch 145/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9599 - kl_loss: 2.5938 - recon_loss: 1.1818 - val_loss: 2.3673 - val_kl_loss: 2.7729 - val_recon_loss: 1.5355\n",
      "Epoch 146/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9411 - kl_loss: 2.5555 - recon_loss: 1.1744 - val_loss: 2.2475 - val_kl_loss: 2.7740 - val_recon_loss: 1.4153\n",
      "Epoch 147/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.9405 - kl_loss: 2.5469 - recon_loss: 1.1765 - val_loss: 2.2408 - val_kl_loss: 2.7228 - val_recon_loss: 1.4239\n",
      "Epoch 148/200\n",
      "1465/1465 [==============================] - 0s 68us/step - loss: 1.9440 - kl_loss: 2.5487 - recon_loss: 1.1794 - val_loss: 2.5709 - val_kl_loss: 2.8956 - val_recon_loss: 1.7023\n",
      "Epoch 149/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9617 - kl_loss: 2.5493 - recon_loss: 1.1969 - val_loss: 2.5811 - val_kl_loss: 2.6784 - val_recon_loss: 1.7776\n",
      "Epoch 150/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9443 - kl_loss: 2.5370 - recon_loss: 1.1833 - val_loss: 2.7096 - val_kl_loss: 2.7593 - val_recon_loss: 1.8818\n",
      "Epoch 151/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9646 - kl_loss: 2.5572 - recon_loss: 1.1975 - val_loss: 4.7323 - val_kl_loss: 2.7080 - val_recon_loss: 3.9199\n",
      "Epoch 152/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9444 - kl_loss: 2.5351 - recon_loss: 1.1839 - val_loss: 2.5130 - val_kl_loss: 2.8271 - val_recon_loss: 1.6649\n",
      "Epoch 153/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9360 - kl_loss: 2.5194 - recon_loss: 1.1802 - val_loss: 3.5510 - val_kl_loss: 2.5445 - val_recon_loss: 2.7876\n",
      "Epoch 154/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.9316 - kl_loss: 2.5035 - recon_loss: 1.1806 - val_loss: 3.0568 - val_kl_loss: 2.9382 - val_recon_loss: 2.1753\n",
      "Epoch 155/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9458 - kl_loss: 2.5734 - recon_loss: 1.1738 - val_loss: 3.0477 - val_kl_loss: 2.8927 - val_recon_loss: 2.1799\n",
      "Epoch 156/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9316 - kl_loss: 2.5608 - recon_loss: 1.1633 - val_loss: 2.5139 - val_kl_loss: 2.6206 - val_recon_loss: 1.7277\n",
      "Epoch 157/200\n",
      "1465/1465 [==============================] - 0s 66us/step - loss: 1.9358 - kl_loss: 2.4990 - recon_loss: 1.1861 - val_loss: 2.4555 - val_kl_loss: 2.8453 - val_recon_loss: 1.6020\n",
      "Epoch 158/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9189 - kl_loss: 2.5307 - recon_loss: 1.1597 - val_loss: 2.8119 - val_kl_loss: 2.6028 - val_recon_loss: 2.0310\n",
      "Epoch 159/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.9680 - kl_loss: 2.5463 - recon_loss: 1.2041 - val_loss: 2.5404 - val_kl_loss: 2.9205 - val_recon_loss: 1.6642\n",
      "Epoch 160/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9040 - kl_loss: 2.5168 - recon_loss: 1.1490 - val_loss: 4.0658 - val_kl_loss: 2.7282 - val_recon_loss: 3.2473\n",
      "Epoch 161/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9503 - kl_loss: 2.5571 - recon_loss: 1.1831 - val_loss: 2.5559 - val_kl_loss: 2.7069 - val_recon_loss: 1.7438\n",
      "Epoch 162/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9402 - kl_loss: 2.5378 - recon_loss: 1.1789 - val_loss: 2.2737 - val_kl_loss: 2.7224 - val_recon_loss: 1.4570\n",
      "Epoch 163/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.9335 - kl_loss: 2.5180 - recon_loss: 1.1781 - val_loss: 2.2831 - val_kl_loss: 2.7804 - val_recon_loss: 1.4490\n",
      "Epoch 164/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.9480 - kl_loss: 2.5347 - recon_loss: 1.1876 - val_loss: 2.4685 - val_kl_loss: 2.7799 - val_recon_loss: 1.6346\n",
      "Epoch 165/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9133 - kl_loss: 2.5313 - recon_loss: 1.1540 - val_loss: 2.4008 - val_kl_loss: 2.8835 - val_recon_loss: 1.5358\n",
      "Epoch 166/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9200 - kl_loss: 2.5230 - recon_loss: 1.1631 - val_loss: 3.0739 - val_kl_loss: 2.6333 - val_recon_loss: 2.2839\n",
      "Epoch 167/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9434 - kl_loss: 2.5098 - recon_loss: 1.1905 - val_loss: 2.7401 - val_kl_loss: 2.6947 - val_recon_loss: 1.9317\n",
      "Epoch 168/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9212 - kl_loss: 2.4871 - recon_loss: 1.1750 - val_loss: 2.2684 - val_kl_loss: 2.7162 - val_recon_loss: 1.4535\n",
      "Epoch 169/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9147 - kl_loss: 2.5188 - recon_loss: 1.1591 - val_loss: 2.3881 - val_kl_loss: 2.7981 - val_recon_loss: 1.5487\n",
      "Epoch 170/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9129 - kl_loss: 2.5308 - recon_loss: 1.1536 - val_loss: 2.2257 - val_kl_loss: 2.7909 - val_recon_loss: 1.3884\n",
      "Epoch 171/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8955 - kl_loss: 2.4994 - recon_loss: 1.1457 - val_loss: 2.5017 - val_kl_loss: 2.6497 - val_recon_loss: 1.7068\n",
      "Epoch 172/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9171 - kl_loss: 2.4748 - recon_loss: 1.1746 - val_loss: 2.4192 - val_kl_loss: 2.7027 - val_recon_loss: 1.6084\n",
      "Epoch 173/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8804 - kl_loss: 2.4679 - recon_loss: 1.1401 - val_loss: 2.3834 - val_kl_loss: 2.6933 - val_recon_loss: 1.5754\n",
      "Epoch 174/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9107 - kl_loss: 2.4819 - recon_loss: 1.1661 - val_loss: 2.4675 - val_kl_loss: 2.7515 - val_recon_loss: 1.6421\n",
      "Epoch 175/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9049 - kl_loss: 2.4921 - recon_loss: 1.1573 - val_loss: 2.5370 - val_kl_loss: 2.8202 - val_recon_loss: 1.6910\n",
      "Epoch 176/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9291 - kl_loss: 2.4997 - recon_loss: 1.1792 - val_loss: 2.3193 - val_kl_loss: 2.7107 - val_recon_loss: 1.5061\n",
      "Epoch 177/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9103 - kl_loss: 2.4990 - recon_loss: 1.1606 - val_loss: 2.2742 - val_kl_loss: 2.7555 - val_recon_loss: 1.4475\n",
      "Epoch 178/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9138 - kl_loss: 2.4699 - recon_loss: 1.1729 - val_loss: 2.6272 - val_kl_loss: 2.6549 - val_recon_loss: 1.8307\n",
      "Epoch 179/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8695 - kl_loss: 2.5104 - recon_loss: 1.1164 - val_loss: 3.0052 - val_kl_loss: 2.6432 - val_recon_loss: 2.2123\n",
      "Epoch 180/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9021 - kl_loss: 2.4800 - recon_loss: 1.1581 - val_loss: 3.1933 - val_kl_loss: 2.7024 - val_recon_loss: 2.3826\n",
      "Epoch 181/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9082 - kl_loss: 2.4786 - recon_loss: 1.1646 - val_loss: 2.2325 - val_kl_loss: 2.7622 - val_recon_loss: 1.4038\n",
      "Epoch 182/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9048 - kl_loss: 2.4850 - recon_loss: 1.1593 - val_loss: 2.2158 - val_kl_loss: 2.8037 - val_recon_loss: 1.3747\n",
      "Epoch 183/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8995 - kl_loss: 2.5168 - recon_loss: 1.1444 - val_loss: 2.3545 - val_kl_loss: 2.6863 - val_recon_loss: 1.5486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.8681 - kl_loss: 2.4419 - recon_loss: 1.1356 - val_loss: 2.4202 - val_kl_loss: 2.5771 - val_recon_loss: 1.6471\n",
      "Epoch 185/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8928 - kl_loss: 2.4695 - recon_loss: 1.1520 - val_loss: 2.3307 - val_kl_loss: 2.8269 - val_recon_loss: 1.4826\n",
      "Epoch 186/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9077 - kl_loss: 2.5084 - recon_loss: 1.1552 - val_loss: 2.7440 - val_kl_loss: 2.8791 - val_recon_loss: 1.8803\n",
      "Epoch 187/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9004 - kl_loss: 2.5276 - recon_loss: 1.1422 - val_loss: 2.4228 - val_kl_loss: 2.7444 - val_recon_loss: 1.5995\n",
      "Epoch 188/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8750 - kl_loss: 2.4582 - recon_loss: 1.1375 - val_loss: 2.4562 - val_kl_loss: 2.7168 - val_recon_loss: 1.6412\n",
      "Epoch 189/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8885 - kl_loss: 2.4652 - recon_loss: 1.1489 - val_loss: 3.3651 - val_kl_loss: 2.6097 - val_recon_loss: 2.5822\n",
      "Epoch 190/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8981 - kl_loss: 2.4749 - recon_loss: 1.1557 - val_loss: 2.5165 - val_kl_loss: 2.6851 - val_recon_loss: 1.7110\n",
      "Epoch 191/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8796 - kl_loss: 2.4419 - recon_loss: 1.1470 - val_loss: 2.4747 - val_kl_loss: 2.6476 - val_recon_loss: 1.6804\n",
      "Epoch 192/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8797 - kl_loss: 2.4530 - recon_loss: 1.1438 - val_loss: 2.8819 - val_kl_loss: 2.8335 - val_recon_loss: 2.0318\n",
      "Epoch 193/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8959 - kl_loss: 2.4631 - recon_loss: 1.1570 - val_loss: 2.2950 - val_kl_loss: 2.7680 - val_recon_loss: 1.4646\n",
      "Epoch 194/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8711 - kl_loss: 2.4675 - recon_loss: 1.1308 - val_loss: 4.1838 - val_kl_loss: 2.7892 - val_recon_loss: 3.3470\n",
      "Epoch 195/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8808 - kl_loss: 2.4620 - recon_loss: 1.1422 - val_loss: 3.0208 - val_kl_loss: 2.7903 - val_recon_loss: 2.1837\n",
      "Epoch 196/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8981 - kl_loss: 2.4565 - recon_loss: 1.1612 - val_loss: 2.4979 - val_kl_loss: 2.7247 - val_recon_loss: 1.6805\n",
      "Epoch 197/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8772 - kl_loss: 2.4805 - recon_loss: 1.1330 - val_loss: 2.7412 - val_kl_loss: 2.7360 - val_recon_loss: 1.9204\n",
      "Epoch 198/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8795 - kl_loss: 2.4483 - recon_loss: 1.1450 - val_loss: 2.3923 - val_kl_loss: 2.6922 - val_recon_loss: 1.5846\n",
      "Epoch 199/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.8555 - kl_loss: 2.4224 - recon_loss: 1.1288 - val_loss: 2.7065 - val_kl_loss: 2.8152 - val_recon_loss: 1.8620\n",
      "Epoch 200/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8814 - kl_loss: 2.4421 - recon_loss: 1.1487 - val_loss: 3.2575 - val_kl_loss: 2.7757 - val_recon_loss: 2.4248\n",
      "========================= Model 2 =========================\n",
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6856        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6420        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 13,276\n",
      "Trainable params: 13,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_2 (Dense)             (None, 12)           300         enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            26          enc_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            26          enc_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,856\n",
      "Trainable params: 6,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 12)           204         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 24)           312         dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_2 (Dense)             (None, 48)           1200        dec_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,420\n",
      "Trainable params: 6,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1465 samples, validate on 365 samples\n",
      "Epoch 1/200\n",
      "1465/1465 [==============================] - 0s 152us/step - loss: 45.1249 - kl_loss: 14.2835 - recon_loss: 40.8399 - val_loss: 11.1516 - val_kl_loss: 11.5815 - val_recon_loss: 7.6772\n",
      "Epoch 2/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 8.1910 - kl_loss: 9.8796 - recon_loss: 5.2272 - val_loss: 6.1320 - val_kl_loss: 7.4091 - val_recon_loss: 3.9093\n",
      "Epoch 3/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 5.9771 - kl_loss: 7.1773 - recon_loss: 3.8239 - val_loss: 5.1529 - val_kl_loss: 6.4444 - val_recon_loss: 3.2195\n",
      "Epoch 4/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 5.1544 - kl_loss: 6.1332 - recon_loss: 3.3144 - val_loss: 4.7284 - val_kl_loss: 5.7179 - val_recon_loss: 3.0130\n",
      "Epoch 5/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 4.6331 - kl_loss: 5.4432 - recon_loss: 3.0001 - val_loss: 4.8041 - val_kl_loss: 5.2958 - val_recon_loss: 3.2154\n",
      "Epoch 6/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 4.2798 - kl_loss: 5.1804 - recon_loss: 2.7257 - val_loss: 4.3336 - val_kl_loss: 4.8524 - val_recon_loss: 2.8779\n",
      "Epoch 7/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 4.0729 - kl_loss: 4.8915 - recon_loss: 2.6054 - val_loss: 3.9659 - val_kl_loss: 4.8387 - val_recon_loss: 2.5143\n",
      "Epoch 8/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 3.8629 - kl_loss: 4.7332 - recon_loss: 2.4429 - val_loss: 3.4519 - val_kl_loss: 4.3927 - val_recon_loss: 2.1341\n",
      "Epoch 9/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.6559 - kl_loss: 4.6158 - recon_loss: 2.2712 - val_loss: 3.8711 - val_kl_loss: 4.1037 - val_recon_loss: 2.6399\n",
      "Epoch 10/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 3.4987 - kl_loss: 4.4542 - recon_loss: 2.1624 - val_loss: 3.9265 - val_kl_loss: 4.1301 - val_recon_loss: 2.6875\n",
      "Epoch 11/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 3.3574 - kl_loss: 4.3164 - recon_loss: 2.0624 - val_loss: 3.4495 - val_kl_loss: 4.2615 - val_recon_loss: 2.1710\n",
      "Epoch 12/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 3.3027 - kl_loss: 4.2319 - recon_loss: 2.0331 - val_loss: 3.6056 - val_kl_loss: 3.9983 - val_recon_loss: 2.4061\n",
      "Epoch 13/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 3.1677 - kl_loss: 4.1715 - recon_loss: 1.9163 - val_loss: 2.7916 - val_kl_loss: 3.9274 - val_recon_loss: 1.6134\n",
      "Epoch 14/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 3.0968 - kl_loss: 4.0886 - recon_loss: 1.8703 - val_loss: 4.7418 - val_kl_loss: 3.9935 - val_recon_loss: 3.5437\n",
      "Epoch 15/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 3.0549 - kl_loss: 4.0538 - recon_loss: 1.8387 - val_loss: 3.6700 - val_kl_loss: 3.8292 - val_recon_loss: 2.5212\n",
      "Epoch 16/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.8886 - kl_loss: 3.9406 - recon_loss: 1.7064 - val_loss: 2.9768 - val_kl_loss: 3.9762 - val_recon_loss: 1.7840\n",
      "Epoch 17/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.9697 - kl_loss: 3.9577 - recon_loss: 1.7824 - val_loss: 3.0611 - val_kl_loss: 4.0167 - val_recon_loss: 1.8561\n",
      "Epoch 18/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.8825 - kl_loss: 3.9799 - recon_loss: 1.6886 - val_loss: 3.4646 - val_kl_loss: 3.7015 - val_recon_loss: 2.3541\n",
      "Epoch 19/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.7960 - kl_loss: 3.8964 - recon_loss: 1.6271 - val_loss: 3.4495 - val_kl_loss: 3.7809 - val_recon_loss: 2.3152\n",
      "Epoch 20/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.7659 - kl_loss: 3.8961 - recon_loss: 1.5971 - val_loss: 2.4946 - val_kl_loss: 3.8830 - val_recon_loss: 1.3297\n",
      "Epoch 21/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 2.7396 - kl_loss: 3.8503 - recon_loss: 1.5845 - val_loss: 2.7288 - val_kl_loss: 3.8108 - val_recon_loss: 1.5855\n",
      "Epoch 22/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.6589 - kl_loss: 3.7992 - recon_loss: 1.5191 - val_loss: 3.2602 - val_kl_loss: 3.7021 - val_recon_loss: 2.1496\n",
      "Epoch 23/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.6276 - kl_loss: 3.8634 - recon_loss: 1.4686 - val_loss: 2.5474 - val_kl_loss: 3.8431 - val_recon_loss: 1.3944\n",
      "Epoch 24/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.6681 - kl_loss: 3.8919 - recon_loss: 1.5005 - val_loss: 2.7068 - val_kl_loss: 3.7869 - val_recon_loss: 1.5707\n",
      "Epoch 25/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.6588 - kl_loss: 3.7777 - recon_loss: 1.5255 - val_loss: 2.3492 - val_kl_loss: 3.7563 - val_recon_loss: 1.2223\n",
      "Epoch 26/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.5303 - kl_loss: 3.8144 - recon_loss: 1.3859 - val_loss: 2.5059 - val_kl_loss: 3.7040 - val_recon_loss: 1.3947\n",
      "Epoch 27/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.5391 - kl_loss: 3.7461 - recon_loss: 1.4153 - val_loss: 2.6267 - val_kl_loss: 3.6861 - val_recon_loss: 1.5209\n",
      "Epoch 28/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.5392 - kl_loss: 3.7922 - recon_loss: 1.4015 - val_loss: 2.8528 - val_kl_loss: 3.7860 - val_recon_loss: 1.7171\n",
      "Epoch 29/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.5267 - kl_loss: 3.7760 - recon_loss: 1.3939 - val_loss: 2.7235 - val_kl_loss: 3.3502 - val_recon_loss: 1.7185\n",
      "Epoch 30/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 2.4790 - kl_loss: 3.6939 - recon_loss: 1.3709 - val_loss: 2.4055 - val_kl_loss: 3.7864 - val_recon_loss: 1.2695\n",
      "Epoch 31/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.4247 - kl_loss: 3.6705 - recon_loss: 1.3236 - val_loss: 2.9556 - val_kl_loss: 3.6807 - val_recon_loss: 1.8514\n",
      "Epoch 32/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.4424 - kl_loss: 3.6636 - recon_loss: 1.3433 - val_loss: 2.2395 - val_kl_loss: 3.6451 - val_recon_loss: 1.1460\n",
      "Epoch 33/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.4538 - kl_loss: 3.6781 - recon_loss: 1.3504 - val_loss: 2.5433 - val_kl_loss: 3.5924 - val_recon_loss: 1.4656\n",
      "Epoch 34/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.3804 - kl_loss: 3.6348 - recon_loss: 1.2899 - val_loss: 2.3220 - val_kl_loss: 3.5567 - val_recon_loss: 1.2550\n",
      "Epoch 35/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.3653 - kl_loss: 3.6450 - recon_loss: 1.2718 - val_loss: 3.0536 - val_kl_loss: 3.8704 - val_recon_loss: 1.8925\n",
      "Epoch 36/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.4020 - kl_loss: 3.6358 - recon_loss: 1.3112 - val_loss: 2.5905 - val_kl_loss: 3.4459 - val_recon_loss: 1.5567\n",
      "Epoch 37/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3731 - kl_loss: 3.6179 - recon_loss: 1.2877 - val_loss: 2.8066 - val_kl_loss: 3.5865 - val_recon_loss: 1.7306\n",
      "Epoch 38/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 2.3305 - kl_loss: 3.6413 - recon_loss: 1.2381 - val_loss: 2.3486 - val_kl_loss: 3.5948 - val_recon_loss: 1.2702\n",
      "Epoch 39/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.2637 - kl_loss: 3.5444 - recon_loss: 1.2004 - val_loss: 2.5763 - val_kl_loss: 3.2314 - val_recon_loss: 1.6069\n",
      "Epoch 40/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2909 - kl_loss: 3.5285 - recon_loss: 1.2323 - val_loss: 2.2609 - val_kl_loss: 3.4226 - val_recon_loss: 1.2341\n",
      "Epoch 41/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.3027 - kl_loss: 3.4748 - recon_loss: 1.2603 - val_loss: 2.9909 - val_kl_loss: 3.4023 - val_recon_loss: 1.9702\n",
      "Epoch 42/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2831 - kl_loss: 3.4957 - recon_loss: 1.2344 - val_loss: 2.2216 - val_kl_loss: 3.3738 - val_recon_loss: 1.2095\n",
      "Epoch 43/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2148 - kl_loss: 3.4442 - recon_loss: 1.1815 - val_loss: 2.5120 - val_kl_loss: 3.3534 - val_recon_loss: 1.5059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 2.2559 - kl_loss: 3.4851 - recon_loss: 1.2103 - val_loss: 2.8241 - val_kl_loss: 3.4358 - val_recon_loss: 1.7933\n",
      "Epoch 45/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1938 - kl_loss: 3.4412 - recon_loss: 1.1615 - val_loss: 3.3960 - val_kl_loss: 3.4588 - val_recon_loss: 2.3584\n",
      "Epoch 46/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.2560 - kl_loss: 3.4542 - recon_loss: 1.2198 - val_loss: 2.4868 - val_kl_loss: 3.6142 - val_recon_loss: 1.4026\n",
      "Epoch 47/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.2263 - kl_loss: 3.4809 - recon_loss: 1.1820 - val_loss: 2.3408 - val_kl_loss: 3.3000 - val_recon_loss: 1.3508\n",
      "Epoch 48/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.2235 - kl_loss: 3.4300 - recon_loss: 1.1945 - val_loss: 2.4176 - val_kl_loss: 3.3672 - val_recon_loss: 1.4074\n",
      "Epoch 49/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1621 - kl_loss: 3.3825 - recon_loss: 1.1473 - val_loss: 2.2190 - val_kl_loss: 3.2830 - val_recon_loss: 1.2341\n",
      "Epoch 50/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1805 - kl_loss: 3.3469 - recon_loss: 1.1765 - val_loss: 3.1580 - val_kl_loss: 3.3041 - val_recon_loss: 2.1668\n",
      "Epoch 51/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1537 - kl_loss: 3.3744 - recon_loss: 1.1414 - val_loss: 3.6536 - val_kl_loss: 3.2631 - val_recon_loss: 2.6746\n",
      "Epoch 52/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.1273 - kl_loss: 3.3741 - recon_loss: 1.1150 - val_loss: 2.8974 - val_kl_loss: 3.2611 - val_recon_loss: 1.9191\n",
      "Epoch 53/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.2006 - kl_loss: 3.3769 - recon_loss: 1.1875 - val_loss: 2.0526 - val_kl_loss: 3.2944 - val_recon_loss: 1.0643\n",
      "Epoch 54/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1068 - kl_loss: 3.3723 - recon_loss: 1.0951 - val_loss: 2.2318 - val_kl_loss: 3.2121 - val_recon_loss: 1.2681\n",
      "Epoch 55/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1285 - kl_loss: 3.3022 - recon_loss: 1.1379 - val_loss: 2.2988 - val_kl_loss: 2.9300 - val_recon_loss: 1.4198\n",
      "Epoch 56/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.0965 - kl_loss: 3.2574 - recon_loss: 1.1193 - val_loss: 4.2585 - val_kl_loss: 3.0030 - val_recon_loss: 3.3576\n",
      "Epoch 57/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.1157 - kl_loss: 3.2807 - recon_loss: 1.1315 - val_loss: 2.1454 - val_kl_loss: 3.2480 - val_recon_loss: 1.1710\n",
      "Epoch 58/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1033 - kl_loss: 3.3299 - recon_loss: 1.1043 - val_loss: 2.5633 - val_kl_loss: 3.1459 - val_recon_loss: 1.6195\n",
      "Epoch 59/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1208 - kl_loss: 3.2595 - recon_loss: 1.1429 - val_loss: 2.5648 - val_kl_loss: 3.2801 - val_recon_loss: 1.5808\n",
      "Epoch 60/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0751 - kl_loss: 3.2436 - recon_loss: 1.1020 - val_loss: 2.3037 - val_kl_loss: 3.3938 - val_recon_loss: 1.2855\n",
      "Epoch 61/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.0984 - kl_loss: 3.2469 - recon_loss: 1.1243 - val_loss: 2.7906 - val_kl_loss: 3.1777 - val_recon_loss: 1.8373\n",
      "Epoch 62/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0880 - kl_loss: 3.2634 - recon_loss: 1.1090 - val_loss: 2.1057 - val_kl_loss: 3.1103 - val_recon_loss: 1.1726\n",
      "Epoch 63/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.0697 - kl_loss: 3.2492 - recon_loss: 1.0950 - val_loss: 3.4762 - val_kl_loss: 3.3305 - val_recon_loss: 2.4771\n",
      "Epoch 64/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.0978 - kl_loss: 3.3047 - recon_loss: 1.1064 - val_loss: 2.1989 - val_kl_loss: 3.1934 - val_recon_loss: 1.2409\n",
      "Epoch 65/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0212 - kl_loss: 3.1492 - recon_loss: 1.0765 - val_loss: 2.1706 - val_kl_loss: 2.9259 - val_recon_loss: 1.2928\n",
      "Epoch 66/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 2.0540 - kl_loss: 3.1939 - recon_loss: 1.0958 - val_loss: 2.1766 - val_kl_loss: 3.2114 - val_recon_loss: 1.2131\n",
      "Epoch 67/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0016 - kl_loss: 3.1510 - recon_loss: 1.0563 - val_loss: 4.1817 - val_kl_loss: 3.1923 - val_recon_loss: 3.2241\n",
      "Epoch 68/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0316 - kl_loss: 3.1850 - recon_loss: 1.0761 - val_loss: 2.3365 - val_kl_loss: 2.9038 - val_recon_loss: 1.4654\n",
      "Epoch 69/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0328 - kl_loss: 3.1525 - recon_loss: 1.0870 - val_loss: 2.1601 - val_kl_loss: 3.1798 - val_recon_loss: 1.2061\n",
      "Epoch 70/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.9992 - kl_loss: 3.1609 - recon_loss: 1.0510 - val_loss: 3.2547 - val_kl_loss: 3.1271 - val_recon_loss: 2.3166\n",
      "Epoch 71/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.0022 - kl_loss: 3.1215 - recon_loss: 1.0658 - val_loss: 2.0710 - val_kl_loss: 3.0970 - val_recon_loss: 1.1419\n",
      "Epoch 72/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.9996 - kl_loss: 3.0999 - recon_loss: 1.0697 - val_loss: 2.1517 - val_kl_loss: 3.1168 - val_recon_loss: 1.2167\n",
      "Epoch 73/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9924 - kl_loss: 3.1196 - recon_loss: 1.0566 - val_loss: 3.2288 - val_kl_loss: 3.2889 - val_recon_loss: 2.2421\n",
      "Epoch 74/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0100 - kl_loss: 3.1181 - recon_loss: 1.0746 - val_loss: 2.0518 - val_kl_loss: 2.9928 - val_recon_loss: 1.1540\n",
      "Epoch 75/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9835 - kl_loss: 3.1133 - recon_loss: 1.0495 - val_loss: 2.3328 - val_kl_loss: 3.1453 - val_recon_loss: 1.3893\n",
      "Epoch 76/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.9284 - kl_loss: 3.0477 - recon_loss: 1.0140 - val_loss: 2.9777 - val_kl_loss: 2.7835 - val_recon_loss: 2.1427\n",
      "Epoch 77/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0091 - kl_loss: 3.1487 - recon_loss: 1.0645 - val_loss: 2.1626 - val_kl_loss: 3.1110 - val_recon_loss: 1.2293\n",
      "Epoch 78/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9416 - kl_loss: 3.0587 - recon_loss: 1.0240 - val_loss: 3.1354 - val_kl_loss: 3.0012 - val_recon_loss: 2.2350\n",
      "Epoch 79/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9788 - kl_loss: 3.1155 - recon_loss: 1.0442 - val_loss: 2.5379 - val_kl_loss: 2.9147 - val_recon_loss: 1.6635\n",
      "Epoch 80/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9203 - kl_loss: 3.0904 - recon_loss: 0.9932 - val_loss: 2.0958 - val_kl_loss: 2.9360 - val_recon_loss: 1.2150\n",
      "Epoch 81/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9587 - kl_loss: 3.1051 - recon_loss: 1.0272 - val_loss: 2.1671 - val_kl_loss: 2.8967 - val_recon_loss: 1.2981\n",
      "Epoch 82/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9250 - kl_loss: 3.0584 - recon_loss: 1.0075 - val_loss: 2.4632 - val_kl_loss: 3.1579 - val_recon_loss: 1.5159\n",
      "Epoch 83/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9637 - kl_loss: 3.0751 - recon_loss: 1.0411 - val_loss: 2.4039 - val_kl_loss: 2.8336 - val_recon_loss: 1.5538\n",
      "Epoch 84/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.9575 - kl_loss: 3.1040 - recon_loss: 1.0263 - val_loss: 2.3426 - val_kl_loss: 2.9042 - val_recon_loss: 1.4713\n",
      "Epoch 85/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8974 - kl_loss: 3.0375 - recon_loss: 0.9861 - val_loss: 2.0616 - val_kl_loss: 2.9943 - val_recon_loss: 1.1633\n",
      "Epoch 86/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.9491 - kl_loss: 3.0739 - recon_loss: 1.0269 - val_loss: 2.0392 - val_kl_loss: 3.0979 - val_recon_loss: 1.1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9043 - kl_loss: 3.0886 - recon_loss: 0.9778 - val_loss: 2.8078 - val_kl_loss: 3.1307 - val_recon_loss: 1.8685\n",
      "Epoch 88/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.9656 - kl_loss: 3.1140 - recon_loss: 1.0314 - val_loss: 2.2148 - val_kl_loss: 3.1182 - val_recon_loss: 1.2793\n",
      "Epoch 89/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8996 - kl_loss: 3.0715 - recon_loss: 0.9781 - val_loss: 2.9241 - val_kl_loss: 2.8863 - val_recon_loss: 2.0582\n",
      "Epoch 90/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9017 - kl_loss: 2.9994 - recon_loss: 1.0019 - val_loss: 2.6858 - val_kl_loss: 2.8272 - val_recon_loss: 1.8377\n",
      "Epoch 91/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8985 - kl_loss: 2.9902 - recon_loss: 1.0014 - val_loss: 2.4304 - val_kl_loss: 2.8433 - val_recon_loss: 1.5774\n",
      "Epoch 92/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8871 - kl_loss: 3.0013 - recon_loss: 0.9868 - val_loss: 2.2774 - val_kl_loss: 2.8092 - val_recon_loss: 1.4346\n",
      "Epoch 93/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.9240 - kl_loss: 3.0066 - recon_loss: 1.0221 - val_loss: 2.1971 - val_kl_loss: 3.0059 - val_recon_loss: 1.2953\n",
      "Epoch 94/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8888 - kl_loss: 3.0086 - recon_loss: 0.9862 - val_loss: 2.2263 - val_kl_loss: 3.0088 - val_recon_loss: 1.3237\n",
      "Epoch 95/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8946 - kl_loss: 3.0463 - recon_loss: 0.9807 - val_loss: 2.0734 - val_kl_loss: 3.0327 - val_recon_loss: 1.1636\n",
      "Epoch 96/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8688 - kl_loss: 2.9252 - recon_loss: 0.9912 - val_loss: 2.6167 - val_kl_loss: 2.9187 - val_recon_loss: 1.7411\n",
      "Epoch 97/200\n",
      "1465/1465 [==============================] - 0s 89us/step - loss: 1.9005 - kl_loss: 3.0196 - recon_loss: 0.9947 - val_loss: 2.2292 - val_kl_loss: 3.0943 - val_recon_loss: 1.3009\n",
      "Epoch 98/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8621 - kl_loss: 2.9743 - recon_loss: 0.9698 - val_loss: 2.1242 - val_kl_loss: 2.9207 - val_recon_loss: 1.2480\n",
      "Epoch 99/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8331 - kl_loss: 2.9663 - recon_loss: 0.9432 - val_loss: 2.4875 - val_kl_loss: 2.9124 - val_recon_loss: 1.6138\n",
      "Epoch 100/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8545 - kl_loss: 2.9792 - recon_loss: 0.9607 - val_loss: 2.8891 - val_kl_loss: 3.0157 - val_recon_loss: 1.9844\n",
      "Epoch 101/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8670 - kl_loss: 2.9716 - recon_loss: 0.9755 - val_loss: 2.1491 - val_kl_loss: 2.8960 - val_recon_loss: 1.2803\n",
      "Epoch 102/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8518 - kl_loss: 2.9764 - recon_loss: 0.9588 - val_loss: 2.1941 - val_kl_loss: 2.9227 - val_recon_loss: 1.3173\n",
      "Epoch 103/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8795 - kl_loss: 3.0364 - recon_loss: 0.9686 - val_loss: 1.9847 - val_kl_loss: 2.9269 - val_recon_loss: 1.1066\n",
      "Epoch 104/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8539 - kl_loss: 2.9599 - recon_loss: 0.9659 - val_loss: 2.0867 - val_kl_loss: 2.8252 - val_recon_loss: 1.2391\n",
      "Epoch 105/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8248 - kl_loss: 2.9386 - recon_loss: 0.9432 - val_loss: 2.3585 - val_kl_loss: 2.9169 - val_recon_loss: 1.4835\n",
      "Epoch 106/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8388 - kl_loss: 2.9864 - recon_loss: 0.9428 - val_loss: 1.9567 - val_kl_loss: 2.9975 - val_recon_loss: 1.0575\n",
      "Epoch 107/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8740 - kl_loss: 2.9758 - recon_loss: 0.9813 - val_loss: 2.5579 - val_kl_loss: 2.9503 - val_recon_loss: 1.6728\n",
      "Epoch 108/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.8251 - kl_loss: 2.9699 - recon_loss: 0.9341 - val_loss: 1.8975 - val_kl_loss: 2.7448 - val_recon_loss: 1.0741\n",
      "Epoch 109/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8224 - kl_loss: 2.9024 - recon_loss: 0.9517 - val_loss: 2.1668 - val_kl_loss: 2.8182 - val_recon_loss: 1.3214\n",
      "Epoch 110/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8577 - kl_loss: 2.9332 - recon_loss: 0.9778 - val_loss: 1.9530 - val_kl_loss: 2.7631 - val_recon_loss: 1.1241\n",
      "Epoch 111/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.7888 - kl_loss: 2.8991 - recon_loss: 0.9191 - val_loss: 1.9967 - val_kl_loss: 2.7308 - val_recon_loss: 1.1775\n",
      "Epoch 112/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8367 - kl_loss: 2.9379 - recon_loss: 0.9553 - val_loss: 2.1342 - val_kl_loss: 2.9792 - val_recon_loss: 1.2405\n",
      "Epoch 113/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7808 - kl_loss: 2.9158 - recon_loss: 0.9061 - val_loss: 2.0634 - val_kl_loss: 2.8254 - val_recon_loss: 1.2158\n",
      "Epoch 114/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8295 - kl_loss: 2.9429 - recon_loss: 0.9467 - val_loss: 2.3416 - val_kl_loss: 3.0643 - val_recon_loss: 1.4223\n",
      "Epoch 115/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8209 - kl_loss: 2.9321 - recon_loss: 0.9413 - val_loss: 2.0355 - val_kl_loss: 2.7942 - val_recon_loss: 1.1972\n",
      "Epoch 116/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7947 - kl_loss: 2.9031 - recon_loss: 0.9237 - val_loss: 2.2313 - val_kl_loss: 2.7533 - val_recon_loss: 1.4053\n",
      "Epoch 117/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7896 - kl_loss: 2.8945 - recon_loss: 0.9212 - val_loss: 2.1174 - val_kl_loss: 2.7834 - val_recon_loss: 1.2824\n",
      "Epoch 118/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8367 - kl_loss: 2.8724 - recon_loss: 0.9750 - val_loss: 2.2794 - val_kl_loss: 2.9019 - val_recon_loss: 1.4088\n",
      "Epoch 119/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7963 - kl_loss: 2.8911 - recon_loss: 0.9290 - val_loss: 1.9456 - val_kl_loss: 2.8016 - val_recon_loss: 1.1051\n",
      "Epoch 120/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8102 - kl_loss: 2.9333 - recon_loss: 0.9302 - val_loss: 1.9226 - val_kl_loss: 2.8226 - val_recon_loss: 1.0758\n",
      "Epoch 121/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8117 - kl_loss: 2.8911 - recon_loss: 0.9444 - val_loss: 2.5257 - val_kl_loss: 2.9616 - val_recon_loss: 1.6373\n",
      "Epoch 122/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7842 - kl_loss: 2.9304 - recon_loss: 0.9051 - val_loss: 2.1086 - val_kl_loss: 2.7175 - val_recon_loss: 1.2933\n",
      "Epoch 123/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7905 - kl_loss: 2.8859 - recon_loss: 0.9247 - val_loss: 2.0214 - val_kl_loss: 2.9734 - val_recon_loss: 1.1294\n",
      "Epoch 124/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7852 - kl_loss: 2.9115 - recon_loss: 0.9117 - val_loss: 1.8933 - val_kl_loss: 2.9468 - val_recon_loss: 1.0093\n",
      "Epoch 125/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8072 - kl_loss: 2.9518 - recon_loss: 0.9216 - val_loss: 1.9999 - val_kl_loss: 2.8882 - val_recon_loss: 1.1335\n",
      "Epoch 126/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7991 - kl_loss: 2.8869 - recon_loss: 0.9330 - val_loss: 2.0069 - val_kl_loss: 2.7358 - val_recon_loss: 1.1862\n",
      "Epoch 127/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7993 - kl_loss: 2.8898 - recon_loss: 0.9323 - val_loss: 2.3855 - val_kl_loss: 2.9444 - val_recon_loss: 1.5021\n",
      "Epoch 128/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7676 - kl_loss: 2.8761 - recon_loss: 0.9047 - val_loss: 2.4598 - val_kl_loss: 2.9659 - val_recon_loss: 1.5700\n",
      "Epoch 129/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7674 - kl_loss: 2.8454 - recon_loss: 0.9138 - val_loss: 2.4397 - val_kl_loss: 2.9191 - val_recon_loss: 1.5640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7817 - kl_loss: 2.8937 - recon_loss: 0.9136 - val_loss: 2.1936 - val_kl_loss: 2.7856 - val_recon_loss: 1.3579\n",
      "Epoch 131/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7759 - kl_loss: 2.8766 - recon_loss: 0.9129 - val_loss: 1.9220 - val_kl_loss: 2.8530 - val_recon_loss: 1.0661\n",
      "Epoch 132/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7637 - kl_loss: 2.8323 - recon_loss: 0.9140 - val_loss: 2.6398 - val_kl_loss: 2.8081 - val_recon_loss: 1.7974\n",
      "Epoch 133/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7508 - kl_loss: 2.8648 - recon_loss: 0.8914 - val_loss: 2.9811 - val_kl_loss: 2.8414 - val_recon_loss: 2.1287\n",
      "Epoch 134/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7643 - kl_loss: 2.8516 - recon_loss: 0.9089 - val_loss: 2.8036 - val_kl_loss: 2.7866 - val_recon_loss: 1.9676\n",
      "Epoch 135/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7871 - kl_loss: 2.8458 - recon_loss: 0.9334 - val_loss: 2.2566 - val_kl_loss: 2.7177 - val_recon_loss: 1.4413\n",
      "Epoch 136/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7496 - kl_loss: 2.8468 - recon_loss: 0.8955 - val_loss: 1.9276 - val_kl_loss: 2.8655 - val_recon_loss: 1.0679\n",
      "Epoch 137/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7491 - kl_loss: 2.8250 - recon_loss: 0.9016 - val_loss: 1.9476 - val_kl_loss: 2.8361 - val_recon_loss: 1.0968\n",
      "Epoch 138/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7640 - kl_loss: 2.8130 - recon_loss: 0.9201 - val_loss: 2.6097 - val_kl_loss: 2.8258 - val_recon_loss: 1.7619\n",
      "Epoch 139/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7222 - kl_loss: 2.8448 - recon_loss: 0.8688 - val_loss: 2.1363 - val_kl_loss: 2.7303 - val_recon_loss: 1.3172\n",
      "Epoch 140/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7513 - kl_loss: 2.7850 - recon_loss: 0.9159 - val_loss: 2.4468 - val_kl_loss: 2.7045 - val_recon_loss: 1.6354\n",
      "Epoch 141/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7435 - kl_loss: 2.8203 - recon_loss: 0.8974 - val_loss: 2.2654 - val_kl_loss: 2.6788 - val_recon_loss: 1.4618\n",
      "Epoch 142/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7463 - kl_loss: 2.8185 - recon_loss: 0.9008 - val_loss: 2.3085 - val_kl_loss: 2.7123 - val_recon_loss: 1.4948\n",
      "Epoch 143/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7003 - kl_loss: 2.7504 - recon_loss: 0.8752 - val_loss: 1.9678 - val_kl_loss: 2.7183 - val_recon_loss: 1.1523\n",
      "Epoch 144/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7723 - kl_loss: 2.8235 - recon_loss: 0.9253 - val_loss: 1.9546 - val_kl_loss: 2.8331 - val_recon_loss: 1.1047\n",
      "Epoch 145/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7383 - kl_loss: 2.8075 - recon_loss: 0.8960 - val_loss: 2.0357 - val_kl_loss: 2.7086 - val_recon_loss: 1.2231\n",
      "Epoch 146/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7204 - kl_loss: 2.7733 - recon_loss: 0.8884 - val_loss: 1.9309 - val_kl_loss: 2.8187 - val_recon_loss: 1.0853\n",
      "Epoch 147/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7282 - kl_loss: 2.7996 - recon_loss: 0.8884 - val_loss: 2.2206 - val_kl_loss: 2.7937 - val_recon_loss: 1.3825\n",
      "Epoch 148/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7404 - kl_loss: 2.8106 - recon_loss: 0.8973 - val_loss: 1.8884 - val_kl_loss: 2.8610 - val_recon_loss: 1.0301\n",
      "Epoch 149/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7208 - kl_loss: 2.7906 - recon_loss: 0.8836 - val_loss: 2.3297 - val_kl_loss: 2.7521 - val_recon_loss: 1.5041\n",
      "Epoch 150/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7234 - kl_loss: 2.7858 - recon_loss: 0.8876 - val_loss: 1.8913 - val_kl_loss: 2.7175 - val_recon_loss: 1.0760\n",
      "Epoch 151/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.7181 - kl_loss: 2.7668 - recon_loss: 0.8881 - val_loss: 2.4479 - val_kl_loss: 2.7620 - val_recon_loss: 1.6193\n",
      "Epoch 152/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7315 - kl_loss: 2.8223 - recon_loss: 0.8848 - val_loss: 2.0735 - val_kl_loss: 2.7681 - val_recon_loss: 1.2431\n",
      "Epoch 153/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7265 - kl_loss: 2.7536 - recon_loss: 0.9004 - val_loss: 1.9836 - val_kl_loss: 2.7023 - val_recon_loss: 1.1729\n",
      "Epoch 154/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7496 - kl_loss: 2.7881 - recon_loss: 0.9131 - val_loss: 2.0759 - val_kl_loss: 2.7426 - val_recon_loss: 1.2531\n",
      "Epoch 155/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.7575 - kl_loss: 2.8468 - recon_loss: 0.9034 - val_loss: 1.9892 - val_kl_loss: 2.7737 - val_recon_loss: 1.1571\n",
      "Epoch 156/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7311 - kl_loss: 2.8182 - recon_loss: 0.8856 - val_loss: 2.0890 - val_kl_loss: 2.8171 - val_recon_loss: 1.2439\n",
      "Epoch 157/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.7499 - kl_loss: 2.7862 - recon_loss: 0.9140 - val_loss: 1.8936 - val_kl_loss: 2.7414 - val_recon_loss: 1.0712\n",
      "Epoch 158/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7279 - kl_loss: 2.7826 - recon_loss: 0.8931 - val_loss: 1.9809 - val_kl_loss: 2.6636 - val_recon_loss: 1.1818\n",
      "Epoch 159/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7095 - kl_loss: 2.7468 - recon_loss: 0.8855 - val_loss: 2.0920 - val_kl_loss: 2.6266 - val_recon_loss: 1.3040\n",
      "Epoch 160/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7126 - kl_loss: 2.7605 - recon_loss: 0.8845 - val_loss: 2.4784 - val_kl_loss: 2.5780 - val_recon_loss: 1.7049\n",
      "Epoch 161/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7597 - kl_loss: 2.8072 - recon_loss: 0.9175 - val_loss: 2.1123 - val_kl_loss: 2.7771 - val_recon_loss: 1.2791\n",
      "Epoch 162/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.6893 - kl_loss: 2.7321 - recon_loss: 0.8696 - val_loss: 1.9860 - val_kl_loss: 2.7524 - val_recon_loss: 1.1603\n",
      "Epoch 163/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7135 - kl_loss: 2.7510 - recon_loss: 0.8882 - val_loss: 2.4277 - val_kl_loss: 2.9477 - val_recon_loss: 1.5434\n",
      "Epoch 164/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7322 - kl_loss: 2.7799 - recon_loss: 0.8982 - val_loss: 2.0768 - val_kl_loss: 2.7501 - val_recon_loss: 1.2518\n",
      "Epoch 165/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.6959 - kl_loss: 2.7899 - recon_loss: 0.8590 - val_loss: 2.4123 - val_kl_loss: 2.7536 - val_recon_loss: 1.5862\n",
      "Epoch 166/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7352 - kl_loss: 2.7563 - recon_loss: 0.9083 - val_loss: 1.9790 - val_kl_loss: 2.8188 - val_recon_loss: 1.1333\n",
      "Epoch 167/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.6848 - kl_loss: 2.7248 - recon_loss: 0.8674 - val_loss: 2.0726 - val_kl_loss: 2.6634 - val_recon_loss: 1.2736\n",
      "Epoch 168/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7056 - kl_loss: 2.7328 - recon_loss: 0.8857 - val_loss: 2.1721 - val_kl_loss: 2.7600 - val_recon_loss: 1.3442\n",
      "Epoch 169/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.6975 - kl_loss: 2.7499 - recon_loss: 0.8725 - val_loss: 2.3222 - val_kl_loss: 2.7178 - val_recon_loss: 1.5069\n",
      "Epoch 170/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7145 - kl_loss: 2.7637 - recon_loss: 0.8854 - val_loss: 2.2232 - val_kl_loss: 2.6780 - val_recon_loss: 1.4198\n",
      "Epoch 171/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.6954 - kl_loss: 2.7394 - recon_loss: 0.8736 - val_loss: 2.0531 - val_kl_loss: 2.6761 - val_recon_loss: 1.2502\n",
      "Epoch 172/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7079 - kl_loss: 2.6861 - recon_loss: 0.9021 - val_loss: 1.9348 - val_kl_loss: 2.7150 - val_recon_loss: 1.1203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.6764 - kl_loss: 2.7426 - recon_loss: 0.8536 - val_loss: 2.3138 - val_kl_loss: 2.6607 - val_recon_loss: 1.5156\n",
      "Epoch 174/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.7034 - kl_loss: 2.7573 - recon_loss: 0.8762 - val_loss: 2.0018 - val_kl_loss: 2.7480 - val_recon_loss: 1.1774\n",
      "Epoch 175/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7157 - kl_loss: 2.7793 - recon_loss: 0.8819 - val_loss: 2.3230 - val_kl_loss: 2.9392 - val_recon_loss: 1.4413\n",
      "Epoch 176/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.6901 - kl_loss: 2.7786 - recon_loss: 0.8565 - val_loss: 2.2683 - val_kl_loss: 2.6116 - val_recon_loss: 1.4848\n",
      "Epoch 177/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7134 - kl_loss: 2.7040 - recon_loss: 0.9023 - val_loss: 1.8527 - val_kl_loss: 2.5887 - val_recon_loss: 1.0761\n",
      "Epoch 178/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.6840 - kl_loss: 2.6895 - recon_loss: 0.8772 - val_loss: 2.5390 - val_kl_loss: 2.6581 - val_recon_loss: 1.7416\n",
      "Epoch 179/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.6995 - kl_loss: 2.7338 - recon_loss: 0.8793 - val_loss: 2.1119 - val_kl_loss: 2.7948 - val_recon_loss: 1.2735\n",
      "Epoch 180/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.6814 - kl_loss: 2.7384 - recon_loss: 0.8599 - val_loss: 2.2566 - val_kl_loss: 2.6867 - val_recon_loss: 1.4506\n",
      "Epoch 181/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.6735 - kl_loss: 2.6916 - recon_loss: 0.8660 - val_loss: 2.1284 - val_kl_loss: 2.7791 - val_recon_loss: 1.2947\n",
      "Epoch 182/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.6763 - kl_loss: 2.6969 - recon_loss: 0.8672 - val_loss: 2.2959 - val_kl_loss: 2.7845 - val_recon_loss: 1.4605\n",
      "Epoch 183/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7018 - kl_loss: 2.7353 - recon_loss: 0.8812 - val_loss: 1.9185 - val_kl_loss: 2.7345 - val_recon_loss: 1.0982\n",
      "Epoch 184/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.6778 - kl_loss: 2.6937 - recon_loss: 0.8697 - val_loss: 1.9915 - val_kl_loss: 2.5152 - val_recon_loss: 1.2370\n",
      "Epoch 185/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.6807 - kl_loss: 2.6649 - recon_loss: 0.8812 - val_loss: 2.0629 - val_kl_loss: 2.7992 - val_recon_loss: 1.2231\n",
      "Epoch 186/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7030 - kl_loss: 2.6870 - recon_loss: 0.8969 - val_loss: 2.0196 - val_kl_loss: 2.6737 - val_recon_loss: 1.2175\n",
      "Epoch 187/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.6455 - kl_loss: 2.6647 - recon_loss: 0.8461 - val_loss: 1.9067 - val_kl_loss: 2.7347 - val_recon_loss: 1.0863\n",
      "Epoch 188/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.6762 - kl_loss: 2.6989 - recon_loss: 0.8665 - val_loss: 2.1334 - val_kl_loss: 2.6948 - val_recon_loss: 1.3249\n",
      "Epoch 189/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.6884 - kl_loss: 2.6959 - recon_loss: 0.8797 - val_loss: 2.2352 - val_kl_loss: 2.8668 - val_recon_loss: 1.3751\n",
      "Epoch 190/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.6604 - kl_loss: 2.6821 - recon_loss: 0.8558 - val_loss: 2.0982 - val_kl_loss: 2.5312 - val_recon_loss: 1.3389\n",
      "Epoch 191/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.6909 - kl_loss: 2.7062 - recon_loss: 0.8790 - val_loss: 2.5924 - val_kl_loss: 2.6234 - val_recon_loss: 1.8053\n",
      "Epoch 192/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.6582 - kl_loss: 2.6954 - recon_loss: 0.8495 - val_loss: 2.5240 - val_kl_loss: 2.7898 - val_recon_loss: 1.6871\n",
      "Epoch 193/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.6593 - kl_loss: 2.6985 - recon_loss: 0.8497 - val_loss: 1.8533 - val_kl_loss: 2.6565 - val_recon_loss: 1.0563\n",
      "Epoch 194/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.6703 - kl_loss: 2.7063 - recon_loss: 0.8584 - val_loss: 2.1029 - val_kl_loss: 2.6004 - val_recon_loss: 1.3228\n",
      "Epoch 195/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7156 - kl_loss: 2.7587 - recon_loss: 0.8880 - val_loss: 1.7745 - val_kl_loss: 2.8210 - val_recon_loss: 0.9283\n",
      "Epoch 196/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.6739 - kl_loss: 2.7111 - recon_loss: 0.8605 - val_loss: 2.1930 - val_kl_loss: 2.6907 - val_recon_loss: 1.3858\n",
      "Epoch 197/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.6610 - kl_loss: 2.6837 - recon_loss: 0.8559 - val_loss: 1.8279 - val_kl_loss: 2.7449 - val_recon_loss: 1.0044\n",
      "Epoch 198/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.6625 - kl_loss: 2.6994 - recon_loss: 0.8526 - val_loss: 2.2106 - val_kl_loss: 2.6796 - val_recon_loss: 1.4067\n",
      "Epoch 199/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.6696 - kl_loss: 2.6717 - recon_loss: 0.8680 - val_loss: 1.8925 - val_kl_loss: 2.7595 - val_recon_loss: 1.0646\n",
      "Epoch 200/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.6590 - kl_loss: 2.6777 - recon_loss: 0.8557 - val_loss: 1.8680 - val_kl_loss: 2.7227 - val_recon_loss: 1.0512\n",
      "========================= Model 3 =========================\n",
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6856        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6420        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 13,276\n",
      "Trainable params: 13,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_2 (Dense)             (None, 12)           300         enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            26          enc_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            26          enc_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,856\n",
      "Trainable params: 6,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 12)           204         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 24)           312         dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_2 (Dense)             (None, 48)           1200        dec_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,420\n",
      "Trainable params: 6,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1465 samples, validate on 365 samples\n",
      "Epoch 1/200\n",
      "1465/1465 [==============================] - 0s 157us/step - loss: 32.8036 - kl_loss: 13.9026 - recon_loss: 28.6328 - val_loss: 10.5141 - val_kl_loss: 11.0378 - val_recon_loss: 7.2028\n",
      "Epoch 2/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 7.9636 - kl_loss: 8.6993 - recon_loss: 5.3538 - val_loss: 7.5052 - val_kl_loss: 7.0179 - val_recon_loss: 5.3998\n",
      "Epoch 3/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 5.8214 - kl_loss: 6.7344 - recon_loss: 3.8011 - val_loss: 5.6247 - val_kl_loss: 7.2487 - val_recon_loss: 3.4501\n",
      "Epoch 4/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 4.9494 - kl_loss: 6.0216 - recon_loss: 3.1429 - val_loss: 4.5561 - val_kl_loss: 6.2325 - val_recon_loss: 2.6864\n",
      "Epoch 5/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 4.4776 - kl_loss: 5.6837 - recon_loss: 2.7724 - val_loss: 6.7894 - val_kl_loss: 5.6037 - val_recon_loss: 5.1083\n",
      "Epoch 6/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 4.3204 - kl_loss: 5.4144 - recon_loss: 2.6961 - val_loss: 4.3532 - val_kl_loss: 5.6370 - val_recon_loss: 2.6621\n",
      "Epoch 7/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.9911 - kl_loss: 5.2283 - recon_loss: 2.4226 - val_loss: 6.6697 - val_kl_loss: 4.7524 - val_recon_loss: 5.2440\n",
      "Epoch 8/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 3.8756 - kl_loss: 5.0050 - recon_loss: 2.3741 - val_loss: 6.6740 - val_kl_loss: 5.3146 - val_recon_loss: 5.0797\n",
      "Epoch 9/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 3.7126 - kl_loss: 4.9155 - recon_loss: 2.2379 - val_loss: 3.8263 - val_kl_loss: 4.8666 - val_recon_loss: 2.3664\n",
      "Epoch 10/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.6285 - kl_loss: 4.8272 - recon_loss: 2.1803 - val_loss: 3.5392 - val_kl_loss: 5.1907 - val_recon_loss: 1.9820\n",
      "Epoch 11/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.5603 - kl_loss: 4.8088 - recon_loss: 2.1177 - val_loss: 3.5826 - val_kl_loss: 4.9252 - val_recon_loss: 2.1051\n",
      "Epoch 12/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 3.4997 - kl_loss: 4.6187 - recon_loss: 2.1140 - val_loss: 5.9406 - val_kl_loss: 4.9062 - val_recon_loss: 4.4688\n",
      "Epoch 13/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 3.4076 - kl_loss: 4.6882 - recon_loss: 2.0012 - val_loss: 5.0775 - val_kl_loss: 4.4693 - val_recon_loss: 3.7368\n",
      "Epoch 14/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 3.3329 - kl_loss: 4.6542 - recon_loss: 1.9367 - val_loss: 3.2851 - val_kl_loss: 4.9539 - val_recon_loss: 1.7989\n",
      "Epoch 15/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 3.2728 - kl_loss: 4.5988 - recon_loss: 1.8932 - val_loss: 3.4580 - val_kl_loss: 4.6295 - val_recon_loss: 2.0691\n",
      "Epoch 16/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 3.1696 - kl_loss: 4.5946 - recon_loss: 1.7912 - val_loss: 3.3014 - val_kl_loss: 4.7390 - val_recon_loss: 1.8797\n",
      "Epoch 17/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 3.1414 - kl_loss: 4.5930 - recon_loss: 1.7635 - val_loss: 3.0721 - val_kl_loss: 4.7049 - val_recon_loss: 1.6606\n",
      "Epoch 18/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 3.0786 - kl_loss: 4.5307 - recon_loss: 1.7193 - val_loss: 3.1408 - val_kl_loss: 4.6302 - val_recon_loss: 1.7518\n",
      "Epoch 19/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.9826 - kl_loss: 4.5686 - recon_loss: 1.6120 - val_loss: 4.3247 - val_kl_loss: 4.8253 - val_recon_loss: 2.8771\n",
      "Epoch 20/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 3.0232 - kl_loss: 4.5778 - recon_loss: 1.6499 - val_loss: 2.7955 - val_kl_loss: 4.5281 - val_recon_loss: 1.4371\n",
      "Epoch 21/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.8774 - kl_loss: 4.4600 - recon_loss: 1.5395 - val_loss: 4.5772 - val_kl_loss: 4.5926 - val_recon_loss: 3.1994\n",
      "Epoch 22/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.9244 - kl_loss: 4.4933 - recon_loss: 1.5764 - val_loss: 3.4277 - val_kl_loss: 4.4510 - val_recon_loss: 2.0924\n",
      "Epoch 23/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.8515 - kl_loss: 4.4630 - recon_loss: 1.5126 - val_loss: 2.7518 - val_kl_loss: 4.7101 - val_recon_loss: 1.3387\n",
      "Epoch 24/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.7793 - kl_loss: 4.4509 - recon_loss: 1.4441 - val_loss: 2.7617 - val_kl_loss: 4.5308 - val_recon_loss: 1.4024\n",
      "Epoch 25/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.8238 - kl_loss: 4.4345 - recon_loss: 1.4935 - val_loss: 4.4797 - val_kl_loss: 4.7807 - val_recon_loss: 3.0455\n",
      "Epoch 26/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.7758 - kl_loss: 4.4795 - recon_loss: 1.4319 - val_loss: 2.7280 - val_kl_loss: 4.6255 - val_recon_loss: 1.3404\n",
      "Epoch 27/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.7343 - kl_loss: 4.4054 - recon_loss: 1.4126 - val_loss: 3.2362 - val_kl_loss: 4.4790 - val_recon_loss: 1.8925\n",
      "Epoch 28/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 2.7126 - kl_loss: 4.4088 - recon_loss: 1.3900 - val_loss: 2.7794 - val_kl_loss: 4.4640 - val_recon_loss: 1.4402\n",
      "Epoch 29/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.6907 - kl_loss: 4.4006 - recon_loss: 1.3705 - val_loss: 2.7350 - val_kl_loss: 4.4800 - val_recon_loss: 1.3910\n",
      "Epoch 30/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.6824 - kl_loss: 4.4418 - recon_loss: 1.3499 - val_loss: 2.7894 - val_kl_loss: 4.5032 - val_recon_loss: 1.4385\n",
      "Epoch 31/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.6311 - kl_loss: 4.3862 - recon_loss: 1.3152 - val_loss: 2.6944 - val_kl_loss: 4.3140 - val_recon_loss: 1.4002\n",
      "Epoch 32/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.6556 - kl_loss: 4.3463 - recon_loss: 1.3517 - val_loss: 2.5769 - val_kl_loss: 4.4472 - val_recon_loss: 1.2428\n",
      "Epoch 33/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.6616 - kl_loss: 4.3348 - recon_loss: 1.3611 - val_loss: 2.4874 - val_kl_loss: 4.4735 - val_recon_loss: 1.1453\n",
      "Epoch 34/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.5357 - kl_loss: 4.2724 - recon_loss: 1.2540 - val_loss: 3.1610 - val_kl_loss: 4.3584 - val_recon_loss: 1.8534\n",
      "Epoch 35/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.5937 - kl_loss: 4.2037 - recon_loss: 1.3326 - val_loss: 2.7921 - val_kl_loss: 4.4401 - val_recon_loss: 1.4601\n",
      "Epoch 36/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.5623 - kl_loss: 4.2710 - recon_loss: 1.2810 - val_loss: 2.6961 - val_kl_loss: 4.3025 - val_recon_loss: 1.4053\n",
      "Epoch 37/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.5738 - kl_loss: 4.2678 - recon_loss: 1.2935 - val_loss: 2.7041 - val_kl_loss: 4.4531 - val_recon_loss: 1.3682\n",
      "Epoch 38/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.4994 - kl_loss: 4.2137 - recon_loss: 1.2353 - val_loss: 2.7551 - val_kl_loss: 4.4048 - val_recon_loss: 1.4337\n",
      "Epoch 39/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.5132 - kl_loss: 4.1362 - recon_loss: 1.2723 - val_loss: 2.4704 - val_kl_loss: 4.2488 - val_recon_loss: 1.1958\n",
      "Epoch 40/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.5088 - kl_loss: 4.1821 - recon_loss: 1.2542 - val_loss: 3.2762 - val_kl_loss: 4.4583 - val_recon_loss: 1.9387\n",
      "Epoch 41/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.4656 - kl_loss: 4.1387 - recon_loss: 1.2240 - val_loss: 2.5524 - val_kl_loss: 4.1016 - val_recon_loss: 1.3220\n",
      "Epoch 42/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.4551 - kl_loss: 4.1621 - recon_loss: 1.2065 - val_loss: 2.7774 - val_kl_loss: 4.2638 - val_recon_loss: 1.4982\n",
      "Epoch 43/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.4689 - kl_loss: 4.1333 - recon_loss: 1.2289 - val_loss: 2.9281 - val_kl_loss: 4.2762 - val_recon_loss: 1.6453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 2.4380 - kl_loss: 4.0642 - recon_loss: 1.2187 - val_loss: 2.4788 - val_kl_loss: 4.2379 - val_recon_loss: 1.2074\n",
      "Epoch 45/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.4387 - kl_loss: 4.0953 - recon_loss: 1.2102 - val_loss: 2.7668 - val_kl_loss: 3.9212 - val_recon_loss: 1.5905\n",
      "Epoch 46/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3863 - kl_loss: 3.9856 - recon_loss: 1.1906 - val_loss: 2.3621 - val_kl_loss: 4.0988 - val_recon_loss: 1.1324\n",
      "Epoch 47/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.3503 - kl_loss: 3.8846 - recon_loss: 1.1849 - val_loss: 2.5994 - val_kl_loss: 4.2329 - val_recon_loss: 1.3296\n",
      "Epoch 48/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.3827 - kl_loss: 4.0060 - recon_loss: 1.1809 - val_loss: 2.4990 - val_kl_loss: 4.1135 - val_recon_loss: 1.2649\n",
      "Epoch 49/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.3852 - kl_loss: 4.0154 - recon_loss: 1.1806 - val_loss: 2.3582 - val_kl_loss: 3.9436 - val_recon_loss: 1.1751\n",
      "Epoch 50/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3730 - kl_loss: 3.9373 - recon_loss: 1.1919 - val_loss: 3.1621 - val_kl_loss: 4.2430 - val_recon_loss: 1.8892\n",
      "Epoch 51/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.3285 - kl_loss: 3.9514 - recon_loss: 1.1431 - val_loss: 2.3244 - val_kl_loss: 4.0743 - val_recon_loss: 1.1021\n",
      "Epoch 52/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.3378 - kl_loss: 3.9362 - recon_loss: 1.1569 - val_loss: 2.6331 - val_kl_loss: 3.9977 - val_recon_loss: 1.4338\n",
      "Epoch 53/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 2.2656 - kl_loss: 3.8522 - recon_loss: 1.1100 - val_loss: 2.2932 - val_kl_loss: 3.8026 - val_recon_loss: 1.1524\n",
      "Epoch 54/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.2939 - kl_loss: 3.8051 - recon_loss: 1.1524 - val_loss: 2.9901 - val_kl_loss: 3.8169 - val_recon_loss: 1.8451\n",
      "Epoch 55/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.2827 - kl_loss: 3.8393 - recon_loss: 1.1309 - val_loss: 2.5322 - val_kl_loss: 3.9722 - val_recon_loss: 1.3405\n",
      "Epoch 56/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.2389 - kl_loss: 3.8102 - recon_loss: 1.0958 - val_loss: 2.5679 - val_kl_loss: 3.9431 - val_recon_loss: 1.3850\n",
      "Epoch 57/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.2505 - kl_loss: 3.7815 - recon_loss: 1.1160 - val_loss: 2.3645 - val_kl_loss: 3.8669 - val_recon_loss: 1.2044\n",
      "Epoch 58/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.2581 - kl_loss: 3.7728 - recon_loss: 1.1263 - val_loss: 2.4218 - val_kl_loss: 3.8822 - val_recon_loss: 1.2571\n",
      "Epoch 59/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.2552 - kl_loss: 3.7926 - recon_loss: 1.1174 - val_loss: 2.6783 - val_kl_loss: 4.2768 - val_recon_loss: 1.3952\n",
      "Epoch 60/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.2633 - kl_loss: 3.8036 - recon_loss: 1.1222 - val_loss: 2.3122 - val_kl_loss: 3.8649 - val_recon_loss: 1.1527\n",
      "Epoch 61/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.2706 - kl_loss: 3.7933 - recon_loss: 1.1326 - val_loss: 2.2012 - val_kl_loss: 3.8301 - val_recon_loss: 1.0522\n",
      "Epoch 62/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.2084 - kl_loss: 3.7850 - recon_loss: 1.0729 - val_loss: 3.5372 - val_kl_loss: 4.1401 - val_recon_loss: 2.2952\n",
      "Epoch 63/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 2.1893 - kl_loss: 3.7097 - recon_loss: 1.0764 - val_loss: 3.1314 - val_kl_loss: 3.9637 - val_recon_loss: 1.9423\n",
      "Epoch 64/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1641 - kl_loss: 3.6304 - recon_loss: 1.0750 - val_loss: 2.7274 - val_kl_loss: 3.7169 - val_recon_loss: 1.6124\n",
      "Epoch 65/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.1872 - kl_loss: 3.6619 - recon_loss: 1.0886 - val_loss: 2.9697 - val_kl_loss: 3.8827 - val_recon_loss: 1.8049\n",
      "Epoch 66/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1852 - kl_loss: 3.6977 - recon_loss: 1.0759 - val_loss: 2.2034 - val_kl_loss: 3.7012 - val_recon_loss: 1.0930\n",
      "Epoch 67/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.2192 - kl_loss: 3.6227 - recon_loss: 1.1324 - val_loss: 2.5367 - val_kl_loss: 3.7926 - val_recon_loss: 1.3989\n",
      "Epoch 68/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.1091 - kl_loss: 3.5446 - recon_loss: 1.0457 - val_loss: 2.9017 - val_kl_loss: 3.6332 - val_recon_loss: 1.8117\n",
      "Epoch 69/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1596 - kl_loss: 3.6152 - recon_loss: 1.0751 - val_loss: 2.1938 - val_kl_loss: 3.6387 - val_recon_loss: 1.1022\n",
      "Epoch 70/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.1676 - kl_loss: 3.6336 - recon_loss: 1.0775 - val_loss: 2.5756 - val_kl_loss: 3.5174 - val_recon_loss: 1.5204\n",
      "Epoch 71/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.1578 - kl_loss: 3.5282 - recon_loss: 1.0994 - val_loss: 2.7163 - val_kl_loss: 3.7323 - val_recon_loss: 1.5966\n",
      "Epoch 72/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1021 - kl_loss: 3.5247 - recon_loss: 1.0447 - val_loss: 3.1511 - val_kl_loss: 3.4964 - val_recon_loss: 2.1022\n",
      "Epoch 73/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.1163 - kl_loss: 3.5158 - recon_loss: 1.0615 - val_loss: 2.1562 - val_kl_loss: 3.6723 - val_recon_loss: 1.0545\n",
      "Epoch 74/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.1200 - kl_loss: 3.5485 - recon_loss: 1.0555 - val_loss: 2.0930 - val_kl_loss: 3.7363 - val_recon_loss: 0.9721\n",
      "Epoch 75/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0528 - kl_loss: 3.4734 - recon_loss: 1.0108 - val_loss: 2.9853 - val_kl_loss: 3.6366 - val_recon_loss: 1.8943\n",
      "Epoch 76/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 2.0985 - kl_loss: 3.4960 - recon_loss: 1.0497 - val_loss: 3.0268 - val_kl_loss: 3.6836 - val_recon_loss: 1.9217\n",
      "Epoch 77/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.1119 - kl_loss: 3.4041 - recon_loss: 1.0906 - val_loss: 2.1244 - val_kl_loss: 3.7314 - val_recon_loss: 1.0050\n",
      "Epoch 78/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.0556 - kl_loss: 3.4735 - recon_loss: 1.0136 - val_loss: 2.3892 - val_kl_loss: 3.5397 - val_recon_loss: 1.3273\n",
      "Epoch 79/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0701 - kl_loss: 3.3988 - recon_loss: 1.0505 - val_loss: 2.1959 - val_kl_loss: 3.3289 - val_recon_loss: 1.1972\n",
      "Epoch 80/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0754 - kl_loss: 3.4330 - recon_loss: 1.0455 - val_loss: 4.4740 - val_kl_loss: 3.5992 - val_recon_loss: 3.3942\n",
      "Epoch 81/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0436 - kl_loss: 3.4365 - recon_loss: 1.0127 - val_loss: 2.3315 - val_kl_loss: 3.5995 - val_recon_loss: 1.2516\n",
      "Epoch 82/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.0516 - kl_loss: 3.3782 - recon_loss: 1.0382 - val_loss: 3.2357 - val_kl_loss: 3.6661 - val_recon_loss: 2.1359\n",
      "Epoch 83/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0353 - kl_loss: 3.3617 - recon_loss: 1.0268 - val_loss: 2.1639 - val_kl_loss: 3.4590 - val_recon_loss: 1.1262\n",
      "Epoch 84/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9990 - kl_loss: 3.3338 - recon_loss: 0.9989 - val_loss: 2.0045 - val_kl_loss: 3.4902 - val_recon_loss: 0.9575\n",
      "Epoch 85/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0454 - kl_loss: 3.3625 - recon_loss: 1.0366 - val_loss: 2.3474 - val_kl_loss: 3.5221 - val_recon_loss: 1.2908\n",
      "Epoch 86/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0348 - kl_loss: 3.3566 - recon_loss: 1.0278 - val_loss: 2.4575 - val_kl_loss: 3.5361 - val_recon_loss: 1.3967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0378 - kl_loss: 3.3355 - recon_loss: 1.0371 - val_loss: 2.0622 - val_kl_loss: 3.4366 - val_recon_loss: 1.0313\n",
      "Epoch 88/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0180 - kl_loss: 3.3348 - recon_loss: 1.0176 - val_loss: 2.1788 - val_kl_loss: 3.5206 - val_recon_loss: 1.1226\n",
      "Epoch 89/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0165 - kl_loss: 3.3012 - recon_loss: 1.0262 - val_loss: 2.2228 - val_kl_loss: 3.4291 - val_recon_loss: 1.1941\n",
      "Epoch 90/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9853 - kl_loss: 3.3040 - recon_loss: 0.9941 - val_loss: 2.3120 - val_kl_loss: 3.2560 - val_recon_loss: 1.3352\n",
      "Epoch 91/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9839 - kl_loss: 3.2777 - recon_loss: 1.0006 - val_loss: 2.3578 - val_kl_loss: 3.4602 - val_recon_loss: 1.3197\n",
      "Epoch 92/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9563 - kl_loss: 3.2061 - recon_loss: 0.9944 - val_loss: 2.1436 - val_kl_loss: 3.1581 - val_recon_loss: 1.1961\n",
      "Epoch 93/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9507 - kl_loss: 3.2109 - recon_loss: 0.9874 - val_loss: 2.2548 - val_kl_loss: 3.0273 - val_recon_loss: 1.3466\n",
      "Epoch 94/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9582 - kl_loss: 3.2051 - recon_loss: 0.9967 - val_loss: 2.4817 - val_kl_loss: 3.3573 - val_recon_loss: 1.4745\n",
      "Epoch 95/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.9888 - kl_loss: 3.2206 - recon_loss: 1.0227 - val_loss: 2.4744 - val_kl_loss: 3.1841 - val_recon_loss: 1.5192\n",
      "Epoch 96/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9641 - kl_loss: 3.2300 - recon_loss: 0.9952 - val_loss: 2.4450 - val_kl_loss: 3.3741 - val_recon_loss: 1.4328\n",
      "Epoch 97/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.9812 - kl_loss: 3.2437 - recon_loss: 1.0081 - val_loss: 2.1069 - val_kl_loss: 3.4948 - val_recon_loss: 1.0585\n",
      "Epoch 98/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9512 - kl_loss: 3.2089 - recon_loss: 0.9885 - val_loss: 2.6746 - val_kl_loss: 3.3991 - val_recon_loss: 1.6549\n",
      "Epoch 99/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9391 - kl_loss: 3.2775 - recon_loss: 0.9559 - val_loss: 2.2811 - val_kl_loss: 3.3988 - val_recon_loss: 1.2614\n",
      "Epoch 100/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9598 - kl_loss: 3.2360 - recon_loss: 0.9890 - val_loss: 2.5387 - val_kl_loss: 3.1447 - val_recon_loss: 1.5953\n",
      "Epoch 101/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9586 - kl_loss: 3.2022 - recon_loss: 0.9979 - val_loss: 2.6023 - val_kl_loss: 3.2250 - val_recon_loss: 1.6348\n",
      "Epoch 102/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9483 - kl_loss: 3.2082 - recon_loss: 0.9859 - val_loss: 2.2988 - val_kl_loss: 3.1472 - val_recon_loss: 1.3546\n",
      "Epoch 103/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9285 - kl_loss: 3.2109 - recon_loss: 0.9653 - val_loss: 2.0883 - val_kl_loss: 3.2123 - val_recon_loss: 1.1246\n",
      "Epoch 104/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9598 - kl_loss: 3.1884 - recon_loss: 1.0032 - val_loss: 1.9245 - val_kl_loss: 3.3261 - val_recon_loss: 0.9267\n",
      "Epoch 105/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8945 - kl_loss: 3.1645 - recon_loss: 0.9452 - val_loss: 2.1531 - val_kl_loss: 3.1565 - val_recon_loss: 1.2062\n",
      "Epoch 106/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9335 - kl_loss: 3.1556 - recon_loss: 0.9869 - val_loss: 1.9902 - val_kl_loss: 3.2268 - val_recon_loss: 1.0222\n",
      "Epoch 107/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.9132 - kl_loss: 3.1622 - recon_loss: 0.9646 - val_loss: 1.9370 - val_kl_loss: 3.1995 - val_recon_loss: 0.9771\n",
      "Epoch 108/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9050 - kl_loss: 3.1332 - recon_loss: 0.9650 - val_loss: 2.0424 - val_kl_loss: 3.2698 - val_recon_loss: 1.0614\n",
      "Epoch 109/200\n",
      "1465/1465 [==============================] - 0s 64us/step - loss: 1.9241 - kl_loss: 3.1792 - recon_loss: 0.9704 - val_loss: 2.7231 - val_kl_loss: 3.1900 - val_recon_loss: 1.7660\n",
      "Epoch 110/200\n",
      "1465/1465 [==============================] - 0s 63us/step - loss: 1.9204 - kl_loss: 3.1633 - recon_loss: 0.9714 - val_loss: 2.1336 - val_kl_loss: 3.0738 - val_recon_loss: 1.2115\n",
      "Epoch 111/200\n",
      "1465/1465 [==============================] - 0s 64us/step - loss: 1.8975 - kl_loss: 3.1465 - recon_loss: 0.9536 - val_loss: 2.3696 - val_kl_loss: 3.3863 - val_recon_loss: 1.3537\n",
      "Epoch 112/200\n",
      "1465/1465 [==============================] - 0s 65us/step - loss: 1.8870 - kl_loss: 3.1396 - recon_loss: 0.9452 - val_loss: 2.1226 - val_kl_loss: 3.0765 - val_recon_loss: 1.1996\n",
      "Epoch 113/200\n",
      "1465/1465 [==============================] - 0s 63us/step - loss: 1.9526 - kl_loss: 3.1440 - recon_loss: 1.0094 - val_loss: 2.1420 - val_kl_loss: 3.2707 - val_recon_loss: 1.1608\n",
      "Epoch 114/200\n",
      "1465/1465 [==============================] - 0s 66us/step - loss: 1.9037 - kl_loss: 3.1663 - recon_loss: 0.9539 - val_loss: 2.1255 - val_kl_loss: 3.1223 - val_recon_loss: 1.1888\n",
      "Epoch 115/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8960 - kl_loss: 3.0875 - recon_loss: 0.9698 - val_loss: 1.9884 - val_kl_loss: 3.1440 - val_recon_loss: 1.0452\n",
      "Epoch 116/200\n",
      "1465/1465 [==============================] - 0s 67us/step - loss: 1.8967 - kl_loss: 3.0963 - recon_loss: 0.9678 - val_loss: 2.6971 - val_kl_loss: 3.0691 - val_recon_loss: 1.7764\n",
      "Epoch 117/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8979 - kl_loss: 3.1056 - recon_loss: 0.9663 - val_loss: 2.9187 - val_kl_loss: 2.9875 - val_recon_loss: 2.0224\n",
      "Epoch 118/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9142 - kl_loss: 3.1643 - recon_loss: 0.9649 - val_loss: 2.6303 - val_kl_loss: 3.3580 - val_recon_loss: 1.6229\n",
      "Epoch 119/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.9083 - kl_loss: 3.0878 - recon_loss: 0.9820 - val_loss: 1.9258 - val_kl_loss: 3.1200 - val_recon_loss: 0.9898\n",
      "Epoch 120/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8793 - kl_loss: 3.1097 - recon_loss: 0.9464 - val_loss: 1.9691 - val_kl_loss: 3.1397 - val_recon_loss: 1.0272\n",
      "Epoch 121/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.8974 - kl_loss: 3.1128 - recon_loss: 0.9636 - val_loss: 2.7184 - val_kl_loss: 3.2857 - val_recon_loss: 1.7327\n",
      "Epoch 122/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8500 - kl_loss: 3.1078 - recon_loss: 0.9177 - val_loss: 2.1876 - val_kl_loss: 3.2412 - val_recon_loss: 1.2152\n",
      "Epoch 123/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8801 - kl_loss: 3.1451 - recon_loss: 0.9366 - val_loss: 2.1675 - val_kl_loss: 3.1957 - val_recon_loss: 1.2088\n",
      "Epoch 124/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8677 - kl_loss: 3.0685 - recon_loss: 0.9472 - val_loss: 4.4728 - val_kl_loss: 3.4119 - val_recon_loss: 3.4492\n",
      "Epoch 125/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8764 - kl_loss: 3.1075 - recon_loss: 0.9441 - val_loss: 2.5290 - val_kl_loss: 3.2260 - val_recon_loss: 1.5612\n",
      "Epoch 126/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8709 - kl_loss: 3.0829 - recon_loss: 0.9461 - val_loss: 2.5868 - val_kl_loss: 3.0251 - val_recon_loss: 1.6793\n",
      "Epoch 127/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8435 - kl_loss: 3.0361 - recon_loss: 0.9327 - val_loss: 2.3002 - val_kl_loss: 3.0228 - val_recon_loss: 1.3934\n",
      "Epoch 128/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8734 - kl_loss: 3.0920 - recon_loss: 0.9458 - val_loss: 2.2962 - val_kl_loss: 2.9061 - val_recon_loss: 1.4244\n",
      "Epoch 129/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8676 - kl_loss: 3.0853 - recon_loss: 0.9420 - val_loss: 2.1654 - val_kl_loss: 3.0891 - val_recon_loss: 1.2387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8645 - kl_loss: 3.0664 - recon_loss: 0.9446 - val_loss: 2.0229 - val_kl_loss: 3.0976 - val_recon_loss: 1.0936\n",
      "Epoch 131/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8463 - kl_loss: 3.0440 - recon_loss: 0.9331 - val_loss: 2.0612 - val_kl_loss: 3.1513 - val_recon_loss: 1.1158\n",
      "Epoch 132/200\n",
      "1465/1465 [==============================] - 0s 90us/step - loss: 1.8426 - kl_loss: 3.1138 - recon_loss: 0.9084 - val_loss: 2.8870 - val_kl_loss: 3.1160 - val_recon_loss: 1.9522\n",
      "Epoch 133/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8556 - kl_loss: 3.0697 - recon_loss: 0.9347 - val_loss: 2.3339 - val_kl_loss: 3.1430 - val_recon_loss: 1.3910\n",
      "Epoch 134/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8351 - kl_loss: 3.0428 - recon_loss: 0.9223 - val_loss: 2.0804 - val_kl_loss: 3.1893 - val_recon_loss: 1.1236\n",
      "Epoch 135/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8327 - kl_loss: 3.0001 - recon_loss: 0.9327 - val_loss: 2.2814 - val_kl_loss: 3.1051 - val_recon_loss: 1.3499\n",
      "Epoch 136/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8303 - kl_loss: 3.0229 - recon_loss: 0.9234 - val_loss: 2.0827 - val_kl_loss: 2.9857 - val_recon_loss: 1.1869\n",
      "Epoch 137/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.8791 - kl_loss: 3.0706 - recon_loss: 0.9579 - val_loss: 2.0928 - val_kl_loss: 3.1825 - val_recon_loss: 1.1381\n",
      "Epoch 138/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.8442 - kl_loss: 3.0858 - recon_loss: 0.9185 - val_loss: 2.7767 - val_kl_loss: 3.0811 - val_recon_loss: 1.8524\n",
      "Epoch 139/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8513 - kl_loss: 3.0892 - recon_loss: 0.9245 - val_loss: 2.0529 - val_kl_loss: 3.0303 - val_recon_loss: 1.1438\n",
      "Epoch 140/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8057 - kl_loss: 2.9869 - recon_loss: 0.9096 - val_loss: 2.5992 - val_kl_loss: 3.2333 - val_recon_loss: 1.6292\n",
      "Epoch 141/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8260 - kl_loss: 3.0406 - recon_loss: 0.9138 - val_loss: 2.4157 - val_kl_loss: 3.0694 - val_recon_loss: 1.4949\n",
      "Epoch 142/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.8431 - kl_loss: 3.0278 - recon_loss: 0.9347 - val_loss: 2.0172 - val_kl_loss: 3.1463 - val_recon_loss: 1.0734\n",
      "Epoch 143/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8336 - kl_loss: 3.0599 - recon_loss: 0.9156 - val_loss: 2.0894 - val_kl_loss: 3.1251 - val_recon_loss: 1.1519\n",
      "Epoch 144/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8252 - kl_loss: 2.9926 - recon_loss: 0.9274 - val_loss: 2.0672 - val_kl_loss: 2.9288 - val_recon_loss: 1.1885\n",
      "Epoch 145/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8190 - kl_loss: 3.0017 - recon_loss: 0.9185 - val_loss: 2.2935 - val_kl_loss: 2.9480 - val_recon_loss: 1.4091\n",
      "Epoch 146/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8074 - kl_loss: 3.0096 - recon_loss: 0.9046 - val_loss: 2.3832 - val_kl_loss: 3.0941 - val_recon_loss: 1.4549\n",
      "Epoch 147/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8146 - kl_loss: 2.9897 - recon_loss: 0.9177 - val_loss: 2.0947 - val_kl_loss: 2.9360 - val_recon_loss: 1.2139\n",
      "Epoch 148/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8058 - kl_loss: 2.9365 - recon_loss: 0.9249 - val_loss: 1.9655 - val_kl_loss: 3.0308 - val_recon_loss: 1.0562\n",
      "Epoch 149/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8212 - kl_loss: 2.9853 - recon_loss: 0.9256 - val_loss: 2.2375 - val_kl_loss: 3.0108 - val_recon_loss: 1.3342\n",
      "Epoch 150/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8194 - kl_loss: 2.9972 - recon_loss: 0.9203 - val_loss: 1.9004 - val_kl_loss: 3.2904 - val_recon_loss: 0.9133\n",
      "Epoch 151/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.7912 - kl_loss: 2.9580 - recon_loss: 0.9038 - val_loss: 3.2643 - val_kl_loss: 3.2402 - val_recon_loss: 2.2923\n",
      "Epoch 152/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7999 - kl_loss: 2.9796 - recon_loss: 0.9061 - val_loss: 2.0057 - val_kl_loss: 3.0306 - val_recon_loss: 1.0965\n",
      "Epoch 153/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.8058 - kl_loss: 2.9191 - recon_loss: 0.9301 - val_loss: 2.1334 - val_kl_loss: 2.9906 - val_recon_loss: 1.2362\n",
      "Epoch 154/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8269 - kl_loss: 3.0427 - recon_loss: 0.9141 - val_loss: 2.8864 - val_kl_loss: 3.0790 - val_recon_loss: 1.9627\n",
      "Epoch 155/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.8300 - kl_loss: 3.0256 - recon_loss: 0.9224 - val_loss: 2.3733 - val_kl_loss: 3.0513 - val_recon_loss: 1.4579\n",
      "Epoch 156/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7811 - kl_loss: 2.9610 - recon_loss: 0.8928 - val_loss: 1.9218 - val_kl_loss: 3.0847 - val_recon_loss: 0.9964\n",
      "Epoch 157/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8092 - kl_loss: 2.9992 - recon_loss: 0.9094 - val_loss: 2.3124 - val_kl_loss: 3.1924 - val_recon_loss: 1.3547\n",
      "Epoch 158/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7997 - kl_loss: 3.0301 - recon_loss: 0.8907 - val_loss: 2.8216 - val_kl_loss: 2.9350 - val_recon_loss: 1.9411\n",
      "Epoch 159/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7988 - kl_loss: 2.9309 - recon_loss: 0.9195 - val_loss: 1.9206 - val_kl_loss: 3.0316 - val_recon_loss: 1.0111\n",
      "Epoch 160/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.8099 - kl_loss: 2.9565 - recon_loss: 0.9229 - val_loss: 2.0850 - val_kl_loss: 3.1787 - val_recon_loss: 1.1314\n",
      "Epoch 161/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7765 - kl_loss: 2.9482 - recon_loss: 0.8921 - val_loss: 2.2700 - val_kl_loss: 2.9944 - val_recon_loss: 1.3716\n",
      "Epoch 162/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7689 - kl_loss: 2.8971 - recon_loss: 0.8998 - val_loss: 2.0563 - val_kl_loss: 3.1435 - val_recon_loss: 1.1133\n",
      "Epoch 163/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7775 - kl_loss: 2.9264 - recon_loss: 0.8996 - val_loss: 1.9487 - val_kl_loss: 3.0156 - val_recon_loss: 1.0440\n",
      "Epoch 164/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8085 - kl_loss: 2.9419 - recon_loss: 0.9259 - val_loss: 2.3424 - val_kl_loss: 2.9497 - val_recon_loss: 1.4575\n",
      "Epoch 165/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7885 - kl_loss: 2.9528 - recon_loss: 0.9027 - val_loss: 2.0132 - val_kl_loss: 3.0353 - val_recon_loss: 1.1026\n",
      "Epoch 166/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7543 - kl_loss: 2.9126 - recon_loss: 0.8805 - val_loss: 1.9782 - val_kl_loss: 3.0012 - val_recon_loss: 1.0778\n",
      "Epoch 167/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7681 - kl_loss: 2.9039 - recon_loss: 0.8969 - val_loss: 2.3135 - val_kl_loss: 2.9402 - val_recon_loss: 1.4314\n",
      "Epoch 168/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 1.7695 - kl_loss: 2.9254 - recon_loss: 0.8918 - val_loss: 2.1189 - val_kl_loss: 2.8789 - val_recon_loss: 1.2553\n",
      "Epoch 169/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7870 - kl_loss: 2.9456 - recon_loss: 0.9033 - val_loss: 2.6409 - val_kl_loss: 2.8167 - val_recon_loss: 1.7958\n",
      "Epoch 170/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7668 - kl_loss: 2.8567 - recon_loss: 0.9098 - val_loss: 1.8837 - val_kl_loss: 3.0252 - val_recon_loss: 0.9762\n",
      "Epoch 171/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7736 - kl_loss: 2.9290 - recon_loss: 0.8949 - val_loss: 2.2841 - val_kl_loss: 3.1582 - val_recon_loss: 1.3367\n",
      "Epoch 172/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.7476 - kl_loss: 2.9128 - recon_loss: 0.8738 - val_loss: 2.4172 - val_kl_loss: 3.1595 - val_recon_loss: 1.4694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.7882 - kl_loss: 2.8684 - recon_loss: 0.9277 - val_loss: 1.9249 - val_kl_loss: 3.0794 - val_recon_loss: 1.0011\n",
      "Epoch 174/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7347 - kl_loss: 2.8760 - recon_loss: 0.8720 - val_loss: 2.0604 - val_kl_loss: 2.9800 - val_recon_loss: 1.1664\n",
      "Epoch 175/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7758 - kl_loss: 2.8834 - recon_loss: 0.9108 - val_loss: 1.9947 - val_kl_loss: 2.8885 - val_recon_loss: 1.1281\n",
      "Epoch 176/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7533 - kl_loss: 2.8937 - recon_loss: 0.8852 - val_loss: 2.0642 - val_kl_loss: 2.8595 - val_recon_loss: 1.2064\n",
      "Epoch 177/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7445 - kl_loss: 2.8662 - recon_loss: 0.8846 - val_loss: 2.1508 - val_kl_loss: 2.9230 - val_recon_loss: 1.2739\n",
      "Epoch 178/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7545 - kl_loss: 2.8600 - recon_loss: 0.8965 - val_loss: 3.3626 - val_kl_loss: 2.8945 - val_recon_loss: 2.4943\n",
      "Epoch 179/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7176 - kl_loss: 2.8349 - recon_loss: 0.8671 - val_loss: 2.1119 - val_kl_loss: 2.9781 - val_recon_loss: 1.2184\n",
      "Epoch 180/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.7795 - kl_loss: 2.8915 - recon_loss: 0.9121 - val_loss: 2.6079 - val_kl_loss: 3.1397 - val_recon_loss: 1.6659\n",
      "Epoch 181/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7389 - kl_loss: 2.8535 - recon_loss: 0.8829 - val_loss: 2.3507 - val_kl_loss: 2.8377 - val_recon_loss: 1.4994\n",
      "Epoch 182/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7527 - kl_loss: 2.8920 - recon_loss: 0.8851 - val_loss: 2.4132 - val_kl_loss: 2.7492 - val_recon_loss: 1.5885\n",
      "Epoch 183/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.7498 - kl_loss: 2.8581 - recon_loss: 0.8924 - val_loss: 2.1495 - val_kl_loss: 2.8978 - val_recon_loss: 1.2801\n",
      "Epoch 184/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.7307 - kl_loss: 2.8021 - recon_loss: 0.8901 - val_loss: 2.2061 - val_kl_loss: 2.9608 - val_recon_loss: 1.3178\n",
      "Epoch 185/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7373 - kl_loss: 2.8492 - recon_loss: 0.8825 - val_loss: 2.1501 - val_kl_loss: 2.9758 - val_recon_loss: 1.2574\n",
      "Epoch 186/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7629 - kl_loss: 2.8608 - recon_loss: 0.9047 - val_loss: 2.0679 - val_kl_loss: 2.9297 - val_recon_loss: 1.1890\n",
      "Epoch 187/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.7435 - kl_loss: 2.8562 - recon_loss: 0.8867 - val_loss: 2.4952 - val_kl_loss: 2.8838 - val_recon_loss: 1.6300\n",
      "Epoch 188/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7437 - kl_loss: 2.8578 - recon_loss: 0.8863 - val_loss: 1.9364 - val_kl_loss: 2.9759 - val_recon_loss: 1.0436\n",
      "Epoch 189/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7268 - kl_loss: 2.8258 - recon_loss: 0.8791 - val_loss: 1.8709 - val_kl_loss: 2.9232 - val_recon_loss: 0.9939\n",
      "Epoch 190/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7048 - kl_loss: 2.7946 - recon_loss: 0.8665 - val_loss: 2.2008 - val_kl_loss: 2.8444 - val_recon_loss: 1.3474\n",
      "Epoch 191/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7552 - kl_loss: 2.8220 - recon_loss: 0.9087 - val_loss: 1.7692 - val_kl_loss: 2.7682 - val_recon_loss: 0.9388\n",
      "Epoch 192/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7375 - kl_loss: 2.8168 - recon_loss: 0.8924 - val_loss: 2.7064 - val_kl_loss: 2.9411 - val_recon_loss: 1.8241\n",
      "Epoch 193/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7376 - kl_loss: 2.8556 - recon_loss: 0.8810 - val_loss: 1.9981 - val_kl_loss: 2.9584 - val_recon_loss: 1.1106\n",
      "Epoch 194/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7240 - kl_loss: 2.8390 - recon_loss: 0.8723 - val_loss: 2.8219 - val_kl_loss: 2.9245 - val_recon_loss: 1.9445\n",
      "Epoch 195/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7487 - kl_loss: 2.8289 - recon_loss: 0.9000 - val_loss: 2.0549 - val_kl_loss: 2.8598 - val_recon_loss: 1.1970\n",
      "Epoch 196/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7419 - kl_loss: 2.8414 - recon_loss: 0.8895 - val_loss: 2.6152 - val_kl_loss: 2.8749 - val_recon_loss: 1.7527\n",
      "Epoch 197/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7320 - kl_loss: 2.8079 - recon_loss: 0.8896 - val_loss: 1.8726 - val_kl_loss: 2.9158 - val_recon_loss: 0.9979\n",
      "Epoch 198/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7337 - kl_loss: 2.8595 - recon_loss: 0.8758 - val_loss: 2.2225 - val_kl_loss: 2.7875 - val_recon_loss: 1.3862\n",
      "Epoch 199/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7295 - kl_loss: 2.8272 - recon_loss: 0.8813 - val_loss: 2.2409 - val_kl_loss: 2.9751 - val_recon_loss: 1.3483\n",
      "Epoch 200/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7364 - kl_loss: 2.8528 - recon_loss: 0.8805 - val_loss: 1.9969 - val_kl_loss: 3.0712 - val_recon_loss: 1.0756\n",
      "========================= Model 4 =========================\n",
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6856        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6420        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 13,276\n",
      "Trainable params: 13,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_2 (Dense)             (None, 12)           300         enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            26          enc_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            26          enc_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,856\n",
      "Trainable params: 6,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 12)           204         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 24)           312         dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_2 (Dense)             (None, 48)           1200        dec_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,420\n",
      "Trainable params: 6,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1464 samples, validate on 366 samples\n",
      "Epoch 1/200\n",
      "1464/1464 [==============================] - 0s 140us/step - loss: 45.4938 - kl_loss: 13.5064 - recon_loss: 41.4419 - val_loss: 12.4941 - val_kl_loss: 7.9228 - val_recon_loss: 10.1173\n",
      "Epoch 2/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 8.8084 - kl_loss: 6.9089 - recon_loss: 6.7357 - val_loss: 6.8239 - val_kl_loss: 5.7114 - val_recon_loss: 5.1105\n",
      "Epoch 3/200\n",
      "1464/1464 [==============================] - 0s 71us/step - loss: 5.9724 - kl_loss: 4.7898 - recon_loss: 4.5355 - val_loss: 9.4299 - val_kl_loss: 4.7740 - val_recon_loss: 7.9977\n",
      "Epoch 4/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 4.8157 - kl_loss: 4.2765 - recon_loss: 3.5328 - val_loss: 5.3094 - val_kl_loss: 3.8062 - val_recon_loss: 4.1675\n",
      "Epoch 5/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 4.1836 - kl_loss: 3.9733 - recon_loss: 2.9916 - val_loss: 3.5711 - val_kl_loss: 3.6192 - val_recon_loss: 2.4853\n",
      "Epoch 6/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 3.7801 - kl_loss: 3.8047 - recon_loss: 2.6387 - val_loss: 4.6790 - val_kl_loss: 3.5540 - val_recon_loss: 3.6128\n",
      "Epoch 7/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 3.6013 - kl_loss: 3.6248 - recon_loss: 2.5138 - val_loss: 3.9658 - val_kl_loss: 3.7520 - val_recon_loss: 2.8402\n",
      "Epoch 8/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 3.5071 - kl_loss: 3.6791 - recon_loss: 2.4034 - val_loss: 3.4268 - val_kl_loss: 3.3383 - val_recon_loss: 2.4254\n",
      "Epoch 9/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 3.2741 - kl_loss: 3.5041 - recon_loss: 2.2229 - val_loss: 3.0924 - val_kl_loss: 3.2970 - val_recon_loss: 2.1033\n",
      "Epoch 10/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 3.2407 - kl_loss: 3.4053 - recon_loss: 2.2191 - val_loss: 3.1734 - val_kl_loss: 3.4528 - val_recon_loss: 2.1376\n",
      "Epoch 11/200\n",
      "1464/1464 [==============================] - 0s 87us/step - loss: 3.1228 - kl_loss: 3.3932 - recon_loss: 2.1048 - val_loss: 3.4116 - val_kl_loss: 3.2985 - val_recon_loss: 2.4220\n",
      "Epoch 12/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 3.0212 - kl_loss: 3.3522 - recon_loss: 2.0156 - val_loss: 2.7116 - val_kl_loss: 3.0960 - val_recon_loss: 1.7828\n",
      "Epoch 13/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.9545 - kl_loss: 3.3218 - recon_loss: 1.9580 - val_loss: 3.4283 - val_kl_loss: 2.8749 - val_recon_loss: 2.5659\n",
      "Epoch 14/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.9381 - kl_loss: 3.2186 - recon_loss: 1.9726 - val_loss: 2.6392 - val_kl_loss: 3.0933 - val_recon_loss: 1.7113\n",
      "Epoch 15/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.8214 - kl_loss: 3.2744 - recon_loss: 1.8391 - val_loss: 2.8803 - val_kl_loss: 2.9573 - val_recon_loss: 1.9932\n",
      "Epoch 16/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.7921 - kl_loss: 3.1828 - recon_loss: 1.8373 - val_loss: 3.9229 - val_kl_loss: 3.2636 - val_recon_loss: 2.9438\n",
      "Epoch 17/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.7272 - kl_loss: 3.1715 - recon_loss: 1.7758 - val_loss: 2.6844 - val_kl_loss: 3.1989 - val_recon_loss: 1.7247\n",
      "Epoch 18/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 2.6751 - kl_loss: 3.1367 - recon_loss: 1.7341 - val_loss: 2.5275 - val_kl_loss: 2.9160 - val_recon_loss: 1.6527\n",
      "Epoch 19/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.6919 - kl_loss: 3.1576 - recon_loss: 1.7446 - val_loss: 3.0149 - val_kl_loss: 3.1295 - val_recon_loss: 2.0761\n",
      "Epoch 20/200\n",
      "1464/1464 [==============================] - 0s 88us/step - loss: 2.6465 - kl_loss: 3.1428 - recon_loss: 1.7036 - val_loss: 2.3224 - val_kl_loss: 2.9007 - val_recon_loss: 1.4522\n",
      "Epoch 21/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.5769 - kl_loss: 3.0611 - recon_loss: 1.6585 - val_loss: 2.6707 - val_kl_loss: 2.8185 - val_recon_loss: 1.8251\n",
      "Epoch 22/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.5609 - kl_loss: 3.0901 - recon_loss: 1.6338 - val_loss: 2.8257 - val_kl_loss: 3.0975 - val_recon_loss: 1.8964\n",
      "Epoch 23/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.5124 - kl_loss: 3.0326 - recon_loss: 1.6026 - val_loss: 2.2041 - val_kl_loss: 2.8168 - val_recon_loss: 1.3590\n",
      "Epoch 24/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 2.4579 - kl_loss: 3.0129 - recon_loss: 1.5540 - val_loss: 2.4572 - val_kl_loss: 2.8021 - val_recon_loss: 1.6166\n",
      "Epoch 25/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.4592 - kl_loss: 2.9918 - recon_loss: 1.5617 - val_loss: 2.5979 - val_kl_loss: 2.8709 - val_recon_loss: 1.7366\n",
      "Epoch 26/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 2.4206 - kl_loss: 2.9889 - recon_loss: 1.5240 - val_loss: 2.7986 - val_kl_loss: 2.8016 - val_recon_loss: 1.9581\n",
      "Epoch 27/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.4940 - kl_loss: 3.0487 - recon_loss: 1.5794 - val_loss: 3.8976 - val_kl_loss: 3.2393 - val_recon_loss: 2.9258\n",
      "Epoch 28/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.4058 - kl_loss: 3.0776 - recon_loss: 1.4825 - val_loss: 3.1675 - val_kl_loss: 3.1144 - val_recon_loss: 2.2332\n",
      "Epoch 29/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 2.4264 - kl_loss: 3.0252 - recon_loss: 1.5189 - val_loss: 2.5317 - val_kl_loss: 3.0081 - val_recon_loss: 1.6293\n",
      "Epoch 30/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 2.3268 - kl_loss: 2.9858 - recon_loss: 1.4311 - val_loss: 2.3022 - val_kl_loss: 2.9921 - val_recon_loss: 1.4046\n",
      "Epoch 31/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.3195 - kl_loss: 2.9088 - recon_loss: 1.4469 - val_loss: 2.2786 - val_kl_loss: 2.8849 - val_recon_loss: 1.4132\n",
      "Epoch 32/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.3416 - kl_loss: 2.9611 - recon_loss: 1.4533 - val_loss: 2.5902 - val_kl_loss: 2.9231 - val_recon_loss: 1.7132\n",
      "Epoch 33/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.3364 - kl_loss: 2.9175 - recon_loss: 1.4611 - val_loss: 2.5154 - val_kl_loss: 2.8110 - val_recon_loss: 1.6721\n",
      "Epoch 34/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.2849 - kl_loss: 2.9389 - recon_loss: 1.4032 - val_loss: 2.4987 - val_kl_loss: 2.7777 - val_recon_loss: 1.6654\n",
      "Epoch 35/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.2894 - kl_loss: 2.9464 - recon_loss: 1.4055 - val_loss: 2.1523 - val_kl_loss: 2.7794 - val_recon_loss: 1.3185\n",
      "Epoch 36/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.3123 - kl_loss: 2.9352 - recon_loss: 1.4317 - val_loss: 2.1343 - val_kl_loss: 2.7668 - val_recon_loss: 1.3042\n",
      "Epoch 37/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.2584 - kl_loss: 2.9625 - recon_loss: 1.3697 - val_loss: 3.6436 - val_kl_loss: 2.6047 - val_recon_loss: 2.8622\n",
      "Epoch 38/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.2699 - kl_loss: 2.9311 - recon_loss: 1.3905 - val_loss: 2.2722 - val_kl_loss: 2.6168 - val_recon_loss: 1.4872\n",
      "Epoch 39/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.2536 - kl_loss: 2.8633 - recon_loss: 1.3946 - val_loss: 2.2267 - val_kl_loss: 2.7461 - val_recon_loss: 1.4029\n",
      "Epoch 40/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.2196 - kl_loss: 2.8726 - recon_loss: 1.3578 - val_loss: 2.1380 - val_kl_loss: 2.7421 - val_recon_loss: 1.3153\n",
      "Epoch 41/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 2.2311 - kl_loss: 2.8987 - recon_loss: 1.3615 - val_loss: 3.9524 - val_kl_loss: 2.5819 - val_recon_loss: 3.1778\n",
      "Epoch 42/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.2425 - kl_loss: 2.9190 - recon_loss: 1.3668 - val_loss: 2.0604 - val_kl_loss: 2.7877 - val_recon_loss: 1.2241\n",
      "Epoch 43/200\n",
      "1464/1464 [==============================] - 0s 74us/step - loss: 2.2098 - kl_loss: 2.8889 - recon_loss: 1.3431 - val_loss: 2.0913 - val_kl_loss: 2.6257 - val_recon_loss: 1.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 2.2231 - kl_loss: 2.8716 - recon_loss: 1.3616 - val_loss: 3.1056 - val_kl_loss: 2.5861 - val_recon_loss: 2.3298\n",
      "Epoch 45/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 2.2316 - kl_loss: 2.9149 - recon_loss: 1.3571 - val_loss: 2.2020 - val_kl_loss: 2.8297 - val_recon_loss: 1.3531\n",
      "Epoch 46/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.2003 - kl_loss: 2.8957 - recon_loss: 1.3316 - val_loss: 2.1386 - val_kl_loss: 2.6048 - val_recon_loss: 1.3572\n",
      "Epoch 47/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 2.1762 - kl_loss: 2.8480 - recon_loss: 1.3218 - val_loss: 2.4384 - val_kl_loss: 2.7020 - val_recon_loss: 1.6278\n",
      "Epoch 48/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.1904 - kl_loss: 2.8681 - recon_loss: 1.3300 - val_loss: 2.7042 - val_kl_loss: 2.7130 - val_recon_loss: 1.8903\n",
      "Epoch 49/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 2.1712 - kl_loss: 2.8625 - recon_loss: 1.3125 - val_loss: 2.2118 - val_kl_loss: 2.6901 - val_recon_loss: 1.4048\n",
      "Epoch 50/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.1930 - kl_loss: 2.8575 - recon_loss: 1.3358 - val_loss: 2.3319 - val_kl_loss: 2.5860 - val_recon_loss: 1.5561\n",
      "Epoch 51/200\n",
      "1464/1464 [==============================] - 0s 87us/step - loss: 2.1960 - kl_loss: 2.8723 - recon_loss: 1.3343 - val_loss: 2.3080 - val_kl_loss: 2.7454 - val_recon_loss: 1.4843\n",
      "Epoch 52/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 2.1768 - kl_loss: 2.9032 - recon_loss: 1.3058 - val_loss: 2.1708 - val_kl_loss: 2.7064 - val_recon_loss: 1.3589\n",
      "Epoch 53/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.1925 - kl_loss: 2.9004 - recon_loss: 1.3224 - val_loss: 2.0726 - val_kl_loss: 2.8017 - val_recon_loss: 1.2320\n",
      "Epoch 54/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 2.1469 - kl_loss: 2.8952 - recon_loss: 1.2784 - val_loss: 2.4181 - val_kl_loss: 2.8063 - val_recon_loss: 1.5762\n",
      "Epoch 55/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.1611 - kl_loss: 2.8853 - recon_loss: 1.2955 - val_loss: 2.0314 - val_kl_loss: 2.6783 - val_recon_loss: 1.2279\n",
      "Epoch 56/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.1603 - kl_loss: 2.8488 - recon_loss: 1.3057 - val_loss: 2.0709 - val_kl_loss: 2.7454 - val_recon_loss: 1.2473\n",
      "Epoch 57/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.1211 - kl_loss: 2.8746 - recon_loss: 1.2587 - val_loss: 2.1421 - val_kl_loss: 2.7326 - val_recon_loss: 1.3223\n",
      "Epoch 58/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.1669 - kl_loss: 2.9034 - recon_loss: 1.2959 - val_loss: 2.6611 - val_kl_loss: 2.8382 - val_recon_loss: 1.8097\n",
      "Epoch 59/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.1169 - kl_loss: 2.7905 - recon_loss: 1.2798 - val_loss: 2.1815 - val_kl_loss: 2.7124 - val_recon_loss: 1.3677\n",
      "Epoch 60/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.1281 - kl_loss: 2.8669 - recon_loss: 1.2680 - val_loss: 2.0456 - val_kl_loss: 2.8798 - val_recon_loss: 1.1817\n",
      "Epoch 61/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.1401 - kl_loss: 2.8825 - recon_loss: 1.2754 - val_loss: 2.2073 - val_kl_loss: 2.8243 - val_recon_loss: 1.3600\n",
      "Epoch 62/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.1085 - kl_loss: 2.8232 - recon_loss: 1.2615 - val_loss: 2.1091 - val_kl_loss: 2.6657 - val_recon_loss: 1.3094\n",
      "Epoch 63/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.1187 - kl_loss: 2.8630 - recon_loss: 1.2598 - val_loss: 2.0005 - val_kl_loss: 2.7319 - val_recon_loss: 1.1810\n",
      "Epoch 64/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.1084 - kl_loss: 2.8221 - recon_loss: 1.2618 - val_loss: 1.9703 - val_kl_loss: 2.8404 - val_recon_loss: 1.1181\n",
      "Epoch 65/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.1008 - kl_loss: 2.8274 - recon_loss: 1.2526 - val_loss: 2.1185 - val_kl_loss: 2.7270 - val_recon_loss: 1.3004\n",
      "Epoch 66/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.1233 - kl_loss: 2.8622 - recon_loss: 1.2646 - val_loss: 2.2533 - val_kl_loss: 2.6707 - val_recon_loss: 1.4521\n",
      "Epoch 67/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.1214 - kl_loss: 2.8560 - recon_loss: 1.2647 - val_loss: 2.5562 - val_kl_loss: 2.6845 - val_recon_loss: 1.7509\n",
      "Epoch 68/200\n",
      "1464/1464 [==============================] - 0s 74us/step - loss: 2.0983 - kl_loss: 2.8201 - recon_loss: 1.2523 - val_loss: 2.2353 - val_kl_loss: 2.4799 - val_recon_loss: 1.4913\n",
      "Epoch 69/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 2.0886 - kl_loss: 2.8188 - recon_loss: 1.2430 - val_loss: 2.2593 - val_kl_loss: 2.8044 - val_recon_loss: 1.4179\n",
      "Epoch 70/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.0857 - kl_loss: 2.7945 - recon_loss: 1.2474 - val_loss: 2.1046 - val_kl_loss: 2.5572 - val_recon_loss: 1.3375\n",
      "Epoch 71/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.0807 - kl_loss: 2.7982 - recon_loss: 1.2412 - val_loss: 2.1520 - val_kl_loss: 2.7657 - val_recon_loss: 1.3223\n",
      "Epoch 72/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 2.0888 - kl_loss: 2.8279 - recon_loss: 1.2405 - val_loss: 2.0768 - val_kl_loss: 2.6391 - val_recon_loss: 1.2851\n",
      "Epoch 73/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.0828 - kl_loss: 2.8085 - recon_loss: 1.2402 - val_loss: 2.2545 - val_kl_loss: 2.6998 - val_recon_loss: 1.4446\n",
      "Epoch 74/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.0810 - kl_loss: 2.8254 - recon_loss: 1.2334 - val_loss: 2.0436 - val_kl_loss: 2.7285 - val_recon_loss: 1.2250\n",
      "Epoch 75/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.0552 - kl_loss: 2.8045 - recon_loss: 1.2139 - val_loss: 2.6008 - val_kl_loss: 2.7514 - val_recon_loss: 1.7754\n",
      "Epoch 76/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.0493 - kl_loss: 2.7812 - recon_loss: 1.2149 - val_loss: 2.1623 - val_kl_loss: 2.7446 - val_recon_loss: 1.3389\n",
      "Epoch 77/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.0616 - kl_loss: 2.7935 - recon_loss: 1.2235 - val_loss: 2.1625 - val_kl_loss: 2.8293 - val_recon_loss: 1.3137\n",
      "Epoch 78/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.0459 - kl_loss: 2.8201 - recon_loss: 1.1999 - val_loss: 1.9875 - val_kl_loss: 2.5680 - val_recon_loss: 1.2171\n",
      "Epoch 79/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.0523 - kl_loss: 2.7869 - recon_loss: 1.2162 - val_loss: 2.3738 - val_kl_loss: 2.6844 - val_recon_loss: 1.5685\n",
      "Epoch 80/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.0304 - kl_loss: 2.7606 - recon_loss: 1.2023 - val_loss: 2.2603 - val_kl_loss: 2.5924 - val_recon_loss: 1.4826\n",
      "Epoch 81/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.0577 - kl_loss: 2.7694 - recon_loss: 1.2269 - val_loss: 2.3747 - val_kl_loss: 2.7554 - val_recon_loss: 1.5481\n",
      "Epoch 82/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 2.0115 - kl_loss: 2.7987 - recon_loss: 1.1719 - val_loss: 2.1209 - val_kl_loss: 2.6002 - val_recon_loss: 1.3408\n",
      "Epoch 83/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 2.0533 - kl_loss: 2.7457 - recon_loss: 1.2296 - val_loss: 2.3062 - val_kl_loss: 2.6425 - val_recon_loss: 1.5134\n",
      "Epoch 84/200\n",
      "1464/1464 [==============================] - 0s 69us/step - loss: 2.0338 - kl_loss: 2.7871 - recon_loss: 1.1976 - val_loss: 2.1750 - val_kl_loss: 2.6447 - val_recon_loss: 1.3816\n",
      "Epoch 85/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 2.0557 - kl_loss: 2.7931 - recon_loss: 1.2177 - val_loss: 2.4249 - val_kl_loss: 2.6909 - val_recon_loss: 1.6177\n",
      "Epoch 86/200\n",
      "1464/1464 [==============================] - 0s 74us/step - loss: 2.0212 - kl_loss: 2.7926 - recon_loss: 1.1834 - val_loss: 2.1337 - val_kl_loss: 2.7406 - val_recon_loss: 1.3115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "1464/1464 [==============================] - 0s 69us/step - loss: 2.0405 - kl_loss: 2.7430 - recon_loss: 1.2176 - val_loss: 2.4093 - val_kl_loss: 2.7323 - val_recon_loss: 1.5896\n",
      "Epoch 88/200\n",
      "1464/1464 [==============================] - 0s 73us/step - loss: 2.0399 - kl_loss: 2.7946 - recon_loss: 1.2015 - val_loss: 2.3606 - val_kl_loss: 2.6771 - val_recon_loss: 1.5575\n",
      "Epoch 89/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 2.0185 - kl_loss: 2.7448 - recon_loss: 1.1951 - val_loss: 2.9962 - val_kl_loss: 2.8746 - val_recon_loss: 2.1338\n",
      "Epoch 90/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 2.0209 - kl_loss: 2.7720 - recon_loss: 1.1893 - val_loss: 2.1441 - val_kl_loss: 2.7076 - val_recon_loss: 1.3318\n",
      "Epoch 91/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 2.0105 - kl_loss: 2.7475 - recon_loss: 1.1863 - val_loss: 2.4728 - val_kl_loss: 2.8761 - val_recon_loss: 1.6100\n",
      "Epoch 92/200\n",
      "1464/1464 [==============================] - 0s 88us/step - loss: 2.0384 - kl_loss: 2.8216 - recon_loss: 1.1919 - val_loss: 2.6789 - val_kl_loss: 2.8262 - val_recon_loss: 1.8310\n",
      "Epoch 93/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 2.0197 - kl_loss: 2.7540 - recon_loss: 1.1935 - val_loss: 2.1006 - val_kl_loss: 2.6461 - val_recon_loss: 1.3067\n",
      "Epoch 94/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.9982 - kl_loss: 2.7463 - recon_loss: 1.1743 - val_loss: 2.1475 - val_kl_loss: 2.7531 - val_recon_loss: 1.3216\n",
      "Epoch 95/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 2.0404 - kl_loss: 2.7465 - recon_loss: 1.2164 - val_loss: 1.9677 - val_kl_loss: 2.7880 - val_recon_loss: 1.1313\n",
      "Epoch 96/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.0227 - kl_loss: 2.7600 - recon_loss: 1.1947 - val_loss: 2.1181 - val_kl_loss: 2.6608 - val_recon_loss: 1.3199\n",
      "Epoch 97/200\n",
      "1464/1464 [==============================] - 0s 73us/step - loss: 2.0080 - kl_loss: 2.7098 - recon_loss: 1.1951 - val_loss: 2.1565 - val_kl_loss: 2.7551 - val_recon_loss: 1.3300\n",
      "Epoch 98/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 2.0157 - kl_loss: 2.7691 - recon_loss: 1.1850 - val_loss: 2.3223 - val_kl_loss: 2.7087 - val_recon_loss: 1.5097\n",
      "Epoch 99/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 2.0021 - kl_loss: 2.7430 - recon_loss: 1.1792 - val_loss: 1.9459 - val_kl_loss: 2.5312 - val_recon_loss: 1.1866\n",
      "Epoch 100/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 2.0046 - kl_loss: 2.7348 - recon_loss: 1.1842 - val_loss: 2.2054 - val_kl_loss: 2.7879 - val_recon_loss: 1.3690\n",
      "Epoch 101/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.9907 - kl_loss: 2.7230 - recon_loss: 1.1737 - val_loss: 2.1627 - val_kl_loss: 2.7099 - val_recon_loss: 1.3497\n",
      "Epoch 102/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9792 - kl_loss: 2.6815 - recon_loss: 1.1747 - val_loss: 1.9800 - val_kl_loss: 2.5870 - val_recon_loss: 1.2039\n",
      "Epoch 103/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.9577 - kl_loss: 2.7116 - recon_loss: 1.1443 - val_loss: 2.1233 - val_kl_loss: 2.6489 - val_recon_loss: 1.3287\n",
      "Epoch 104/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.9930 - kl_loss: 2.7148 - recon_loss: 1.1786 - val_loss: 2.3979 - val_kl_loss: 2.6706 - val_recon_loss: 1.5967\n",
      "Epoch 105/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 2.0173 - kl_loss: 2.7235 - recon_loss: 1.2003 - val_loss: 1.9310 - val_kl_loss: 2.7241 - val_recon_loss: 1.1138\n",
      "Epoch 106/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9581 - kl_loss: 2.6951 - recon_loss: 1.1495 - val_loss: 3.4310 - val_kl_loss: 2.6928 - val_recon_loss: 2.6232\n",
      "Epoch 107/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 2.0009 - kl_loss: 2.7095 - recon_loss: 1.1881 - val_loss: 1.9999 - val_kl_loss: 2.8994 - val_recon_loss: 1.1300\n",
      "Epoch 108/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9780 - kl_loss: 2.7542 - recon_loss: 1.1517 - val_loss: 2.0120 - val_kl_loss: 2.7070 - val_recon_loss: 1.1999\n",
      "Epoch 109/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.9889 - kl_loss: 2.7216 - recon_loss: 1.1724 - val_loss: 1.9934 - val_kl_loss: 2.5071 - val_recon_loss: 1.2413\n",
      "Epoch 110/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.9570 - kl_loss: 2.6927 - recon_loss: 1.1491 - val_loss: 2.4464 - val_kl_loss: 2.4796 - val_recon_loss: 1.7025\n",
      "Epoch 111/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.9812 - kl_loss: 2.6783 - recon_loss: 1.1777 - val_loss: 2.5461 - val_kl_loss: 2.6023 - val_recon_loss: 1.7654\n",
      "Epoch 112/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.9706 - kl_loss: 2.7118 - recon_loss: 1.1571 - val_loss: 2.5600 - val_kl_loss: 2.6738 - val_recon_loss: 1.7578\n",
      "Epoch 113/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9859 - kl_loss: 2.7060 - recon_loss: 1.1741 - val_loss: 1.9485 - val_kl_loss: 2.5754 - val_recon_loss: 1.1759\n",
      "Epoch 114/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.9728 - kl_loss: 2.7046 - recon_loss: 1.1614 - val_loss: 2.0232 - val_kl_loss: 2.7674 - val_recon_loss: 1.1930\n",
      "Epoch 115/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.9916 - kl_loss: 2.7253 - recon_loss: 1.1741 - val_loss: 2.3766 - val_kl_loss: 2.8784 - val_recon_loss: 1.5131\n",
      "Epoch 116/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 1.9516 - kl_loss: 2.7061 - recon_loss: 1.1398 - val_loss: 1.8906 - val_kl_loss: 2.6245 - val_recon_loss: 1.1033\n",
      "Epoch 117/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.9846 - kl_loss: 2.7201 - recon_loss: 1.1686 - val_loss: 1.9807 - val_kl_loss: 2.6017 - val_recon_loss: 1.2002\n",
      "Epoch 118/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.9409 - kl_loss: 2.6627 - recon_loss: 1.1421 - val_loss: 2.2671 - val_kl_loss: 2.5951 - val_recon_loss: 1.4886\n",
      "Epoch 119/200\n",
      "1464/1464 [==============================] - 0s 71us/step - loss: 1.9702 - kl_loss: 2.7045 - recon_loss: 1.1588 - val_loss: 1.9410 - val_kl_loss: 2.5761 - val_recon_loss: 1.1682\n",
      "Epoch 120/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.9450 - kl_loss: 2.6844 - recon_loss: 1.1397 - val_loss: 2.1710 - val_kl_loss: 2.5784 - val_recon_loss: 1.3975\n",
      "Epoch 121/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.9356 - kl_loss: 2.6433 - recon_loss: 1.1426 - val_loss: 2.0357 - val_kl_loss: 2.6001 - val_recon_loss: 1.2557\n",
      "Epoch 122/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.9604 - kl_loss: 2.6996 - recon_loss: 1.1506 - val_loss: 1.9130 - val_kl_loss: 2.7234 - val_recon_loss: 1.0960\n",
      "Epoch 123/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.9772 - kl_loss: 2.6786 - recon_loss: 1.1736 - val_loss: 2.3260 - val_kl_loss: 2.7310 - val_recon_loss: 1.5067\n",
      "Epoch 124/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9349 - kl_loss: 2.6603 - recon_loss: 1.1368 - val_loss: 2.1644 - val_kl_loss: 2.6744 - val_recon_loss: 1.3621\n",
      "Epoch 125/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.9128 - kl_loss: 2.6518 - recon_loss: 1.1172 - val_loss: 2.1020 - val_kl_loss: 2.5357 - val_recon_loss: 1.3413\n",
      "Epoch 126/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.9561 - kl_loss: 2.6789 - recon_loss: 1.1524 - val_loss: 2.0503 - val_kl_loss: 2.6507 - val_recon_loss: 1.2551\n",
      "Epoch 127/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9405 - kl_loss: 2.6537 - recon_loss: 1.1443 - val_loss: 1.9564 - val_kl_loss: 2.6810 - val_recon_loss: 1.1521\n",
      "Epoch 128/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.9317 - kl_loss: 2.6334 - recon_loss: 1.1417 - val_loss: 2.2337 - val_kl_loss: 2.5519 - val_recon_loss: 1.4681\n",
      "Epoch 129/200\n",
      "1464/1464 [==============================] - 0s 73us/step - loss: 1.9276 - kl_loss: 2.6266 - recon_loss: 1.1397 - val_loss: 1.9286 - val_kl_loss: 2.5383 - val_recon_loss: 1.1671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 1.9243 - kl_loss: 2.6409 - recon_loss: 1.1321 - val_loss: 2.0223 - val_kl_loss: 2.5965 - val_recon_loss: 1.2433\n",
      "Epoch 131/200\n",
      "1464/1464 [==============================] - 0s 87us/step - loss: 1.9122 - kl_loss: 2.6248 - recon_loss: 1.1248 - val_loss: 2.0519 - val_kl_loss: 2.5708 - val_recon_loss: 1.2807\n",
      "Epoch 132/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.9037 - kl_loss: 2.6198 - recon_loss: 1.1177 - val_loss: 1.9918 - val_kl_loss: 2.5083 - val_recon_loss: 1.2393\n",
      "Epoch 133/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.9246 - kl_loss: 2.6308 - recon_loss: 1.1354 - val_loss: 1.9874 - val_kl_loss: 2.6103 - val_recon_loss: 1.2043\n",
      "Epoch 134/200\n",
      "1464/1464 [==============================] - 0s 89us/step - loss: 1.9016 - kl_loss: 2.5973 - recon_loss: 1.1224 - val_loss: 2.1625 - val_kl_loss: 2.6207 - val_recon_loss: 1.3763\n",
      "Epoch 135/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.9221 - kl_loss: 2.6273 - recon_loss: 1.1339 - val_loss: 2.3941 - val_kl_loss: 2.6288 - val_recon_loss: 1.6055\n",
      "Epoch 136/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.9391 - kl_loss: 2.6541 - recon_loss: 1.1429 - val_loss: 2.0650 - val_kl_loss: 2.5920 - val_recon_loss: 1.2874\n",
      "Epoch 137/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9273 - kl_loss: 2.6342 - recon_loss: 1.1371 - val_loss: 2.0817 - val_kl_loss: 2.5466 - val_recon_loss: 1.3177\n",
      "Epoch 138/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 1.9162 - kl_loss: 2.6285 - recon_loss: 1.1276 - val_loss: 2.4645 - val_kl_loss: 2.5080 - val_recon_loss: 1.7121\n",
      "Epoch 139/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.9139 - kl_loss: 2.6212 - recon_loss: 1.1276 - val_loss: 2.3817 - val_kl_loss: 2.5607 - val_recon_loss: 1.6135\n",
      "Epoch 140/200\n",
      "1464/1464 [==============================] - 0s 89us/step - loss: 1.9439 - kl_loss: 2.6332 - recon_loss: 1.1539 - val_loss: 1.8789 - val_kl_loss: 2.5972 - val_recon_loss: 1.0997\n",
      "Epoch 141/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 1.9270 - kl_loss: 2.6484 - recon_loss: 1.1325 - val_loss: 1.9030 - val_kl_loss: 2.5308 - val_recon_loss: 1.1438\n",
      "Epoch 142/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.9356 - kl_loss: 2.6519 - recon_loss: 1.1400 - val_loss: 2.0556 - val_kl_loss: 2.7039 - val_recon_loss: 1.2444\n",
      "Epoch 143/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.9412 - kl_loss: 2.6762 - recon_loss: 1.1384 - val_loss: 2.0874 - val_kl_loss: 2.6774 - val_recon_loss: 1.2842\n",
      "Epoch 144/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.9298 - kl_loss: 2.6666 - recon_loss: 1.1298 - val_loss: 1.9786 - val_kl_loss: 2.5190 - val_recon_loss: 1.2229\n",
      "Epoch 145/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.8835 - kl_loss: 2.6092 - recon_loss: 1.1008 - val_loss: 2.3749 - val_kl_loss: 2.7649 - val_recon_loss: 1.5454\n",
      "Epoch 146/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.9206 - kl_loss: 2.6181 - recon_loss: 1.1352 - val_loss: 2.2243 - val_kl_loss: 2.7378 - val_recon_loss: 1.4029\n",
      "Epoch 147/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.9206 - kl_loss: 2.6149 - recon_loss: 1.1362 - val_loss: 2.2505 - val_kl_loss: 2.6824 - val_recon_loss: 1.4458\n",
      "Epoch 148/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 1.9014 - kl_loss: 2.5954 - recon_loss: 1.1228 - val_loss: 1.9538 - val_kl_loss: 2.4615 - val_recon_loss: 1.2154\n",
      "Epoch 149/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.8900 - kl_loss: 2.6172 - recon_loss: 1.1048 - val_loss: 2.1031 - val_kl_loss: 2.4702 - val_recon_loss: 1.3620\n",
      "Epoch 150/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.9005 - kl_loss: 2.5853 - recon_loss: 1.1249 - val_loss: 1.9638 - val_kl_loss: 2.5045 - val_recon_loss: 1.2125\n",
      "Epoch 151/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.8996 - kl_loss: 2.5764 - recon_loss: 1.1267 - val_loss: 1.9240 - val_kl_loss: 2.5744 - val_recon_loss: 1.1517\n",
      "Epoch 152/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.8984 - kl_loss: 2.5735 - recon_loss: 1.1263 - val_loss: 2.2179 - val_kl_loss: 2.6622 - val_recon_loss: 1.4192\n",
      "Epoch 153/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.8997 - kl_loss: 2.6373 - recon_loss: 1.1085 - val_loss: 1.8611 - val_kl_loss: 2.4648 - val_recon_loss: 1.1217\n",
      "Epoch 154/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 1.8956 - kl_loss: 2.6053 - recon_loss: 1.1140 - val_loss: 2.0724 - val_kl_loss: 2.4441 - val_recon_loss: 1.3392\n",
      "Epoch 155/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.9105 - kl_loss: 2.5652 - recon_loss: 1.1410 - val_loss: 1.9706 - val_kl_loss: 2.4775 - val_recon_loss: 1.2273\n",
      "Epoch 156/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8670 - kl_loss: 2.5481 - recon_loss: 1.1026 - val_loss: 2.2031 - val_kl_loss: 2.5192 - val_recon_loss: 1.4473\n",
      "Epoch 157/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.9263 - kl_loss: 2.6011 - recon_loss: 1.1460 - val_loss: 2.0935 - val_kl_loss: 2.7374 - val_recon_loss: 1.2723\n",
      "Epoch 158/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.8756 - kl_loss: 2.6039 - recon_loss: 1.0945 - val_loss: 2.0564 - val_kl_loss: 2.5519 - val_recon_loss: 1.2908\n",
      "Epoch 159/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.8946 - kl_loss: 2.5687 - recon_loss: 1.1239 - val_loss: 2.1128 - val_kl_loss: 2.5762 - val_recon_loss: 1.3400\n",
      "Epoch 160/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 1.9095 - kl_loss: 2.6159 - recon_loss: 1.1247 - val_loss: 2.0500 - val_kl_loss: 2.5265 - val_recon_loss: 1.2921\n",
      "Epoch 161/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.9304 - kl_loss: 2.5551 - recon_loss: 1.1639 - val_loss: 1.9341 - val_kl_loss: 2.5778 - val_recon_loss: 1.1608\n",
      "Epoch 162/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.8981 - kl_loss: 2.5895 - recon_loss: 1.1212 - val_loss: 2.0402 - val_kl_loss: 2.6563 - val_recon_loss: 1.2433\n",
      "Epoch 163/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.8733 - kl_loss: 2.5727 - recon_loss: 1.1015 - val_loss: 2.2165 - val_kl_loss: 2.7183 - val_recon_loss: 1.4010\n",
      "Epoch 164/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.8609 - kl_loss: 2.5396 - recon_loss: 1.0990 - val_loss: 2.5052 - val_kl_loss: 2.5447 - val_recon_loss: 1.7418\n",
      "Epoch 165/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.8977 - kl_loss: 2.5372 - recon_loss: 1.1365 - val_loss: 2.0691 - val_kl_loss: 2.5587 - val_recon_loss: 1.3015\n",
      "Epoch 166/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 1.8890 - kl_loss: 2.5295 - recon_loss: 1.1302 - val_loss: 2.0282 - val_kl_loss: 2.6045 - val_recon_loss: 1.2469\n",
      "Epoch 167/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8756 - kl_loss: 2.5690 - recon_loss: 1.1049 - val_loss: 2.2576 - val_kl_loss: 2.6370 - val_recon_loss: 1.4665\n",
      "Epoch 168/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8638 - kl_loss: 2.5543 - recon_loss: 1.0975 - val_loss: 2.0425 - val_kl_loss: 2.4393 - val_recon_loss: 1.3107\n",
      "Epoch 169/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.8761 - kl_loss: 2.5468 - recon_loss: 1.1121 - val_loss: 2.4470 - val_kl_loss: 2.6850 - val_recon_loss: 1.6415\n",
      "Epoch 170/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8775 - kl_loss: 2.5537 - recon_loss: 1.1114 - val_loss: 2.0520 - val_kl_loss: 2.4910 - val_recon_loss: 1.3047\n",
      "Epoch 171/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.8909 - kl_loss: 2.5368 - recon_loss: 1.1298 - val_loss: 1.9198 - val_kl_loss: 2.4733 - val_recon_loss: 1.1778\n",
      "Epoch 172/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.8715 - kl_loss: 2.5467 - recon_loss: 1.1075 - val_loss: 1.9113 - val_kl_loss: 2.4831 - val_recon_loss: 1.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "1464/1464 [==============================] - 0s 77us/step - loss: 1.8542 - kl_loss: 2.5365 - recon_loss: 1.0933 - val_loss: 2.0977 - val_kl_loss: 2.3999 - val_recon_loss: 1.3777\n",
      "Epoch 174/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8610 - kl_loss: 2.4714 - recon_loss: 1.1196 - val_loss: 2.1652 - val_kl_loss: 2.5818 - val_recon_loss: 1.3907\n",
      "Epoch 175/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.8552 - kl_loss: 2.4915 - recon_loss: 1.1078 - val_loss: 2.1941 - val_kl_loss: 2.3358 - val_recon_loss: 1.4933\n",
      "Epoch 176/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.8459 - kl_loss: 2.4973 - recon_loss: 1.0967 - val_loss: 2.3741 - val_kl_loss: 2.4001 - val_recon_loss: 1.6540\n",
      "Epoch 177/200\n",
      "1464/1464 [==============================] - 0s 76us/step - loss: 1.8633 - kl_loss: 2.5294 - recon_loss: 1.1045 - val_loss: 2.4032 - val_kl_loss: 2.5376 - val_recon_loss: 1.6420\n",
      "Epoch 178/200\n",
      "1464/1464 [==============================] - 0s 79us/step - loss: 1.8646 - kl_loss: 2.5184 - recon_loss: 1.1091 - val_loss: 1.8571 - val_kl_loss: 2.6041 - val_recon_loss: 1.0758\n",
      "Epoch 179/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.8631 - kl_loss: 2.5381 - recon_loss: 1.1017 - val_loss: 1.9658 - val_kl_loss: 2.3988 - val_recon_loss: 1.2462\n",
      "Epoch 180/200\n",
      "1464/1464 [==============================] - 0s 86us/step - loss: 1.8276 - kl_loss: 2.4707 - recon_loss: 1.0864 - val_loss: 2.0241 - val_kl_loss: 2.3679 - val_recon_loss: 1.3137\n",
      "Epoch 181/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.8681 - kl_loss: 2.4777 - recon_loss: 1.1248 - val_loss: 2.1553 - val_kl_loss: 2.5593 - val_recon_loss: 1.3875\n",
      "Epoch 182/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8777 - kl_loss: 2.5339 - recon_loss: 1.1175 - val_loss: 2.2867 - val_kl_loss: 2.5278 - val_recon_loss: 1.5284\n",
      "Epoch 183/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8700 - kl_loss: 2.4980 - recon_loss: 1.1206 - val_loss: 2.2594 - val_kl_loss: 2.3723 - val_recon_loss: 1.5477\n",
      "Epoch 184/200\n",
      "1464/1464 [==============================] - 0s 87us/step - loss: 1.8364 - kl_loss: 2.4922 - recon_loss: 1.0888 - val_loss: 2.3658 - val_kl_loss: 2.4035 - val_recon_loss: 1.6447\n",
      "Epoch 185/200\n",
      "1464/1464 [==============================] - 0s 80us/step - loss: 1.8595 - kl_loss: 2.4864 - recon_loss: 1.1136 - val_loss: 1.9471 - val_kl_loss: 2.4644 - val_recon_loss: 1.2078\n",
      "Epoch 186/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8383 - kl_loss: 2.4861 - recon_loss: 1.0925 - val_loss: 1.9898 - val_kl_loss: 2.5133 - val_recon_loss: 1.2358\n",
      "Epoch 187/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.8367 - kl_loss: 2.4894 - recon_loss: 1.0899 - val_loss: 1.9687 - val_kl_loss: 2.3669 - val_recon_loss: 1.2586\n",
      "Epoch 188/200\n",
      "1464/1464 [==============================] - 0s 83us/step - loss: 1.8449 - kl_loss: 2.4833 - recon_loss: 1.0999 - val_loss: 2.0278 - val_kl_loss: 2.4883 - val_recon_loss: 1.2813\n",
      "Epoch 189/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8562 - kl_loss: 2.5086 - recon_loss: 1.1036 - val_loss: 1.8469 - val_kl_loss: 2.4986 - val_recon_loss: 1.0973\n",
      "Epoch 190/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.8348 - kl_loss: 2.4971 - recon_loss: 1.0857 - val_loss: 2.6910 - val_kl_loss: 2.4264 - val_recon_loss: 1.9631\n",
      "Epoch 191/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.8621 - kl_loss: 2.4783 - recon_loss: 1.1186 - val_loss: 1.9205 - val_kl_loss: 2.6056 - val_recon_loss: 1.1389\n",
      "Epoch 192/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.8159 - kl_loss: 2.4586 - recon_loss: 1.0783 - val_loss: 1.8421 - val_kl_loss: 2.3470 - val_recon_loss: 1.1380\n",
      "Epoch 193/200\n",
      "1464/1464 [==============================] - 0s 85us/step - loss: 1.8394 - kl_loss: 2.4389 - recon_loss: 1.1077 - val_loss: 1.8764 - val_kl_loss: 2.3065 - val_recon_loss: 1.1844\n",
      "Epoch 194/200\n",
      "1464/1464 [==============================] - 0s 75us/step - loss: 1.8557 - kl_loss: 2.4586 - recon_loss: 1.1181 - val_loss: 2.0151 - val_kl_loss: 2.4909 - val_recon_loss: 1.2679\n",
      "Epoch 195/200\n",
      "1464/1464 [==============================] - 0s 78us/step - loss: 1.8431 - kl_loss: 2.5144 - recon_loss: 1.0888 - val_loss: 1.8601 - val_kl_loss: 2.4060 - val_recon_loss: 1.1383\n",
      "Epoch 196/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.8278 - kl_loss: 2.4611 - recon_loss: 1.0895 - val_loss: 1.8693 - val_kl_loss: 2.3389 - val_recon_loss: 1.1676\n",
      "Epoch 197/200\n",
      "1464/1464 [==============================] - 0s 84us/step - loss: 1.8456 - kl_loss: 2.4608 - recon_loss: 1.1073 - val_loss: 2.1101 - val_kl_loss: 2.4389 - val_recon_loss: 1.3785\n",
      "Epoch 198/200\n",
      "1464/1464 [==============================] - 0s 81us/step - loss: 1.8411 - kl_loss: 2.4636 - recon_loss: 1.1020 - val_loss: 1.9757 - val_kl_loss: 2.4726 - val_recon_loss: 1.2340\n",
      "Epoch 199/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8222 - kl_loss: 2.4709 - recon_loss: 1.0810 - val_loss: 2.0254 - val_kl_loss: 2.4222 - val_recon_loss: 1.2988\n",
      "Epoch 200/200\n",
      "1464/1464 [==============================] - 0s 82us/step - loss: 1.8474 - kl_loss: 2.4720 - recon_loss: 1.1058 - val_loss: 2.0745 - val_kl_loss: 2.5924 - val_recon_loss: 1.2968\n",
      "========================= Model 5 =========================\n",
      "complete model: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_true (InputLayer)             (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cond (InputLayer)               (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 2), (None, 2 6856        x_true[0][0]                     \n",
      "                                                                 cond[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sample_z (Lambda)               (None, 2)            0           encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 96)           6420        sample_z[0][0]                   \n",
      "                                                                 cond[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 13,276\n",
      "Trainable params: 13,276\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "encoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_x_true (InputLayer)         (None, 96)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_input (Concatenate)         (None, 110)          0           enc_x_true[0][0]                 \n",
      "                                                                 enc_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_0 (Dense)             (None, 48)           5328        enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_1 (Dense)             (None, 24)           1176        enc_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_dense_2 (Dense)             (None, 12)           300         enc_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_mu (Dense)         (None, 2)            26          enc_dense_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "latent_dense_log_sigma (Dense)  (None, 2)            26          enc_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,856\n",
      "Trainable params: 6,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "decoder: \n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_z (InputLayer)              (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_cond (InputLayer)           (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (Concatenate)         (None, 16)           0           dec_z[0][0]                      \n",
      "                                                                 dec_cond[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_0 (Dense)             (None, 12)           204         dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_1 (Dense)             (None, 24)           312         dec_dense_0[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_dense_2 (Dense)             (None, 48)           1200        dec_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dec_x_hat (Dense)               (None, 96)           4704        dec_dense_2[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 6,420\n",
      "Trainable params: 6,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "--- START TRAINING ---\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1465 samples, validate on 365 samples\n",
      "Epoch 1/200\n",
      "1465/1465 [==============================] - 0s 137us/step - loss: 41.5447 - kl_loss: 16.0246 - recon_loss: 36.7373 - val_loss: 13.5583 - val_kl_loss: 15.6504 - val_recon_loss: 8.8632\n",
      "Epoch 2/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 9.5396 - kl_loss: 11.0265 - recon_loss: 6.2317 - val_loss: 6.8401 - val_kl_loss: 6.9291 - val_recon_loss: 4.7614\n",
      "Epoch 3/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 6.5566 - kl_loss: 7.7929 - recon_loss: 4.2187 - val_loss: 5.4247 - val_kl_loss: 5.9734 - val_recon_loss: 3.6327\n",
      "Epoch 4/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 5.4573 - kl_loss: 6.9661 - recon_loss: 3.3675 - val_loss: 6.6339 - val_kl_loss: 6.5170 - val_recon_loss: 4.6788\n",
      "Epoch 5/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 4.7591 - kl_loss: 6.0290 - recon_loss: 2.9504 - val_loss: 5.5080 - val_kl_loss: 4.9842 - val_recon_loss: 4.0127\n",
      "Epoch 6/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 4.3854 - kl_loss: 5.6541 - recon_loss: 2.6892 - val_loss: 3.7802 - val_kl_loss: 4.7814 - val_recon_loss: 2.3458\n",
      "Epoch 7/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 4.1798 - kl_loss: 5.3830 - recon_loss: 2.5649 - val_loss: 4.5734 - val_kl_loss: 3.9920 - val_recon_loss: 3.3757\n",
      "Epoch 8/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 3.8631 - kl_loss: 4.9765 - recon_loss: 2.3702 - val_loss: 3.7676 - val_kl_loss: 4.3555 - val_recon_loss: 2.4610\n",
      "Epoch 9/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 3.6679 - kl_loss: 4.6937 - recon_loss: 2.2598 - val_loss: 3.4741 - val_kl_loss: 4.3955 - val_recon_loss: 2.1554\n",
      "Epoch 10/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.4948 - kl_loss: 4.4587 - recon_loss: 2.1571 - val_loss: 5.3130 - val_kl_loss: 4.4825 - val_recon_loss: 3.9683\n",
      "Epoch 11/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.3412 - kl_loss: 4.2999 - recon_loss: 2.0512 - val_loss: 2.9639 - val_kl_loss: 3.8450 - val_recon_loss: 1.8103\n",
      "Epoch 12/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 3.2860 - kl_loss: 4.2179 - recon_loss: 2.0206 - val_loss: 3.0825 - val_kl_loss: 3.7564 - val_recon_loss: 1.9556\n",
      "Epoch 13/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 3.2602 - kl_loss: 4.1106 - recon_loss: 2.0271 - val_loss: 3.4310 - val_kl_loss: 3.4499 - val_recon_loss: 2.3960\n",
      "Epoch 14/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 3.1282 - kl_loss: 4.0687 - recon_loss: 1.9076 - val_loss: 2.8595 - val_kl_loss: 3.8793 - val_recon_loss: 1.6957\n",
      "Epoch 15/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 3.0412 - kl_loss: 3.9716 - recon_loss: 1.8498 - val_loss: 3.4587 - val_kl_loss: 3.8318 - val_recon_loss: 2.3092\n",
      "Epoch 16/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.9923 - kl_loss: 3.9051 - recon_loss: 1.8208 - val_loss: 4.6676 - val_kl_loss: 3.8024 - val_recon_loss: 3.5269\n",
      "Epoch 17/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.8846 - kl_loss: 3.7401 - recon_loss: 1.7626 - val_loss: 4.4598 - val_kl_loss: 3.3130 - val_recon_loss: 3.4659\n",
      "Epoch 18/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 2.9562 - kl_loss: 3.7521 - recon_loss: 1.8306 - val_loss: 2.8394 - val_kl_loss: 3.3929 - val_recon_loss: 1.8216\n",
      "Epoch 19/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.8370 - kl_loss: 3.7677 - recon_loss: 1.7067 - val_loss: 3.4070 - val_kl_loss: 3.4579 - val_recon_loss: 2.3697\n",
      "Epoch 20/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.8651 - kl_loss: 3.7050 - recon_loss: 1.7536 - val_loss: 2.5640 - val_kl_loss: 3.3772 - val_recon_loss: 1.5509\n",
      "Epoch 21/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 2.7713 - kl_loss: 3.7318 - recon_loss: 1.6518 - val_loss: 2.8707 - val_kl_loss: 3.3033 - val_recon_loss: 1.8798\n",
      "Epoch 22/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.7377 - kl_loss: 3.7280 - recon_loss: 1.6193 - val_loss: 2.3971 - val_kl_loss: 3.2735 - val_recon_loss: 1.4150\n",
      "Epoch 23/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.6306 - kl_loss: 3.6301 - recon_loss: 1.5416 - val_loss: 4.1331 - val_kl_loss: 3.3679 - val_recon_loss: 3.1227\n",
      "Epoch 24/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.6638 - kl_loss: 3.5981 - recon_loss: 1.5844 - val_loss: 2.5920 - val_kl_loss: 3.4998 - val_recon_loss: 1.5421\n",
      "Epoch 25/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 2.5761 - kl_loss: 3.5619 - recon_loss: 1.5075 - val_loss: 3.0604 - val_kl_loss: 3.2387 - val_recon_loss: 2.0888\n",
      "Epoch 26/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.6091 - kl_loss: 3.5344 - recon_loss: 1.5488 - val_loss: 2.6048 - val_kl_loss: 3.2761 - val_recon_loss: 1.6219\n",
      "Epoch 27/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.5668 - kl_loss: 3.5392 - recon_loss: 1.5050 - val_loss: 2.3556 - val_kl_loss: 3.1818 - val_recon_loss: 1.4011\n",
      "Epoch 28/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.5094 - kl_loss: 3.4810 - recon_loss: 1.4651 - val_loss: 2.1781 - val_kl_loss: 3.3752 - val_recon_loss: 1.1656\n",
      "Epoch 29/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.4976 - kl_loss: 3.5023 - recon_loss: 1.4469 - val_loss: 2.3308 - val_kl_loss: 3.1832 - val_recon_loss: 1.3758\n",
      "Epoch 30/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.4788 - kl_loss: 3.5280 - recon_loss: 1.4204 - val_loss: 3.8150 - val_kl_loss: 3.1951 - val_recon_loss: 2.8565\n",
      "Epoch 31/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.4142 - kl_loss: 3.5026 - recon_loss: 1.3634 - val_loss: 2.8651 - val_kl_loss: 3.0766 - val_recon_loss: 1.9422\n",
      "Epoch 32/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3974 - kl_loss: 3.4499 - recon_loss: 1.3624 - val_loss: 3.1059 - val_kl_loss: 2.9847 - val_recon_loss: 2.2105\n",
      "Epoch 33/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.4245 - kl_loss: 3.4974 - recon_loss: 1.3753 - val_loss: 2.1841 - val_kl_loss: 3.1307 - val_recon_loss: 1.2449\n",
      "Epoch 34/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 2.3939 - kl_loss: 3.4681 - recon_loss: 1.3535 - val_loss: 2.4970 - val_kl_loss: 3.1186 - val_recon_loss: 1.5614\n",
      "Epoch 35/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.3678 - kl_loss: 3.4357 - recon_loss: 1.3371 - val_loss: 3.3872 - val_kl_loss: 3.1527 - val_recon_loss: 2.4414\n",
      "Epoch 36/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.3332 - kl_loss: 3.4389 - recon_loss: 1.3015 - val_loss: 2.3721 - val_kl_loss: 3.0775 - val_recon_loss: 1.4488\n",
      "Epoch 37/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.3321 - kl_loss: 3.3916 - recon_loss: 1.3146 - val_loss: 2.0881 - val_kl_loss: 3.0240 - val_recon_loss: 1.1809\n",
      "Epoch 38/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.2991 - kl_loss: 3.3793 - recon_loss: 1.2853 - val_loss: 2.2306 - val_kl_loss: 3.2681 - val_recon_loss: 1.2502\n",
      "Epoch 39/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.3307 - kl_loss: 3.3987 - recon_loss: 1.3111 - val_loss: 2.4464 - val_kl_loss: 3.1697 - val_recon_loss: 1.4954\n",
      "Epoch 40/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.3019 - kl_loss: 3.3839 - recon_loss: 1.2868 - val_loss: 2.2326 - val_kl_loss: 3.3360 - val_recon_loss: 1.2318\n",
      "Epoch 41/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.2786 - kl_loss: 3.4539 - recon_loss: 1.2425 - val_loss: 2.8690 - val_kl_loss: 3.3235 - val_recon_loss: 1.8720\n",
      "Epoch 42/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.2585 - kl_loss: 3.4112 - recon_loss: 1.2352 - val_loss: 1.9450 - val_kl_loss: 3.1698 - val_recon_loss: 0.9940\n",
      "Epoch 43/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.2259 - kl_loss: 3.3814 - recon_loss: 1.2115 - val_loss: 2.1137 - val_kl_loss: 3.1582 - val_recon_loss: 1.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.2189 - kl_loss: 3.3416 - recon_loss: 1.2165 - val_loss: 2.4093 - val_kl_loss: 3.1365 - val_recon_loss: 1.4683\n",
      "Epoch 45/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1789 - kl_loss: 3.4281 - recon_loss: 1.1505 - val_loss: 1.8631 - val_kl_loss: 3.0171 - val_recon_loss: 0.9580\n",
      "Epoch 46/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.2071 - kl_loss: 3.3220 - recon_loss: 1.2105 - val_loss: 1.9622 - val_kl_loss: 3.0816 - val_recon_loss: 1.0377\n",
      "Epoch 47/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.1820 - kl_loss: 3.3030 - recon_loss: 1.1911 - val_loss: 2.2376 - val_kl_loss: 3.0611 - val_recon_loss: 1.3193\n",
      "Epoch 48/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.1806 - kl_loss: 3.3525 - recon_loss: 1.1748 - val_loss: 1.9877 - val_kl_loss: 3.2750 - val_recon_loss: 1.0052\n",
      "Epoch 49/200\n",
      "1465/1465 [==============================] - 0s 74us/step - loss: 2.1738 - kl_loss: 3.3752 - recon_loss: 1.1612 - val_loss: 2.4521 - val_kl_loss: 2.9122 - val_recon_loss: 1.5785\n",
      "Epoch 50/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1785 - kl_loss: 3.3012 - recon_loss: 1.1882 - val_loss: 1.9446 - val_kl_loss: 3.0729 - val_recon_loss: 1.0228\n",
      "Epoch 51/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1584 - kl_loss: 3.2820 - recon_loss: 1.1738 - val_loss: 2.2241 - val_kl_loss: 3.0948 - val_recon_loss: 1.2956\n",
      "Epoch 52/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0948 - kl_loss: 3.2822 - recon_loss: 1.1102 - val_loss: 3.9563 - val_kl_loss: 3.2360 - val_recon_loss: 2.9855\n",
      "Epoch 53/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.1631 - kl_loss: 3.3162 - recon_loss: 1.1683 - val_loss: 2.1165 - val_kl_loss: 3.1987 - val_recon_loss: 1.1569\n",
      "Epoch 54/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 2.1343 - kl_loss: 3.2888 - recon_loss: 1.1477 - val_loss: 2.2321 - val_kl_loss: 2.9865 - val_recon_loss: 1.3361\n",
      "Epoch 55/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1269 - kl_loss: 3.2801 - recon_loss: 1.1429 - val_loss: 2.0516 - val_kl_loss: 2.9027 - val_recon_loss: 1.1808\n",
      "Epoch 56/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.1048 - kl_loss: 3.3149 - recon_loss: 1.1103 - val_loss: 2.9395 - val_kl_loss: 3.1966 - val_recon_loss: 1.9805\n",
      "Epoch 57/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 2.1129 - kl_loss: 3.3161 - recon_loss: 1.1181 - val_loss: 1.9466 - val_kl_loss: 3.0051 - val_recon_loss: 1.0450\n",
      "Epoch 58/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 2.1069 - kl_loss: 3.2915 - recon_loss: 1.1195 - val_loss: 2.3281 - val_kl_loss: 3.2314 - val_recon_loss: 1.3587\n",
      "Epoch 59/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.1101 - kl_loss: 3.3153 - recon_loss: 1.1156 - val_loss: 2.2477 - val_kl_loss: 2.9785 - val_recon_loss: 1.3542\n",
      "Epoch 60/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 2.0721 - kl_loss: 3.2775 - recon_loss: 1.0889 - val_loss: 2.5509 - val_kl_loss: 2.9560 - val_recon_loss: 1.6641\n",
      "Epoch 61/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.1124 - kl_loss: 3.2901 - recon_loss: 1.1254 - val_loss: 1.8813 - val_kl_loss: 3.1108 - val_recon_loss: 0.9481\n",
      "Epoch 62/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0518 - kl_loss: 3.2651 - recon_loss: 1.0723 - val_loss: 2.4559 - val_kl_loss: 3.0697 - val_recon_loss: 1.5350\n",
      "Epoch 63/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.0625 - kl_loss: 3.2317 - recon_loss: 1.0930 - val_loss: 1.9682 - val_kl_loss: 2.9656 - val_recon_loss: 1.0786\n",
      "Epoch 64/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0836 - kl_loss: 3.2804 - recon_loss: 1.0995 - val_loss: 2.0288 - val_kl_loss: 3.0440 - val_recon_loss: 1.1156\n",
      "Epoch 65/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0827 - kl_loss: 3.3280 - recon_loss: 1.0843 - val_loss: 1.8233 - val_kl_loss: 2.9702 - val_recon_loss: 0.9323\n",
      "Epoch 66/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0451 - kl_loss: 3.2303 - recon_loss: 1.0760 - val_loss: 2.3839 - val_kl_loss: 3.0943 - val_recon_loss: 1.4556\n",
      "Epoch 67/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 2.0728 - kl_loss: 3.2489 - recon_loss: 1.0981 - val_loss: 2.1739 - val_kl_loss: 3.1686 - val_recon_loss: 1.2233\n",
      "Epoch 68/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0423 - kl_loss: 3.2445 - recon_loss: 1.0690 - val_loss: 2.1848 - val_kl_loss: 3.1389 - val_recon_loss: 1.2432\n",
      "Epoch 69/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0506 - kl_loss: 3.2433 - recon_loss: 1.0776 - val_loss: 2.2015 - val_kl_loss: 2.8979 - val_recon_loss: 1.3322\n",
      "Epoch 70/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 2.0479 - kl_loss: 3.2581 - recon_loss: 1.0704 - val_loss: 2.0158 - val_kl_loss: 3.0924 - val_recon_loss: 1.0881\n",
      "Epoch 71/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 2.0272 - kl_loss: 3.2648 - recon_loss: 1.0478 - val_loss: 1.9448 - val_kl_loss: 3.0340 - val_recon_loss: 1.0346\n",
      "Epoch 72/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 2.0149 - kl_loss: 3.2285 - recon_loss: 1.0463 - val_loss: 1.9285 - val_kl_loss: 3.1165 - val_recon_loss: 0.9936\n",
      "Epoch 73/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0347 - kl_loss: 3.2708 - recon_loss: 1.0535 - val_loss: 1.9372 - val_kl_loss: 3.0092 - val_recon_loss: 1.0345\n",
      "Epoch 74/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 2.0278 - kl_loss: 3.2088 - recon_loss: 1.0651 - val_loss: 1.8110 - val_kl_loss: 2.8738 - val_recon_loss: 0.9489\n",
      "Epoch 75/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.9855 - kl_loss: 3.2082 - recon_loss: 1.0230 - val_loss: 2.0958 - val_kl_loss: 3.0057 - val_recon_loss: 1.1941\n",
      "Epoch 76/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.9945 - kl_loss: 3.1721 - recon_loss: 1.0428 - val_loss: 1.8630 - val_kl_loss: 2.9223 - val_recon_loss: 0.9863\n",
      "Epoch 77/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.9859 - kl_loss: 3.2088 - recon_loss: 1.0232 - val_loss: 2.3276 - val_kl_loss: 3.0035 - val_recon_loss: 1.4265\n",
      "Epoch 78/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.9849 - kl_loss: 3.1773 - recon_loss: 1.0317 - val_loss: 1.7576 - val_kl_loss: 2.9234 - val_recon_loss: 0.8806\n",
      "Epoch 79/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9882 - kl_loss: 3.1698 - recon_loss: 1.0372 - val_loss: 2.3310 - val_kl_loss: 2.8893 - val_recon_loss: 1.4642\n",
      "Epoch 80/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 2.0114 - kl_loss: 3.1710 - recon_loss: 1.0601 - val_loss: 2.8474 - val_kl_loss: 3.0457 - val_recon_loss: 1.9337\n",
      "Epoch 81/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 2.0246 - kl_loss: 3.2273 - recon_loss: 1.0564 - val_loss: 1.9094 - val_kl_loss: 2.8862 - val_recon_loss: 1.0435\n",
      "Epoch 82/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9954 - kl_loss: 3.1499 - recon_loss: 1.0505 - val_loss: 1.7655 - val_kl_loss: 2.8925 - val_recon_loss: 0.8977\n",
      "Epoch 83/200\n",
      "1465/1465 [==============================] - 0s 70us/step - loss: 1.9578 - kl_loss: 3.1489 - recon_loss: 1.0131 - val_loss: 2.1522 - val_kl_loss: 2.9033 - val_recon_loss: 1.2812\n",
      "Epoch 84/200\n",
      "1465/1465 [==============================] - 0s 72us/step - loss: 1.9870 - kl_loss: 3.1849 - recon_loss: 1.0315 - val_loss: 1.7587 - val_kl_loss: 2.9612 - val_recon_loss: 0.8704\n",
      "Epoch 85/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.9749 - kl_loss: 3.1594 - recon_loss: 1.0271 - val_loss: 1.8818 - val_kl_loss: 3.0534 - val_recon_loss: 0.9658\n",
      "Epoch 86/200\n",
      "1465/1465 [==============================] - 0s 66us/step - loss: 1.9721 - kl_loss: 3.1953 - recon_loss: 1.0135 - val_loss: 1.8366 - val_kl_loss: 2.9728 - val_recon_loss: 0.9448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9579 - kl_loss: 3.1577 - recon_loss: 1.0106 - val_loss: 1.7777 - val_kl_loss: 3.0282 - val_recon_loss: 0.8693\n",
      "Epoch 88/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.9298 - kl_loss: 3.1530 - recon_loss: 0.9839 - val_loss: 1.9118 - val_kl_loss: 2.9545 - val_recon_loss: 1.0254\n",
      "Epoch 89/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9635 - kl_loss: 3.1366 - recon_loss: 1.0225 - val_loss: 1.7612 - val_kl_loss: 2.8840 - val_recon_loss: 0.8960\n",
      "Epoch 90/200\n",
      "1465/1465 [==============================] - 0s 90us/step - loss: 1.9776 - kl_loss: 3.1834 - recon_loss: 1.0226 - val_loss: 1.9135 - val_kl_loss: 2.9666 - val_recon_loss: 1.0235\n",
      "Epoch 91/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.9627 - kl_loss: 3.1592 - recon_loss: 1.0150 - val_loss: 1.8010 - val_kl_loss: 2.8351 - val_recon_loss: 0.9504\n",
      "Epoch 92/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.9569 - kl_loss: 3.1771 - recon_loss: 1.0037 - val_loss: 2.1042 - val_kl_loss: 2.9156 - val_recon_loss: 1.2295\n",
      "Epoch 93/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.9471 - kl_loss: 3.1140 - recon_loss: 1.0129 - val_loss: 1.9256 - val_kl_loss: 2.8361 - val_recon_loss: 1.0748\n",
      "Epoch 94/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.9142 - kl_loss: 3.1336 - recon_loss: 0.9742 - val_loss: 1.9967 - val_kl_loss: 2.9336 - val_recon_loss: 1.1166\n",
      "Epoch 95/200\n",
      "1465/1465 [==============================] - 0s 87us/step - loss: 1.9297 - kl_loss: 3.1448 - recon_loss: 0.9863 - val_loss: 2.0507 - val_kl_loss: 2.7867 - val_recon_loss: 1.2147\n",
      "Epoch 96/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9408 - kl_loss: 3.1232 - recon_loss: 1.0038 - val_loss: 1.7851 - val_kl_loss: 2.9720 - val_recon_loss: 0.8935\n",
      "Epoch 97/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.9106 - kl_loss: 3.0959 - recon_loss: 0.9818 - val_loss: 2.1127 - val_kl_loss: 2.7361 - val_recon_loss: 1.2919\n",
      "Epoch 98/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9113 - kl_loss: 3.1073 - recon_loss: 0.9791 - val_loss: 1.9338 - val_kl_loss: 3.0193 - val_recon_loss: 1.0281\n",
      "Epoch 99/200\n",
      "1465/1465 [==============================] - 0s 71us/step - loss: 1.9343 - kl_loss: 3.1052 - recon_loss: 1.0027 - val_loss: 1.8707 - val_kl_loss: 2.8206 - val_recon_loss: 1.0245\n",
      "Epoch 100/200\n",
      "1465/1465 [==============================] - 0s 67us/step - loss: 1.8940 - kl_loss: 3.0706 - recon_loss: 0.9729 - val_loss: 1.7685 - val_kl_loss: 2.7175 - val_recon_loss: 0.9532\n",
      "Epoch 101/200\n",
      "1465/1465 [==============================] - 0s 67us/step - loss: 1.9404 - kl_loss: 3.0915 - recon_loss: 1.0130 - val_loss: 1.8459 - val_kl_loss: 3.0023 - val_recon_loss: 0.9452\n",
      "Epoch 102/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9285 - kl_loss: 3.1536 - recon_loss: 0.9824 - val_loss: 2.2792 - val_kl_loss: 2.9613 - val_recon_loss: 1.3908\n",
      "Epoch 103/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.9391 - kl_loss: 3.1344 - recon_loss: 0.9988 - val_loss: 2.0617 - val_kl_loss: 2.9905 - val_recon_loss: 1.1645\n",
      "Epoch 104/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8975 - kl_loss: 3.1096 - recon_loss: 0.9646 - val_loss: 2.2694 - val_kl_loss: 2.8994 - val_recon_loss: 1.3996\n",
      "Epoch 105/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8742 - kl_loss: 3.0534 - recon_loss: 0.9582 - val_loss: 1.9023 - val_kl_loss: 2.8586 - val_recon_loss: 1.0447\n",
      "Epoch 106/200\n",
      "1465/1465 [==============================] - 0s 89us/step - loss: 1.9273 - kl_loss: 3.1064 - recon_loss: 0.9954 - val_loss: 1.9272 - val_kl_loss: 2.8081 - val_recon_loss: 1.0847\n",
      "Epoch 107/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9063 - kl_loss: 3.0599 - recon_loss: 0.9883 - val_loss: 1.7309 - val_kl_loss: 2.7945 - val_recon_loss: 0.8926\n",
      "Epoch 108/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.9022 - kl_loss: 3.0754 - recon_loss: 0.9796 - val_loss: 2.2142 - val_kl_loss: 3.0369 - val_recon_loss: 1.3032\n",
      "Epoch 109/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.9124 - kl_loss: 3.1263 - recon_loss: 0.9745 - val_loss: 2.0182 - val_kl_loss: 2.8254 - val_recon_loss: 1.1706\n",
      "Epoch 110/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8868 - kl_loss: 3.0814 - recon_loss: 0.9624 - val_loss: 2.1673 - val_kl_loss: 2.6859 - val_recon_loss: 1.3616\n",
      "Epoch 111/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9131 - kl_loss: 3.0557 - recon_loss: 0.9964 - val_loss: 1.8734 - val_kl_loss: 2.8590 - val_recon_loss: 1.0157\n",
      "Epoch 112/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8895 - kl_loss: 3.0873 - recon_loss: 0.9633 - val_loss: 1.9577 - val_kl_loss: 2.8125 - val_recon_loss: 1.1140\n",
      "Epoch 113/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8778 - kl_loss: 3.0537 - recon_loss: 0.9616 - val_loss: 1.8216 - val_kl_loss: 2.7994 - val_recon_loss: 0.9818\n",
      "Epoch 114/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.9041 - kl_loss: 3.1051 - recon_loss: 0.9725 - val_loss: 2.0501 - val_kl_loss: 3.0574 - val_recon_loss: 1.1329\n",
      "Epoch 115/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8794 - kl_loss: 3.0551 - recon_loss: 0.9628 - val_loss: 1.6748 - val_kl_loss: 2.8528 - val_recon_loss: 0.8189\n",
      "Epoch 116/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8876 - kl_loss: 3.0859 - recon_loss: 0.9618 - val_loss: 1.7524 - val_kl_loss: 2.7834 - val_recon_loss: 0.9173\n",
      "Epoch 117/200\n",
      "1465/1465 [==============================] - 0s 75us/step - loss: 1.8914 - kl_loss: 3.0450 - recon_loss: 0.9779 - val_loss: 1.7180 - val_kl_loss: 2.9178 - val_recon_loss: 0.8426\n",
      "Epoch 118/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8894 - kl_loss: 3.0929 - recon_loss: 0.9615 - val_loss: 1.9379 - val_kl_loss: 3.0011 - val_recon_loss: 1.0376\n",
      "Epoch 119/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8808 - kl_loss: 3.0968 - recon_loss: 0.9518 - val_loss: 1.9066 - val_kl_loss: 2.8242 - val_recon_loss: 1.0593\n",
      "Epoch 120/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8381 - kl_loss: 3.0246 - recon_loss: 0.9307 - val_loss: 1.7356 - val_kl_loss: 2.8269 - val_recon_loss: 0.8875\n",
      "Epoch 121/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8705 - kl_loss: 3.0414 - recon_loss: 0.9581 - val_loss: 1.8542 - val_kl_loss: 3.0629 - val_recon_loss: 0.9353\n",
      "Epoch 122/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.8656 - kl_loss: 3.0867 - recon_loss: 0.9396 - val_loss: 1.8865 - val_kl_loss: 2.8882 - val_recon_loss: 1.0200\n",
      "Epoch 123/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8448 - kl_loss: 2.9990 - recon_loss: 0.9451 - val_loss: 1.6925 - val_kl_loss: 2.7914 - val_recon_loss: 0.8550\n",
      "Epoch 124/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.8542 - kl_loss: 3.0263 - recon_loss: 0.9463 - val_loss: 1.7348 - val_kl_loss: 2.8953 - val_recon_loss: 0.8663\n",
      "Epoch 125/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8605 - kl_loss: 3.0312 - recon_loss: 0.9511 - val_loss: 2.1533 - val_kl_loss: 2.9430 - val_recon_loss: 1.2704\n",
      "Epoch 126/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.8267 - kl_loss: 3.0348 - recon_loss: 0.9163 - val_loss: 1.7522 - val_kl_loss: 2.9091 - val_recon_loss: 0.8794\n",
      "Epoch 127/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8410 - kl_loss: 3.0565 - recon_loss: 0.9241 - val_loss: 1.7162 - val_kl_loss: 2.7789 - val_recon_loss: 0.8825\n",
      "Epoch 128/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8426 - kl_loss: 3.0025 - recon_loss: 0.9418 - val_loss: 1.7106 - val_kl_loss: 2.7923 - val_recon_loss: 0.8729\n",
      "Epoch 129/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8442 - kl_loss: 3.0334 - recon_loss: 0.9341 - val_loss: 1.6917 - val_kl_loss: 2.7684 - val_recon_loss: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8282 - kl_loss: 3.0256 - recon_loss: 0.9205 - val_loss: 2.0997 - val_kl_loss: 2.8643 - val_recon_loss: 1.2404\n",
      "Epoch 131/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8416 - kl_loss: 3.0184 - recon_loss: 0.9361 - val_loss: 1.7534 - val_kl_loss: 2.8413 - val_recon_loss: 0.9010\n",
      "Epoch 132/200\n",
      "1465/1465 [==============================] - 0s 89us/step - loss: 1.8632 - kl_loss: 3.0325 - recon_loss: 0.9534 - val_loss: 1.8428 - val_kl_loss: 2.7252 - val_recon_loss: 1.0252\n",
      "Epoch 133/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8346 - kl_loss: 2.9941 - recon_loss: 0.9364 - val_loss: 1.8453 - val_kl_loss: 2.7399 - val_recon_loss: 1.0233\n",
      "Epoch 134/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8350 - kl_loss: 2.9562 - recon_loss: 0.9481 - val_loss: 1.7969 - val_kl_loss: 2.7980 - val_recon_loss: 0.9575\n",
      "Epoch 135/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.8223 - kl_loss: 2.9965 - recon_loss: 0.9234 - val_loss: 1.6180 - val_kl_loss: 2.6073 - val_recon_loss: 0.8358\n",
      "Epoch 136/200\n",
      "1465/1465 [==============================] - 0s 73us/step - loss: 1.8604 - kl_loss: 3.0253 - recon_loss: 0.9528 - val_loss: 1.6822 - val_kl_loss: 2.7290 - val_recon_loss: 0.8635\n",
      "Epoch 137/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8164 - kl_loss: 2.9324 - recon_loss: 0.9366 - val_loss: 1.7173 - val_kl_loss: 2.7565 - val_recon_loss: 0.8903\n",
      "Epoch 138/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8255 - kl_loss: 3.0121 - recon_loss: 0.9219 - val_loss: 1.9926 - val_kl_loss: 2.6470 - val_recon_loss: 1.1985\n",
      "Epoch 139/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.8514 - kl_loss: 2.9899 - recon_loss: 0.9544 - val_loss: 1.7914 - val_kl_loss: 2.8035 - val_recon_loss: 0.9504\n",
      "Epoch 140/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.8463 - kl_loss: 2.9872 - recon_loss: 0.9501 - val_loss: 1.8734 - val_kl_loss: 2.7350 - val_recon_loss: 1.0529\n",
      "Epoch 141/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8196 - kl_loss: 3.0178 - recon_loss: 0.9143 - val_loss: 1.9616 - val_kl_loss: 2.7619 - val_recon_loss: 1.1330\n",
      "Epoch 142/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8062 - kl_loss: 2.9632 - recon_loss: 0.9173 - val_loss: 1.8401 - val_kl_loss: 2.8113 - val_recon_loss: 0.9967\n",
      "Epoch 143/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.8090 - kl_loss: 2.9874 - recon_loss: 0.9128 - val_loss: 1.9990 - val_kl_loss: 2.6076 - val_recon_loss: 1.2167\n",
      "Epoch 144/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.8101 - kl_loss: 2.9964 - recon_loss: 0.9111 - val_loss: 1.8300 - val_kl_loss: 2.5979 - val_recon_loss: 1.0506\n",
      "Epoch 145/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.8438 - kl_loss: 2.9880 - recon_loss: 0.9474 - val_loss: 1.7344 - val_kl_loss: 2.7178 - val_recon_loss: 0.9190\n",
      "Epoch 146/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.8045 - kl_loss: 2.9782 - recon_loss: 0.9111 - val_loss: 1.9433 - val_kl_loss: 2.8090 - val_recon_loss: 1.1006\n",
      "Epoch 147/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7997 - kl_loss: 2.9888 - recon_loss: 0.9031 - val_loss: 1.8497 - val_kl_loss: 2.8257 - val_recon_loss: 1.0020\n",
      "Epoch 148/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.8305 - kl_loss: 2.9937 - recon_loss: 0.9323 - val_loss: 1.7990 - val_kl_loss: 2.6892 - val_recon_loss: 0.9923\n",
      "Epoch 149/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.8102 - kl_loss: 2.9841 - recon_loss: 0.9149 - val_loss: 1.8533 - val_kl_loss: 2.7089 - val_recon_loss: 1.0407\n",
      "Epoch 150/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7728 - kl_loss: 2.9500 - recon_loss: 0.8878 - val_loss: 1.6945 - val_kl_loss: 2.6924 - val_recon_loss: 0.8868\n",
      "Epoch 151/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7895 - kl_loss: 2.9165 - recon_loss: 0.9145 - val_loss: 1.9974 - val_kl_loss: 2.7477 - val_recon_loss: 1.1731\n",
      "Epoch 152/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.8208 - kl_loss: 2.9500 - recon_loss: 0.9358 - val_loss: 1.7318 - val_kl_loss: 2.7191 - val_recon_loss: 0.9161\n",
      "Epoch 153/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7718 - kl_loss: 2.9299 - recon_loss: 0.8928 - val_loss: 1.7691 - val_kl_loss: 2.8762 - val_recon_loss: 0.9063\n",
      "Epoch 154/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7931 - kl_loss: 2.9152 - recon_loss: 0.9185 - val_loss: 1.8086 - val_kl_loss: 2.7695 - val_recon_loss: 0.9777\n",
      "Epoch 155/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7637 - kl_loss: 2.8639 - recon_loss: 0.9045 - val_loss: 1.7475 - val_kl_loss: 2.5191 - val_recon_loss: 0.9918\n",
      "Epoch 156/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7741 - kl_loss: 2.8958 - recon_loss: 0.9054 - val_loss: 1.7233 - val_kl_loss: 2.6886 - val_recon_loss: 0.9167\n",
      "Epoch 157/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7953 - kl_loss: 2.9728 - recon_loss: 0.9035 - val_loss: 1.6485 - val_kl_loss: 2.7905 - val_recon_loss: 0.8113\n",
      "Epoch 158/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7639 - kl_loss: 2.8823 - recon_loss: 0.8992 - val_loss: 1.7163 - val_kl_loss: 2.5603 - val_recon_loss: 0.9482\n",
      "Epoch 159/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7850 - kl_loss: 2.8765 - recon_loss: 0.9221 - val_loss: 2.0373 - val_kl_loss: 2.6983 - val_recon_loss: 1.2278\n",
      "Epoch 160/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7868 - kl_loss: 2.9286 - recon_loss: 0.9083 - val_loss: 1.6862 - val_kl_loss: 2.7027 - val_recon_loss: 0.8754\n",
      "Epoch 161/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7867 - kl_loss: 2.9515 - recon_loss: 0.9012 - val_loss: 1.9775 - val_kl_loss: 2.7868 - val_recon_loss: 1.1415\n",
      "Epoch 162/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7914 - kl_loss: 2.9460 - recon_loss: 0.9076 - val_loss: 2.0636 - val_kl_loss: 2.9443 - val_recon_loss: 1.1803\n",
      "Epoch 163/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7732 - kl_loss: 2.9355 - recon_loss: 0.8925 - val_loss: 1.6755 - val_kl_loss: 2.6343 - val_recon_loss: 0.8852\n",
      "Epoch 164/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7953 - kl_loss: 2.9128 - recon_loss: 0.9214 - val_loss: 2.5754 - val_kl_loss: 2.8074 - val_recon_loss: 1.7331\n",
      "Epoch 165/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.8326 - kl_loss: 2.9290 - recon_loss: 0.9539 - val_loss: 2.3053 - val_kl_loss: 2.9693 - val_recon_loss: 1.4145\n",
      "Epoch 166/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7953 - kl_loss: 2.9931 - recon_loss: 0.8974 - val_loss: 1.6749 - val_kl_loss: 2.6605 - val_recon_loss: 0.8767\n",
      "Epoch 167/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7970 - kl_loss: 2.9428 - recon_loss: 0.9142 - val_loss: 1.7624 - val_kl_loss: 2.7109 - val_recon_loss: 0.9491\n",
      "Epoch 168/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7924 - kl_loss: 2.9252 - recon_loss: 0.9148 - val_loss: 2.0231 - val_kl_loss: 2.7724 - val_recon_loss: 1.1913\n",
      "Epoch 169/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7538 - kl_loss: 2.9202 - recon_loss: 0.8778 - val_loss: 1.8651 - val_kl_loss: 2.6122 - val_recon_loss: 1.0814\n",
      "Epoch 170/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7794 - kl_loss: 2.9026 - recon_loss: 0.9086 - val_loss: 1.6310 - val_kl_loss: 2.6703 - val_recon_loss: 0.8299\n",
      "Epoch 171/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7556 - kl_loss: 2.9037 - recon_loss: 0.8845 - val_loss: 1.6660 - val_kl_loss: 2.7019 - val_recon_loss: 0.8554\n",
      "Epoch 172/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7913 - kl_loss: 2.9512 - recon_loss: 0.9059 - val_loss: 2.2457 - val_kl_loss: 2.6420 - val_recon_loss: 1.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.7504 - kl_loss: 2.8868 - recon_loss: 0.8843 - val_loss: 1.8505 - val_kl_loss: 2.6231 - val_recon_loss: 1.0636\n",
      "Epoch 174/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7520 - kl_loss: 2.8839 - recon_loss: 0.8869 - val_loss: 1.8226 - val_kl_loss: 2.7353 - val_recon_loss: 1.0020\n",
      "Epoch 175/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7521 - kl_loss: 2.8521 - recon_loss: 0.8965 - val_loss: 1.8715 - val_kl_loss: 2.6394 - val_recon_loss: 1.0797\n",
      "Epoch 176/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7630 - kl_loss: 2.8913 - recon_loss: 0.8956 - val_loss: 1.7290 - val_kl_loss: 2.6754 - val_recon_loss: 0.9263\n",
      "Epoch 177/200\n",
      "1465/1465 [==============================] - 0s 90us/step - loss: 1.7572 - kl_loss: 2.8726 - recon_loss: 0.8955 - val_loss: 1.6025 - val_kl_loss: 2.6931 - val_recon_loss: 0.7945\n",
      "Epoch 178/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7570 - kl_loss: 2.8955 - recon_loss: 0.8884 - val_loss: 2.0142 - val_kl_loss: 2.6872 - val_recon_loss: 1.2080\n",
      "Epoch 179/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7861 - kl_loss: 2.9052 - recon_loss: 0.9146 - val_loss: 1.7919 - val_kl_loss: 2.6868 - val_recon_loss: 0.9859\n",
      "Epoch 180/200\n",
      "1465/1465 [==============================] - 0s 86us/step - loss: 1.7961 - kl_loss: 2.9351 - recon_loss: 0.9156 - val_loss: 2.1432 - val_kl_loss: 2.7353 - val_recon_loss: 1.3226\n",
      "Epoch 181/200\n",
      "1465/1465 [==============================] - 0s 85us/step - loss: 1.7611 - kl_loss: 2.9141 - recon_loss: 0.8869 - val_loss: 2.0250 - val_kl_loss: 2.6888 - val_recon_loss: 1.2184\n",
      "Epoch 182/200\n",
      "1465/1465 [==============================] - 0s 80us/step - loss: 1.7666 - kl_loss: 2.8895 - recon_loss: 0.8997 - val_loss: 1.7436 - val_kl_loss: 2.7180 - val_recon_loss: 0.9282\n",
      "Epoch 183/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7987 - kl_loss: 2.9342 - recon_loss: 0.9185 - val_loss: 1.7626 - val_kl_loss: 2.5888 - val_recon_loss: 0.9860\n",
      "Epoch 184/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7419 - kl_loss: 2.8487 - recon_loss: 0.8873 - val_loss: 2.0745 - val_kl_loss: 2.7681 - val_recon_loss: 1.2441\n",
      "Epoch 185/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7483 - kl_loss: 2.8678 - recon_loss: 0.8880 - val_loss: 2.3799 - val_kl_loss: 2.8470 - val_recon_loss: 1.5258\n",
      "Epoch 186/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7504 - kl_loss: 2.8966 - recon_loss: 0.8815 - val_loss: 1.8589 - val_kl_loss: 2.6185 - val_recon_loss: 1.0733\n",
      "Epoch 187/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7688 - kl_loss: 2.9180 - recon_loss: 0.8934 - val_loss: 1.9480 - val_kl_loss: 2.7613 - val_recon_loss: 1.1196\n",
      "Epoch 188/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7427 - kl_loss: 2.8661 - recon_loss: 0.8829 - val_loss: 1.8923 - val_kl_loss: 2.7167 - val_recon_loss: 1.0772\n",
      "Epoch 189/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7554 - kl_loss: 2.8762 - recon_loss: 0.8926 - val_loss: 2.1525 - val_kl_loss: 2.6661 - val_recon_loss: 1.3527\n",
      "Epoch 190/200\n",
      "1465/1465 [==============================] - 0s 81us/step - loss: 1.7349 - kl_loss: 2.8314 - recon_loss: 0.8854 - val_loss: 1.8593 - val_kl_loss: 2.6755 - val_recon_loss: 1.0567\n",
      "Epoch 191/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7494 - kl_loss: 2.8374 - recon_loss: 0.8982 - val_loss: 1.5968 - val_kl_loss: 2.6849 - val_recon_loss: 0.7913\n",
      "Epoch 192/200\n",
      "1465/1465 [==============================] - 0s 83us/step - loss: 1.7151 - kl_loss: 2.8400 - recon_loss: 0.8631 - val_loss: 1.6298 - val_kl_loss: 2.5026 - val_recon_loss: 0.8790\n",
      "Epoch 193/200\n",
      "1465/1465 [==============================] - 0s 76us/step - loss: 1.7329 - kl_loss: 2.8544 - recon_loss: 0.8766 - val_loss: 1.6006 - val_kl_loss: 2.5615 - val_recon_loss: 0.8321\n",
      "Epoch 194/200\n",
      "1465/1465 [==============================] - 0s 82us/step - loss: 1.7394 - kl_loss: 2.8327 - recon_loss: 0.8896 - val_loss: 1.7021 - val_kl_loss: 2.5951 - val_recon_loss: 0.9236\n",
      "Epoch 195/200\n",
      "1465/1465 [==============================] - 0s 78us/step - loss: 1.7323 - kl_loss: 2.8700 - recon_loss: 0.8713 - val_loss: 1.8618 - val_kl_loss: 2.6367 - val_recon_loss: 1.0708\n",
      "Epoch 196/200\n",
      "1465/1465 [==============================] - 0s 77us/step - loss: 1.7278 - kl_loss: 2.8172 - recon_loss: 0.8826 - val_loss: 1.9282 - val_kl_loss: 2.6659 - val_recon_loss: 1.1284\n",
      "Epoch 197/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7258 - kl_loss: 2.8586 - recon_loss: 0.8683 - val_loss: 1.8234 - val_kl_loss: 2.6349 - val_recon_loss: 1.0329\n",
      "Epoch 198/200\n",
      "1465/1465 [==============================] - 0s 84us/step - loss: 1.7352 - kl_loss: 2.8342 - recon_loss: 0.8849 - val_loss: 1.8032 - val_kl_loss: 2.6337 - val_recon_loss: 1.0131\n",
      "Epoch 199/200\n",
      "1465/1465 [==============================] - 0s 88us/step - loss: 1.7359 - kl_loss: 2.8609 - recon_loss: 0.8776 - val_loss: 1.9099 - val_kl_loss: 2.6134 - val_recon_loss: 1.1258\n",
      "Epoch 200/200\n",
      "1465/1465 [==============================] - 0s 79us/step - loss: 1.7324 - kl_loss: 2.8660 - recon_loss: 0.8726 - val_loss: 2.2460 - val_kl_loss: 2.6234 - val_recon_loss: 1.4590\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5040acc550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for name_period, el in dict_datasets.items():\n",
    "    dataset = el['dataset']\n",
    "\n",
    "    print('========================= Model {} ========================='.format(idx+1))\n",
    "\n",
    "    # Prepare model characteristics\n",
    "    name_model = '{}_cvae_conso_e48-24-12_d12-24-48_gran-{}_z{}_beta{}_x-{}_cond-{}'.format(name_period,name_granu,z_dim,beta,name_type_x, name_type_cond)\n",
    "\n",
    "    # Compile model\n",
    "    model = CVAE(cond_dim=cond_dim, \n",
    "             e_dims=e_dims, \n",
    "             d_dims=d_dims, \n",
    "             z_dim=z_dim, \n",
    "             beta=beta,\n",
    "             name=name_model, \n",
    "             output=path_out)\n",
    "\n",
    "    # Prepare callbacks\n",
    "    callbacks = []\n",
    "    \n",
    "    tensorboard_model = TensorBoard(log_dir=os.path.join(path_out, name_model, 'results', 'logs', time.strftime('%Y-%m-%d_%H:%M', time.localtime(time.time()))))\n",
    "    tensorboard_summary = TensorBoard(log_dir=os.path.join(path_out, 'logs', name_model))\n",
    "\n",
    "    callbacks.append(tensorboard_model)\n",
    "    callbacks.append(tensorboard_summary)\n",
    "\n",
    "    # Train model\n",
    "    model.main_train(dataset, training_epochs=training_epochs, batch_size=batch_size, \n",
    "                     callbacks=callbacks, verbose=False)\n",
    "\n",
    "    # Get result and put it in results\n",
    "    result = {}\n",
    "    result['name'] = name_period\n",
    "    for key, el in model.history.items():\n",
    "        result[key] = el[-1]\n",
    "    \n",
    "    results_df= results_df.append(result, ignore_index=True)\n",
    "    results_df.to_csv(os.path.join(path_results, 'cv_results.csv'), sep=';')\n",
    "\n",
    "    # Reset graph\n",
    "    K.clear_session()\n",
    "    import tensorflow as tf\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_out, 'dict_datasets.pickle'),'wb') as f:\n",
    "    pickle.dump(dict_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
